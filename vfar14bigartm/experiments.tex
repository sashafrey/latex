\documentclass{article}

% TEMP PACKAGES!
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{cmap}

% These ones should be added to the article's .tex file...
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{standalone}
\usepackage{array}
\usepackage{amsmath,amssymb}

\usepackage{url}

%... and these rows too.
\pgfplotsset{ every non boxed x axis/.append style={x axis line style=-},
     every non boxed y axis/.append style={y axis line style=-}}
\pgfplotsset{compat = 1.3}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\renewcommand{\vec}[1]{{\boldsymbol #1}}

\begin{document}

\section{Сравнение BigARTM с другими библиотеками}

Сравниваем BigARTM с реализацией алгоритма Online VB LDA в библиотеке для тематического моделирования Gensim\footnote{\url{http://radimrehurek.com/gensim/}} и Vowpal Wabbit\footnote{\url{https://github.com/JohnLangford/vowpal_wabbit/wiki/Latent-Dirichlet-Allocation}}. Для сравнения будем строить тематическую модель для документов из английской Википедии (корпус описан далее). 

\paragraph{Перплексия.} Алгоритм LDA представляет тематическую модель в виде распределений Дирихле на строчки $\Theta$ и столбцы $\Phi$:
\begin{gather}
	\vec{\theta}_{d} \sim \text{Dir}(\vec{\gamma}_d), \quad \vec{\phi}_{t} \sim \text{Dir}(\vec{\lambda}_t)
\end{gather}
Для того чтобы сравнить перплексию матриц $\Phi, \Theta$ для hold-out документов, мы будем брать средние распределения 
\begin{gather}
	\vec{\theta}^\text{mean}_d = \mathbb{E}_{\text{Dir}(\vec{\gamma}_d)}\vec{\theta}_d, \quad \vec{\phi}^\text{mean}_t = \mathbb{E}_{\text{Dir}(\vec{\gamma}_d)} \vec{\phi}_t.
\end{gather}
 
\paragraph{Параметры эксперимента}
\begin{itemize}
	\item Машина: x86\_64, 32 ядра, 1324.898MHz
	\item Corpus: English Wikipedia shapshot \texttt{2014-12-08}, hold-out 100'000 documents
	\item Topics: 100
	\item One pass through train documents of the corpus
	\item Batch size: 10'000 (\texttt{chunksize} in Gensim, \texttt{--minibatch} in VW)
	\item Update rule: $\rho = (\tau_0 + t)^{-\kappa}$, $\tau_0 = 1$, $\kappa = 0.5$
	\item Update after each batch in non-parallel implementation, update after $P$ batches when running in $P$ parallel threads (\texttt{update\_every = num\_processors})
	\item LDA Priors: $\alpha = 0.1,\, \beta = 0.1$ ($\vec{\theta}_d \sim \text{Dir}(\alpha),\, \vec{\phi}_t \sim \text{Dir}(\beta)$)
\end{itemize}


\begin{table}
	\centering
	\label{tab:libraries_comparison}

	\begin{tabular}[t]{c|c|ccc}
	\hline
	Library & Proc. & Train Time & Inference Time & Perplexity \\
	\hline
	BigARTM Smoothing & 1 & 62 min & 127 sec & 4000 \\
	Gensim LDA & 1 & 369 min & 395 sec & 4161 \\
	Vowpal Wabbit LDA & 1 & 73 min & 120 sec & 4108 \\
	\hline
	BigARTM Smoothing & 8 & 8 min & 24 sec & 4304 \\
	Gensim LDA-Multicore & 8 & 70 min & 338 sec & 4470 \\
	\end{tabular}
	\caption{Сравнение BigARTM с реализацией LDA в библиотеке Gensim и Vowpal Wabbit. Train Time — время на обучение модели, Inference Time — время вычисления $\theta_d$ для всех документов из hold-out. Perplexity — перплексия посчитанная по обученной модели $\{\vec{\phi}_t\}$ на hold-out документах $\{vec{\theta}_d\}$, в случае Gensim и VW в качестве распределений $\vec{\phi}_t$ и $\vec{\theta}_d$ брались средние из соответствующих распределений Дирихле.}
\end{table}

\begin{figure}
	\centering
	\label{fig:bigartm_speedup}
	\includegraphics[height=6cm]{bigartm_speedup}
	\caption{BigARTM speed up }
\end{figure}



\section{On LDA and ARTM}

Данный раздел посвящён сравнению моделей LDA и ARTM. В экспериментах, сравнивающих BigARTM с Gensim и VW.LDA, мы показали, что алгоритм Online PLSA со сглаживающим регуляризатором и Online VB LDA работают схожим образом. Поэтому в этом эксперименте будут оцениваться характеристики PLSA со сглаживающим регуляризатором (который мы далее будем называть LDA) и ARTM (суть PLSA с набором регуляризаторов).

\paragraph{Текстовая коллекция} Все наши эксперименты проводились на корпусе английской Википедии
\footnote{Коллекция была получена с помощью gensim.make\_wikicorpus.}
, объём которой $|D| \approx 3.7 \times 10^6$ документов. Словарь имеет размер $|W| \approx 10^5$, общая длина коллекции в словах $n \approx 577 \times 10^6$.

\paragraph{Параметры эксперимента}
В этом эксперименте мы будем пользоваться следующими функционалами качества моделирования:
\begin{itemize}
	\item Перплексия на контрольной выборке\
	\footnote{Объём контрольной выборки, на которой перплексия измерялась в ходе прохода по коллекции --- 10 тыс. документов. Кроме того, была измерена результирующая перплексия на выборке из 100 тыс. документов.}.
	\item Разреженность матрицы $\Phi$.
	\item Разреженность матрицы $\Theta$ документов обучающей выборки.
	\item Характеристики ядер тем (размер, чистота, контрастность) (\cite{LABEL} ССЫЛКА НА НУЖНУЮ ПУБЛИКАЦИЮ КВ!!!).
\end{itemize}
Обе модели будут иметь следующий общий набор параметров, с которыми будет запускаться BigARTM: 1 проход по коллекции
\footnote{Подразумевается один полный проход по всей коллекции и повторный проход по первым $1.5 \times 10^5$ документам для уточнения их распределений.}
, 10 проходов по каждому документу, 100 выделяемых тем. Матрица $\Theta$, построенная на предыдущем проходе по документу, используется в качестве начального приближения на текущем. Параметры обновления матрицы $\Phi$, $\kappa$ и $\tau_0$, равны 0.5 и 64 соответственно
\footnote{Как это было в экспериментах в \ref{LABEL} РАЗДЕЛ ПРО СРАВНЕНИЕ БИБЛИОТЕК!!!}
. Порог $p(t|w)$ для ядровых функционалов --- 0.25,. Размер батча равен 10000, обновления модели производится каждые батч.

Параметры LDA $\alpha = \beta = \cfrac{1}{|T|}$.

\begin{figure}[h!]
\begin{tabular}{cc}
\input{plot_perplexity_sparsity}
&
\input{plot_kernel}
\end{tabular}
\caption{Comparison of LDA (thin) and ARTM (bold) models. X axis is a number of processed documents.} \label{fig:comparison_plot}
\end{figure}

Регуляризатор для ARTM, представляющий собой смесь разреживания и декорреляции тем, описывается формулой

\begin{align}
    R(\Phi,\Theta)
    =&
    - \beta \sum_{t\in T} \sum_{w\in W} \beta_w \ln \phi_{wt}
    - \alpha \sum_{d\in D} \sum_{t\in T} \alpha_t \ln \theta_{td}
    \notag
\\  {}&
    - \gamma
        \sum_{t\in T}
        \sum_{s\in T\backslash t}
        \sum_{w\in W} \phi_{wt}\phi_{ws}
    \to \max.
\end{align}

Отсюда получаются формулы M-шага

\begin{align}
    \phi_{wt} &\propto
        \Bigl(n_{wt}
            - \beta \underbrace{\beta_{w} [t\!\in\! T]}_{\substack{\text{sparsing}\\\text{topic}}} {}
            - \gamma \underbrace{[t\!\in\! T]\: \phi_{wt} \!\sum_{s\in T\backslash t}\! \phi_{ws}}_{\text{decorrelation}} {}
        \Bigr)_{+};
\\
    \theta_{td} &\propto
        \Bigl(n_{td}
            - \alpha \underbrace{\alpha_{t} [t\!\in\! T]}_{\substack{\text{sparsing}\\\text{topic}}} {}
        \Bigr)_{+}.
\end{align}

Коэффициенты $\beta_w$ и $\alpha_t$ примем равными 1, $\forall w,t$. Коэффициенты регуляризации $\alpha,  \beta$ и $\gamma$ возьмём постоянными на протяжении всего прохода по коллекции. Их значения: $\alpha = 0.15, \beta = 0.009, \gamma = 7.8 \times 10^5$.

\paragraph{Результаты} В таблице \ref{tab:model_comparison} приведены финальные значения функционалов качества после одного прохода по коллекции для моделей LDA и ARTM. Видно, что комбинация регуляризаторов разреживания и декорреляции улучшает качество результирующей модели с небольшими потерями перплексии.

\begin{table}[t]
\caption{Comparison of LDA and ARTM models. Quality functionals: $\mathcal{P}_{10k}$ $\mathcal{P}_{100k}$ --- hold-out perplexity on 10.000 and 100.000 documents sets, $\mathcal{S}_{\Phi}$, $\mathcal{S}_{\Theta}$ --- sparsity of $\Phi$ and $\Theta$ matrices (in \%), $\mathcal{K}_{s}$, $\mathcal{K}_{p}$, $\mathcal{K}_{c}$ --- average topic kernel size, purity and contrast respectively.}
\label{tab:model_comparison}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}[t]{P{10.5em}|P{3.5em} P{3.5em} P{3.5em} P{3.5em} P{3.5em} P{3.5em} P{3.5em} }
\hline
Model/Functional & $\mathcal{P}_{10k}$ & $\mathcal{P}_{100k}$ &  $\mathcal{S}_{\Phi}$ & $\mathcal{S}_{\Theta}$ &  $\mathcal{K}_{s}$ & $\mathcal{K}_{p}$ &  $\mathcal{K}_{c}$ \\
\hline
LDA              & 3499 & 3827 & 0.0  & 0.0  & 931  & 0.535 & 0.516 \\
ARTM             & 3592 & 3944 & 96.3 & 80.5 & 1135 & 0.810 & 0.732 \\
\hline
\end{tabular}
\end{center}
\end{table}

Более подробно процесс обучения представлен на \ref{fig:comparison_plot}. На верхнем графике показано убывание перплексии и замеры разреженностей матриц $\Phi$ и $\Theta$. На нижнем --- усреднённые характеристики ядер тем. Видно, что LDA совершенно не способствует разреживанию и даёт менее чистые и контрастные ядра тем, чем ARTM.

\end{document}
