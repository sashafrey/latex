
В данном разделе обзорно рассматриваются несколько существующих популярные реализаций параллельных алгоритмов тематического моделирования. Все они основаны на модели LDA, имеют разные технические и алгоритмические детали. Рассмотрим их основные преимущества и недостатки, после чего опишем 

\subsection{Краткий обзор}

\paragraph{AD-LDA}
Данный алгоритм был предложен в работе \cite{ad_lda}. В AD-LDA был реализован только межпроцессорный параллелизм, возможность работы на кластере не обсуждалась. Данная работа является одной из первых, в которых была осознана возможность параллелизации процеса обучения модели, и в этом её серьёзное достоинство. С другой стороны, в технической реализации имеется серьёзные недостаток --- собранные на разных процессорах статистики сливаются вместе во время специально выделяемого шага синхронизации. Это приводит к тому, что скорость работы алгоритма определяется скростью работы самого медленного процессора. Оставляет желать лучшего отказоустойчивость системы. Нагрузка на сеть так же неравномерна --- во время синхронизации она чрезвычайно высока, в прочие же временные промежутки сеть простаивает. Кроме того, в данном алгоритме каждый процессор обладает своей матрицей $\Phi$, что приводит к неоправданным расходам памяти.

\paragraph{PLDA}
PLDA был описан в статье \cite{plda}. Алгоритм представляет собой модификацию AD-LDA, в которой параллелизм реализован в двух вариантах: с помощью MPI и Map-Reduce (при этом работа опять же распределяется между процессорами, кластерная реализация отсутствует). Авторам удалось добиться повышения отказоустойчивости, однако сделано это было путём увеличения потребления дисковой памяти (которая и в AD-LDA потреблялась в слишком большом объёме). Основной недостаток AD-LDA --- шаг синхронизации --- так же был унаследован PLDA.

\paragraph{Y!LDA}
Следующим шагом для параллельных алгоритмов обучения тематических стала работа Y!LDA, описанная в \cite{y_lda}. Авторы описали недостатки Ad-LDA и предложили свой вариант параллелизма, теперь уже как в рамках одной машины, так и в кластере. Основным достижением стало избавление от выделенного шага синхронизации. Авторы отметили его основные недостатки (плохая отказоустойчивость, неравномерная нагрузка на сеть и ожидание медленных процессоров) и предложили в рамках одной машины ввести специальный поток синхронизации, к которому ядра будут обращаться независимо друг от друга. Кроме того, было принято решение хранить на одной машине только одну копию матрицы $\Phi$, общую для всех процессоров.

Для кластера авторы предложили использовать т.н. <<архитектуру классной доски>>. Суть её состоит в том, что каждый компьютер в сети обращается к глобальной матрице $\Phi$ независимо от других, обновляя нужные статистики. Порцией обновления при этом является один термин. Кроме того, было предложено и хорошее техническое решение --- модель (матрица $\Phi$) хранится в \verb|memcached|. Кластерный параллелизм был реализован в Hadoop.
\footnote{memcached — связующее программное обеспечение, реализующее сервис кэширования данных в оперативной памяти на основе парадигмы хеш-таблицы.}.

\paragraph{PLDA+}
В этой публикации была предпринята попытка модифицировать PLDA. Авторы сделали алгоритм параллельным в кластерном смысле. Для того, чтобы побороть проблемы, связанные с шагом синхронизации, было решено использовать конвейерную архитектуру. Для этого всё множество процессоров делитя на две группы --- вычислители и коммутаторы. Первая группа производит сэмплирование (собственно расчёт модели), вторая отвечает за своевременную доставку данных. Вкупе с равномерным распределением коллекции в виде блоков по процессорам-вычислителям, а также наличием механизма приоритетов, сглаживающем узкие места во время счёта, это даёт, согласно экспериментам, хорошие результаты. Тем не менее, один существенный недостаток у данной реализации имеется --- значимая часть вычислительных ресурсов тратится не на счёт, а на своевременную доставку данных.

\subsection{BigARTM}
Из всего вышеописанного было сделано несколько выводов:

\begin{enumerate}
	\item Алгоритм обязательно должен быть асинхронным.
	\item Матрица $\Phi$ должна быть локальной в рамках компьютера, а не ядра.
	\item Использование \verb|memcached| является хорошей идеей
	\footnote{К сожалению, сервис memcached предназначен для использования только под Linux. Тем не менее, это не является большой проблемой, поскольку большинство кластеров работает именно под Unix-подобными ОС и кластерный параллелизм BigARTM будет ориенторован именно под них. Windows-версия библиотеки, по сути, предназначена для использования на локальной машине. Хотя, возможно, к релизу для Windows-версии будет написан сервис, осуществляющий функции memcached, как это уже сделано в рамках многопроцессорного параллелизма.}
	.
	\item В качестве кластера лучше использовать MPI, а не Hadoop.
	\item Коллекцию следует разделить на блоки, обрабатываемые независимо.
\end{enumerate}

Все эти идеи положены в основу BigARTM. Они либо уже реализованы, либо будут реализованы к релизу библиотеки.