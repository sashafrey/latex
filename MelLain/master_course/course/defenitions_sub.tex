
$\quad\;\:$Прежде всего рассмотрим некоторые базовые понятия и необходимые обозначения.

Вероятностная тематическая модель (ВТМ) описывает каждую тему дискретным распределением на множестве терминов, каждый документ --- дискретным распределением на множестве тем. Предполагается, что коллекция документов --- это последовательность терминов, выбранных случайно и независимо из смеси таких распределений, и ставится задача восстановления компонент смеси по выборке.

\begin{itemize}
	\item $D$ --- коллекция текстовых документов.
	\item $W$ --- словарь коллекции текстов.
	\item $T$ --- множество тем.
\end{itemize}

Документы в коллекции можно представить в виде так называемого <<мешка слов>>. В рамках этой концепции документ рассматривается как множество терминов из словаря и соответствующих им счётчиков частот встречаемости.

{\bf Замечание: } <<Мешок слов>> на данный момент является основным способом представления коллекции, однако в последнее время всё большее развитие получают идеи хранения документа в виде последовательности слов. Порядок слов при этом становится важным и используется для улучшения качества обучения модели. BigARTM будет поддерживать оба способа представления.

После принятия гипотезы <<мешка слов>>, коллекция представляется в виде матрицы $F_{W \times D}$, строки которой соответствуют терминам из словаря, а столбцы --- документам коллекции. На пересечении строки и столбца находится оценка вероятности встретить данное слово в данном документе. Эта оценка является отношением числа раз, которое слово встретилось в документе, к общему числу слов в этом документе. Таким образом, столбцы матрицы $F$ представляют собой распределения вероятностей.

При рассмотрении коллекции в виде пар $(d, w)$, где $w$ --- номер термина, а $d$ --- номер документа, вводятся следующие счётчики частот:

\begin{itemize}
	\item $n_{dw}$ --- число вхождений термина $w$ в документ $d$;
	\item $n_d = \sum_{w \in W} n_{dw}$ --- длина документа $d$ в терминах;
	\item $n_w = \sum_{d \in D} n_{dw}$ --- число вхожденией документа $w$ во все документы коллекции;
	\item $n = \sum_{d \in D}\sum_{w \in d} n_{dw}$ --- длина коллекции $D$ в терминах; 
\end{itemize}

Если же рассматривать коллекцию в виде троек $(d, w, t)$, где $d$, $w$ и $t$ --- номера соответствующих документа, термина и темы, то можно ввести такие счётчики: 

\begin{itemize}
	\item $n_{dwt}$ --- число троек, в которых термин $w$ встретился в документе $d$ и связан с темой~$t$;
	\item $n_{dt} = \sum_{w \in W} n_{dwt}$ --- число троек, в которых термин из документа $d$ связан с темой $t$;
	\item $n_{wt} = \sum_{d \in D} n_{dwt}$ --- число троек, в которых термин $w$ связан с темой $t$;
	\item $n_t = \sum_{d \in D}\sum_{w \in d} n_{dwt}$ --- число троек, связанных с темой $t$;
\end{itemize}

С использованием данных счётчиков можно ввести следующие частотные оценки вероятностей, связанных со скрытой переменной $t$:

\begin{itemize}\label{label_1}
\item $ 
	\hat p(w|t) = \cfrac{n_{wt}}{n_t}, \quad
	\hat p(t|d) = \cfrac{n_{dt}}{n_d}, $ 
\end{itemize}

Ставится задача разложения матрицы $F$ в произведение двух матриц $\Phi$ и $\Theta$ меньшего размера, таких, что

\begin{itemize}
\item $\Phi = (\phi_{wt})_{W \times T}, \; \phi_{wt} = \hat p(w|t)$ --- матрица <<термины-темы>>;
\item $\Theta = (\theta_{td})_{T \times D}, \; \theta_{td} = \hat p(t|d)$ --- матрица <<темы-документы>>;
\end{itemize}

Поставленная задача ($F \approx \Phi \Theta$) эквивалентна поиску матриц $\Phi$ и $\Theta$, максимизирующих следующий функционал правдоподобия:

\begin{equation}\label{eq_1}
 	L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \sum_{t \in T} \phi_{wt} \theta_{td} \rightarrow \max_{\Phi, \Theta}
\end{equation}
