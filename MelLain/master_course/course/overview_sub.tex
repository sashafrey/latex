
$\quad\;\:$В данном разделе кратко рассматриваются некоторые известные параллельные алгоритмы тематического моделирования. Все они основаны на модели LDA, имеют разные технические и алгоритмические детали. Рассмотрим их основные преимущества и недостатки, после чего опишем на базе полученных выводов ключевые требования к BigARTM. 

\subsection{Краткий обзор}

\subsubsection{AD-LDA}
$\quad\;\:$Данный алгоритм был предложен в работе \cite{ad_lda}. В AD-LDA был реализован только межпроцессорный параллелизм, возможность работы на кластере не обсуждалась. 

Представленная архитектура предполагает размещение документов в случайном порядке на каждом из участвующих в вычислениях процессоров. Все процессоры одновременно производят сэмплирование, получая счётчики матрицы $\Theta$. Этот процесс происходит до тех пор, пока последний процессор не закончит работу. После производится шаг синхронизации, на котором счётчики со всех процессоров сливаются вместе. Затем происходит обновление глобальной матрицы $\Phi$. Новая $\Phi$ рассылается каждому процессору, и начинается следующая итерация симплирования. И так до тех пор, пока не выполнится заданный критерий останова.

Данная работа является одной из первых, в которых была осознана возможность параллелизации процеса обучения модели, и в этом её основное достоинство. Но сама реализация имеет ряд серьёзных недостатков:

\begin{enumerate}
	\item Скорость работы алгоритма определяется скоростью самого медленного процессора.
	\item Нагрузка на сеть неравномерна --- высокая во время синхронизации и почти нулевая в процессе сэмплирования. 
	\item Система обладает низкой отказоустойчивостью.
	\item Наличие отдельной матрицы $\Phi$ у каждого процессора приводит к большим затратам памяти.
\end{enumerate}

Как видно, большинство недостатков возникают в виде следствия наличия отдельного шага синхронизации.

\subsubsection{PLDA}
$\quad\;\:$PLDA был описан в статье \cite{plda}. Алгоритм представляет собой модификацию AD-LDA, в которой параллелизм реализован в двух вариантах: с помощью MPI и Map-Reduce (при этом работа опять же распределяется между процессорами, кластерная реализация отсутствует). 

Архитектура не претерпела больших изменений. Существенной модификацией является использование контрольных точек --- откачки данных после очередной итерации сэмплирования на жёсткий диск. В случае сбоя имеется возможность восстановить данные и продолжить вычисления. В MPI-реализации это действительно работает хорошо. В Map-Reduce версии реализация контрольных точек требует больших затрат памяти. Авторы сами рекомендуют использовать MPI.

Собственно, помимо низкой отказоустойчивости, PLDA сохранил все недостатки AD-LDA.

\subsubsection{Y!LDA}
$\quad\;\:$Следующим шагом для параллельных алгоритмов обучения тематических стал Y!LDA, описанный в \cite{y_lda}. Авторы рассмотрели недостатки AD-LDA и предложили свой вариант параллелизма, теперь уже как в рамках одной машины, так и на кластере. Основным достижением стало избавление от выделенного шага синхронизации. Авторы отметили его основные недостатки (плохая отказоустойчивость, неравномерная нагрузка на сеть и ожидание медленных процессоров) 
и предложили способ их разрешения, который будет рассмотрен далее.

Коллекция делится между машинами. В свою очередь, в рамках каждой ноды документы распределяются по процессорам. Все процессоры одного компьютера имеют общую матрицу $\Phi$, что существенно снижает затраты памяти. На каждой ноде имеется т.н. поток синхронизации, к которому ядра обращаются независимо по мере завершения обработки очередной порции документов.

Глобальная матрица $\Phi$ хранится в \verb|memcached|
\footnote{memcached — связующее программное обеспечение, реализующее сервис кэширования данных в оперативной памяти на основе парадигмы хеш-таблицы.}
.
Работа кластера организована с использованием т.н. <<архитектуры классной доски>>. Суть её состоит в том, что каждый компьютер в сети обращается к глобальной матрице $\Phi$ независимо от других, обновляя полученные им счётчики. Порцией обновления при этом является один термин. Кластерный параллелизм был реализован в Hadoop.

\subsubsection{PLDA+}
$\quad\;\:$В публикации \cite{plda_plus} была предпринята попытка модифицировать PLDA. Авторы сделали алгоритм параллельным в кластерном смысле. Для того, чтобы побороть проблемы, связанные с шагом синхронизации, было решено использовать конвейерную архитектуру. Для этого всё множество процессоров делится на две группы --- вычислители и коммутаторы. Первая группа производит сэмплирование (собственно расчёт модели), вторая отвечает за своевременную доставку данных. Вкупе с равномерным распределением коллекции в виде блоков по процессорам-вычислителям, а также наличием механизма приоритетов, сглаживающем узкие места во время счёта, это даёт, согласно экспериментам, хорошие результаты. Тем не менее, один существенный недостаток у данной реализации имеется --- значимая часть вычислительных ресурсов тратится не на счёт, а на своевременную доставку данных.

\subsection{BigARTM}
$\quad\;\:$Из всего описанного были сделаны следующие выводы:

\begin{enumerate}
	\item Алгоритм обязательно должен быть асинхронным, что позволит избежать недостатков AD-LDA.
	\item Матрица $\Phi$ должна быть локальной в рамках компьютера, а не ядра, что приведёт к существенной экономии оперативной памяти.
	\item Использование \verb|memcached| является хорошим решением для хранения глобальной матрицы $\Phi$
	\footnote{К сожалению, сервис memcached предназначен для использования только под Linux. Тем не менее, это не является большой проблемой, поскольку большинство кластеров работает именно под Unix-подобными ОС и кластерный параллелизм BigARTM будет ориенторован именно под них. Windows-версия библиотеки, по сути, предназначена для использования на локальной машине. Хотя, возможно, к релизу для Windows-версии будет написан сервис, осуществляющий функции memcached, как это уже сделано в рамках многопроцессорного параллелизма.}
	.
	\item Коллекцию следует разделить на пакеты документов, обрабатываемых независимо.
\end{enumerate}

Все эти идеи положены в основу BigARTM. Они либо уже реализованы, либо будут реализованы к моменту релиза библиотеки.
Таким образом, BigARTM архитектурно похожа на Y!LDA, за исключением использованного фреймворка для организации кластерного параллелизма (MPI вместо Hadoop).