
\subsection{PLSA}\label{plsa_alg}

$\quad\;\:$Вероятностный латентный семантический анализ (PLSA) был предложен Т.Хофманном в~\cite{hofmann_plsa}.

Принимается гипотеза условной независимости, утверждающая, что вероятность появления термина в данном документе зависит только от темы этого термина и не зависит от документа. Вероятностная порождающая модель PLSA имеет следующий вид: 
\begin{equation}\label{eq_generic}
	p(w|d) = \sum_{t \in T} p(w|t) p(t|d)
\end{equation}

PLSA можно реализовать с помощью ЕМ-алгоритма. Итерационный процесс состоит из двух шагов --- Е-шага (Expectation) и М-шага (Maximization). На Е-шаге по текущим значениям $\phi_{wt}$ и $\theta_{td}$ c помощью формулы Байеса вычисляются условные вероятности $p(t|d,w)$:
\[
	H_{dwt} = p(t|d,w) = \cfrac{\phi_{wt}\theta_{td}}{\sum_{s \in T}\phi_{ws}\theta_{sd}}
\]

На М-шаге по условным вероятностям $H_{dwt}$ вычисляются новые приближения параметров $\phi_{wt}$ и $\theta_{td}$. Используются указанные в предыдущем разделе формулы:
\[
	\phi_{wt} = \cfrac{n_{wt}}{n_t}, \quad
	\theta_{td} = \cfrac{n_{dt}}{n_d}, \quad	
\]

Однако данная версия алгоритма плохо подходит для задачи параллельной обработки бальших массивов данных по двум причинам:

\begin{itemize}
	\item За время одного прохода по коллекции распределения терминов в темах (матрица $\Phi$) успевают много раз сойтись, а распределния тем в документах проходят лишь одну итерацию.
	\item Трёхмерная вспогательная матрица $H_{dwt}$ становится чрезвычайно большой.
\end{itemize}

Избавиться от этих проблем позволяет т.н. пакетный онлайновый ЕМ-алгоритм. Пакетным он называется потому, что вся коллекция делится на блоки документов, обрабатываемых независимо. Онлайновость означает потоковую обработку корпуса, когда модель дообучается, получая на вход всё новые документы.

Помимо описанных преимуществ, данная модификация позволят также не хранить в явном виде матрицу $\Theta$. Вероятности $\theta_{td}$ не нужны по окончании обработки документа $d$, поэтому матрицу $(\theta_{td})_{T \times D}$ можно заменить вектором $(\theta_t)_T$.


{\bf Замечание:} Работа онлайнового алгоритма отличается для больших и малых коллекций. Для большой коллекции достаточно одного прохода по всем документам, в то время как маленькие требуют многократного прохода. В случаях малых и динамических коллекций (т.е.меняющих со временм свою тематику) значимость пакетов убывает по мере поступления новых, поэтому необходимо ввести параметр $\rho_j \in (0,1]$, отвечающий за скорость <<забывания>> старых оценок: 
\[
	n_{wt} := \rho_j n_{wt} + \tilde n_{wt}
\] 
где $\tilde n_{wt}$ --- новые полученные счётчики.

Алгоритм Online Batch PLSA реализует все описанные идеи. Он выглядит следующим образом:

\vspace{10pt}
\hrule 
{\bf Online Batch PLSA}
\vspace{4pt}

\hrule
\vspace{10pt}

\begin{algorithmic}[1]
\STATE инициализировать $\phi_{wt}$ для всех $w \in W$ и $t \in T$;
\STATE $n_{wt} := 0, n_t := 0$ для всех $w \in W$ и $t \in T$;
\FORALL{пакетов $D_j$, j = 1,...,J}
	\REPEAT
		\STATE $\tilde n_{wt} := 0, \tilde n_t := 0$ для всех $w \in W$ и $t \in T$;
		\FORALL{ $d \in D_j$}
			\STATE инициализировать $\theta_{td}$ для всех $t \in T$;
			\REPEAT
				\STATE $Z_w := \sum_{t \in T} \phi_{wt} \theta_{td}$ для всех $w \in d$;
				\STATE $\theta_{td} := \cfrac{1}{n_d} \sum_{w \in d} n_{dw} \phi_{wt} \theta_{td} / Z_w$ для всех $t \in T$;
			\UNTIL{$\theta_d$ не сойдётся};
			\STATE увеличить $\tilde n_{wt}, \tilde n_t$ на $n_{dw} \phi_{wt} \theta_{td} / Z_w$ для всех $w \in W$ и $t \in T$;
		\ENDFOR
		\STATE $\phi_{wt} := \cfrac{\rho_j n_{wt} + \tilde n_{wt}}{\rho_j n_{t} + \tilde n_{t}}$ для всех $w \in W$ и $t \in T$ таких, что $\tilde n_{wt} > 0$;
	\UNTIL{$\Phi$ не сойдутся};
	\STATE $n_{wt} := \rho_j n_{wt} + \tilde n_{wt}$ для всех $w \in W$ и $t \in T$;
	\STATE $n_t := \rho_j n_t + \tilde n_t$ для всех $t \in T$;
\ENDFOR
\end{algorithmic}

\vspace{10pt}
\hrule


Такой алгоритм хорошо параллелится. $\Theta$ выводится на нодах, $\Phi$ --- на мастере.

\subsection{Аддитивная регуляризация}

$\quad\;\:$Неоднозначность матричного разложения $F \approx \Theta \Phi$ даёт свободу выбора матриц из правой части равенства, позволяя наложить на тематическую модель дополнительные требования.  
Модифицируем максимизируемый функционал \ref{eq_1}:

\begin{equation}
	\quad L(\Phi,\Theta) + R(\Phi,\Theta) \rightarrow \max_{\Phi,\Theta}
\end{equation}	

\begin{equation}\label{reg_label}
 	R(\Phi,\Theta) = \sum_{i = 1}^{n} \tau_i R_i(\Phi,\Theta)
\end{equation}
где $R_i(\Phi,\Theta)$ --- дополнительные требования к модели, $\tau_i$ --- неотрицательные  коэффициенты регуляризации, выполнены условия неотрицательности и нормировки столбцов матриц $\Phi$ и $\Theta$.
 	 
Решение этой задачи приводит к обощению формул М-шага в ЕМ-алгоритме:
\begin{equation}
	\phi_{wt} = \cfrac{\left(n_{wt} + \phi_{wt} \cfrac{\partial R}{\partial \phi_{wt}} (\Phi,\Theta) \right)_+}{\sum_{u \in W} \left(n_{ut} + \phi_{ut} \cfrac{\partial R}{\partial \phi_{ut}} (\Phi,\Theta) \right)_+}, \quad 
 	\theta_{td} = \cfrac{\left(n_{dt} + \theta_{td} \cfrac{\partial R}{\partial \theta_{td}} (\Phi,\Theta) \right)_+}{\sum_{s \in T} \left(n_{ds} + \theta_{sd} \cfrac{\partial R}{\partial \theta_{sd}} (\Phi,\Theta) \right)_+}
\end{equation} 
 	 
 	 $n_{wt}$ и $n_{dt}$ определяются аналогично из формул предыдущего раздела.
 	 
Таким образом, суть добавления регуляризаторов --- в простом изменении формул М-шага.

{\bf Замечание:} Использование регуляризаторов требует аккуратного выстраивания т.н. траектории регуляризации. Этот процесс включает в себя настройку параметров, определение времени подключения/отключения того или иного регуляризатора, используемого в модели и т.п.