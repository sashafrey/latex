
\subsection{PLSA}\label{plsa_alg}

Вероятностный латентный семантический анализ (PLSA) был предложен Т.Хофманном в~\cite{hofmann_plsa}.

Принимается гипотеза условной независимости, утверждающая, что вероятность появления термина в данном документе зависит только от темы этого термина и не зависит от документа. Вероятностная порождающая модель PLSA имеет следующий вид: 
\begin{equation}\label{eq_generic}
	p(w|d) = \sum_{t \in T} p(w|t) p(t|d)
\end{equation}

PLSA можно реализовать с помощью ЕМ-алгоритма. Итерационный процесс состоит из двух шагов --- Е-шага (Expectation) и М-шага (Maximization). На Е-шаге по текущим значениям $\phi_{wt}$ и $\theta_{td}$ c помощью формулы Байеса вычисляются условные вероятности $p(t|d,w)$:
\[
	H_{dwt} = p(t|d,w) = \cfrac{\phi_{wt}\theta_{td}}{\sum_{s \in T}\phi_{ws}\theta_{sd}}
\]

На М-шаге по условным вероятностям $H_{dwt}$ вычисляются новые приближения параметров $\phi_{wt}$ и $\theta_{td}$. Используются указанные в предыдущем разделе формулы:
\[
	\phi_{wt} = \cfrac{n_{wt}}{n_t}, \quad
	\theta_{td} = \cfrac{n_{dt}}{n_d}, \quad	
\]

Однако данная версия алгоритма плохо подходит для задачи параллельной обработки бальших массивов данных по двум причинам:

\begin{itemize}
	\item За время одного прохода по коллекции распределения терминов в темах (матрица $\Phi$) успевают много раз сойтись, а распределния тем в документах проходят лишь одну итерацию.
	\item Трёхмерная вспогательная матрица $H_{dwt}$ становится чрезвычайно большой.
\end{itemize}

Избавиться от этих проблем позволяет т.н. пакетный онлайновый ЕМ-алгоритм. Пакетным он называется потому, что вся коллекция делится на блоки документов, обрабатываемых независимо. Онлайновость означает потоковую обработку коллекции, когда модель дообучается, получая на вход всё новые документы.

Помимо описанных преимуществ, данная модификация позволят также не хранить в явном виде матрицу $\Theta$. Вероятности $\theta_{td}$ не нужны по окончании обработки документа $d$, поэтому матрицу $(\theta_{td})_{T \times D}$ можно заменить вектором $(\theta_t)_T$.


{\bf Замечание:} Работа онлайнового алгоритма отличается для больших и малых коллекций. Для большой коллекции достаточно одного прохода по всем документам, в то время как маленькие требуют многократного прохода. В случаях малых и динамических коллекций (т.е.меняющих со временм свою тематику) значимость пакетов убывает по мере поступления новых, поэтому необходимо ввести параметр $\rho_j \in (0,1]$, отвечающий за скорость <<забывания>> старых оценок: 
\[
	\hat n_{wt} := \rho_j \hat n_{wt} + \tilde n_{wt}
\] 

Алгоритм Online Batch PLSA реализует все описанные идеи. Он выглядит следующим образом:

ЗДЕСЬ БУДЕТ АЛГОРИТМ ИЗ voron2013ptm.
\marginpar{ToDo}

Такой алгоритм хорошо параллелится. $\Theta$ выводится на нодах, $\Phi$ --- на мастере.

\subsection{Аддитивная регуляризация}

Неоднозначность матричного разложения $F \approx \Theta \Phi$ даёт свободу выбора матриц из правой части равенства, позволяя наложить на тематическую модель дополнительные требования.  
Модифицируем максимизируемый функционал \ref{eq_1}:

\begin{equation}
	\quad L(\Phi,\Theta) + R(\Phi,\Theta) \rightarrow \max_{\Phi,\Theta}
\end{equation}	

\[
 	R(\Phi,\Theta) = \sum_{i = 1}^{n} \tau_i R_i(\Phi,\Theta)
\]	
где $R_i(\Phi,\Theta)$ --- дополнительные требования к модели, $\tau_i$ --- неотрицательные  коэффициенты регуляризации, выполнены условия неотрицательности и нормировки столбцов матриц $\Phi$ и $\Theta$.
 	 
Решение этой задачи приводит к обощению формул М-шага в ЕМ-алгоритме:
\begin{equation}
	\phi_{wt} = \cfrac{\left(n_{wt} + \phi_{wt} \cfrac{\partial R}{\partial \phi_{wt}} (\Phi,\Theta) \right)_+}{\sum_{u \in W} \left(n_{ut} + \phi_{ut} \cfrac{\partial R}{\partial \phi_{ut}} (\Phi,\Theta) \right)_+}, \quad 
 	\theta_{td} = \cfrac{\left(n_{dt} + \theta_{td} \cfrac{\partial R}{\partial \theta_{td}} (\Phi,\Theta) \right)_+}{\sum_{s \in T} \left(n_{ds} + \theta_{sd} \cfrac{\partial R}{\partial \theta_{sd}} (\Phi,\Theta) \right)_+}
\end{equation} 
 	 
 	 $n_{wt}$ и $n_{dt}$ определяются аналогично из формул предыдущего раздела.
 	 
Таким образом, суть добавления регуляризаторов --- в простом изменении формул М-шага.

{\bf Замечание:} Использование регуляризаторов требует аккуратного выстраивания т.н. траектории регуляризации. Этот процесс включает в себя настройку параметров, определение времени подключения/отключения того или иного регуляризатора, используемого в модели и т.п.

\subsection{Регуляризатор Дирихле}

Рассмотрим популярную на сегодняшний день модель обучения LDA (latent Dirichlet allocation). Она основана на разложении \ref{eq_generic} при дополнительном предположении, что векторы документов $\theta_d = (\theta_{td} \in {\bf R}^{|T|})$ и векторы тем $\phi_t = (\phi_{wt} \in {\bf R}^{|W|})$ порождаются распределениями Дирихле с параметрами $\alpha \in {\bf R}^{|T|}$ и $\beta \in {\bf R}^{|W|}$ соответственно. В работе \cite{voron2013ptm} показано, что отличие LDA от PLSA --- в сглаживании $\phi_{wt}$ и $\theta_{td}$:

\begin{equation}
	\phi_{wt} = \cfrac{\hat n_{wt} + \beta_w}{\hat n_t + \beta_0}, \quad 
 	\theta_{td} = \cfrac{\hat n_{td} + \alpha_t}{\hat n_d + \alpha_0}, \quad
	\alpha_0 = \sum_t \alpha_t, \; \beta_0 = \sum_w \beta_w
\end{equation}

Т.е. LDA --- это PLSA со встроенным регуляризатором сглаживания (здесь и далее --- регуляризатор Дирихле). Сглаживание может быть полезным для некоторых столбцов матриц, но в общем случае оно противоречит тому, что матрицы $\Phi$ и $\Theta$ должны быть сильно разреженными. 