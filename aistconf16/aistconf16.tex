\documentclass[russian,english]{llncs}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[final]{graphicx}
\usepackage{epstopdf}
\usepackage[labelsep=period]{caption}
\usepackage[hyphens]{url}
\usepackage{amssymb,amsmath,mathrsfs}
\usepackage[russian,english]{babel}
%\usepackage{multicol}
\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
%\usepackage{algorithm}
%\usepackage[noend]{algorithmic}
\usepackage{color}
\usepackage{cmap}
\usepackage{array}
\usepackage{tikz}
\usepackage{pgfplots}
%\usepackage{verbatim}
\usepackage{standalone}

\tolerance=1000
\hbadness=5000
\newcommand{\const}{\mathrm{const}}
\newcommand{\tsum}{\mathop{\textstyle\sum}\limits}
\newcommand{\tprod}{\mathop{\textstyle\prod}\limits}
\newcommand{\cov}{\mathop{\rm cov}\limits}
\newcommand{\Dir}{\mathop{\rm Dir}\nolimits}
\newcommand{\norm}{\mathop{\rm norm}\limits}
\newcommand{\KL}{\mathop{\rm KL}\nolimits}
%\renewcommand{\geq}{\geqslant}
%\renewcommand{\leq}{\leqslant}
\newcommand{\eps}{\varepsilon}
\newcommand{\cond}{\mspace{3mu}{|}\mspace{3mu}}
\newcommand{\Loss}{\mathscr{L}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\cL}{\mathscr{L}}
\newcommand{\cP}{\mathscr{P}}
\newcommand{\kw}[1]{\textsf{#1}}
\SetKwFor{ForAll}{\textbf{for all}}{}{}

%... and these rows too.
\pgfplotsset{ every non boxed x axis/.append style={x axis line style=-},
     every non boxed y axis/.append style={y axis line style=-}}
\pgfplotsset{compat = 1.3}

\begin{document}
%%Analysis of Images, Social Networks, and Texts
\title{
    Parallel Non-blocking Deterministic Algorithm for Online Topic Modeling
}
\author{
    Oleksandr Frei\inst{1}
    \and
    Murat Apishev\inst{2}
}
\institute{\noindent
    Schlumberger Information Solutions,
    ~~\email{oleksandr.frei@gmail.com}
    \and
    Lomonosov Moscow State University,
    ~~\email{great-mel@yandex.ru}
}

\maketitle

\begin{abstract}
In this paper we present a new asynchronous algorithm for learning additively regularized topic models
and discuss the main architectural details of our implementation.
The key property of the new algorithm is that it behaves in a fully deterministic fashion,
which is typically hard to achieve in a non-blocking parallel implementation. The algorithm
%has been
had been
recently implemented in the BigARTM library (\texttt{http://bigartm.org}).
Our new algorithm is compatible with all features previously introduced in BigARTM library,
including multimodality, regularizers and scores calculation.
Previous BigARTM version was already faster than the alternative open source topic modeling packages such as Vowpal Wabbit or Gensim.
Yet, in our experiments we show that the new implementation spends even less time to achieve the same perplexity, has better CPU utilization and lower memory usage.

\vspace{1em}
\textbf{Keywords:}
    probabilistic topic modeling,
    Probabilistic Latent Sematic Analysis,
    Latent Dirichlet Allocation,
    Additive Regularization of Topic Models,
    stochastic matrix factorization,
    EM-algorithm,
    online learning,
    asynchronous and parallel computing,
    BigARTM.
\end{abstract}

\section{Introduction}

Deterministic behavior is an important property for any algorithm,
including those of a stochastic nature.
For the end users of a software system run-to-run reproducibility is a must-have property,
because this is what they expect based on their previous experience.
Indeed, refreshing a web-pages or re-doing an operation tend to
produce an identical result, regardless of how much software complexity is hidden behind the scenes.
For the researches determinism is also important
because it enables them to reproduce their old experiments
and study impact of various parameters on the result.
Finally, for the developers of the algorithm
determinism allow to reproduce bugs and write simple unit-tests with well-defined results.

Determinism is particularly hard to achieve
in concurrent implementations, because in a multi-threaded environment
it might not be sufficient to just fix a random seed or an initial approximation.
In this paper we present a deterministic modification of parallel non-blocking algorithm
for online topic modeling, previously developed for BigARTM library.
We implement new algorithm in BigARTM and demonstrate that new version converges faster
than previous algorithm in terms of perplexity,
yet being more efficient in CPU and memory usage.

The rest of the paper is organized as follows.
In~section~\ref{sec:Notation}
we introduce basic notation used throughout this paper.
In~section~\ref{sec:Previous}
we summarize the previous algorithm for online topic modeling, used in $BigARTM v0.6$.
In~section~\ref{sec:Algorithm}
we~present a deterministic non-blocking modification of the algorithm.
In~section~\ref{sec:Architecture}
we~describe the architecture, implemented in $BigARTM v0.7$.
In~section~\ref{sec:Experiments}
we~report results of our experiments on large datasets.
In~section~\ref{sec:Conclusions}
we~discuss advantages, limitations and open problems of BigARTM.

%sashafrey:
%really technical topics in my opinion does not belong to AISTconf article;
%it would be more appropriate to discuss the following in the full 12-page article in http://www.ispras.ru/en/ journal
%\begin{itemize}
%    \item Details of CLI interface and python interface, usage examples
%    \item List new features: coherence score and regularizer, classification, documents markdown (aka $p_{tdw}$ matrices)
%    \item Our technologies (Protobuf for low-level C API, Boost Serialize for import/export, GLog, GFlags, GTest, etc)
%    \item Our build solution and CI solution (CMake, Visual Studio, GitHub, Git submodules, Travis, Apveyour, Read-The-Docs)
%    \item Why people care about run-to-run reproducibility
%    https://software.intel.com/en-us/articles/consistency-of-floating-point-results-using-the-intel-compiler
%\end{itemize}

\section{Notation}
\label{sec:Notation}

Let
$D$ denote a finite set (collection) of texts and
$W$ denote a~finite set (vocabulary) of all terms from these texts.
Let
$n_{dw}$ denote the number of occurrences of a term $w \in W$ in a document $d \in D$;
$n_{dw}$ values form a sparse matrix of size $|W| \times |D|$,
known as \emph{bag-of-words} representation of the collection.

Given an $(n_{dw})$ matrix, an additively-regularized topic model (ARTM) finds two matrices:
$\Phi = \{\phi_{wt}\}$ and $\Theta = \{\theta_{td}\}$,
of sizes $|W| \times |T|$ and $|T| \times |D|$ respectively,
where $|T|$ is a used-defined number of \emph{topics} in the model.
Matrices $\Phi$ and $\Theta$
provide a compressed representation of the $(n_{dw})$ matrix:
\[
n_{dw} \approx n_d \sum_{t \in T} \phi_{wt} \theta_{td}, \text { for all } d \in D, w \in W,
\]
where $n_d = \sum_{w \in W} n_{dw}$ denotes the total number of terms in a document $d$.
%Matrices $\Phi$ and $\Theta$ are \emph{stochastic}, e.g. their elements are non-negative and all columns are normalized to unity.
%In the sequel we will use ``norm'' operator to transform
%a~real vector into a discrete distribution.
%For example some arbitrary positive values $n_{td}$ and $n_{wt}$
%can be turned into $\theta_{td}$ and $\phi_{wt}$ as follows:
%\begin{align}
%    \theta_{td} = \norm_{t\in T} n_{td} = n_{td} \big/ \sum_{s\in T} n_{sd}; \\
%    \phi_{wt} = \norm_{w \in W} n_{wt} = n_{wt} \big/ \sum_{v \in W} n_{vt}.
%\end{align}

To learn $\Phi$ and $\Theta$ from $(n_{dw})$ ARTM maximizes
the log-likelihood, regularized via an additional penalty term $R(\Phi, \Theta)$:
\begin{gather}
\label{eq:ARTM}
    \sum_{d\in D}\sum_{w\in W} n_{dw} \ln \sum_{t\in T} \phi_{wt} \theta_{td} + R(\Phi, \Theta)
    \;\to\; \max_{\Phi,\Theta}.
\end{gather}
With no regularization (${R=0}$) it corresponds to PLSA~\cite{hofmann99plsi}.
Many Bayesian topic models can be considered
as special cases of ARTM with different regularizers~$R$,
as shown in~\cite{voron14mlj,voron14aist}.

In \cite{voron14dan-eng} it is shown that the \mbox{local} maximum $(\Phi,\Theta)$
of the problem~\eqref{eq:ARTM} satisfies
\begin{align}
    \label{eq:Estep}
    p_{tdw} &= \norm_{t\in T} \bigl(\phi_{wt}\theta_{td}\bigr);
\\
    \label{eq:Mstep:phi}
    \phi_{wt} &= \norm_{w\in W}
        \biggl(
            \sum_{d\in D} n_{dw} p_{tdw} + \phi_{wt} \frac{\partial R}{\partial \phi_{wt}}
        \biggr);
%    \phi_{wt} &= \norm_{w\in W}
%        \biggl(
%            n_{wt} + r_{wt}
%        \biggr);
%    \quad
%    n_{wt} = \sum_{d\in D} n_{dw} p_{tdw};
%    \quad
%    r_{wt} = \phi_{wt} \frac{\partial R}{\partial \phi_{wt}};
\\
    \label{eq:Mstep:theta}
    \theta_{td} &= \norm_{t\in T}
        \biggl(
            \sum_{w\in d} n_{dw} p_{tdw} + \theta_{td} \frac{\partial R}{\partial \theta_{td}}
        \biggr);
%    \theta_{td} &= \norm_{t\in T}
%        \biggl(
%            n_{td} + r_{td}
%        \biggr);
%    \quad
%    n_{td} = \sum_{w\in d} n_{dw} p_{tdw};
%    \quad
%    r_{td} = \theta_{td} \frac{\partial R}{\partial \theta_{td}};
        %\sum_{m=1}^M \tau_m \!\!\sum_{w\in W^m}\!\!\! n_{dw} p_{tdw};
\end{align}
where operator
$\norm_{i \in I} x_i = \frac{\max\{x_i,0\}}{\sum\limits_{j\in I} \max\{x_j,0\}}$
transforms a~vector $(x_i)_{i \in I}$ to a~discrete distribution.

Learning of $\Phi$ and $\Theta$ from \eqref{eq:Estep}--\eqref{eq:Mstep:theta} can be done by EM-algorithm,
which starts from a random initial approximation and iterates
E-step \eqref{eq:Estep} and
M-steps \eqref{eq:Mstep:phi},\eqref{eq:Mstep:theta}
until convergence.
In the sequel we discuss several variations of such EM-algorithm,
which are all based on the above formulas but differ in the way how operations are ordered and grouped together.

%In additional to plain text many collections has additional data,
%such as authors, class or category labels, date-time stamps.
%In \cite{vfardy15cikmtm} this data can be represented as \emph{modalities},
%where the overall vocabulary $W$ is split into $M$ subsets
%$W = W^1 \sqcup \dots \sqcup W^M$, one subset per modality,
%and $\Phi$ matrix is normalized independently within each modality:
%\begin{align}
%    \sum_{w \in W^m} \phi_{wt} = 1, \; \text{ for all } t \in T, m = 1, \dots, M \\\notag
%\end{align}


\section{Previous algorithms}
\label{sec:Previous}

\SetAlgoSkip{}
\begin{algorithm2e}[t]
\caption{Offline algorithm}
\label{alg:Offline}
\BlankLine
\KwIn{collection $D$;}
\KwOut{matrix $\Phi = (\phi_{wt})$;}
\BlankLine
initialize $(\phi_{wt})$\;
\Repeat{$(\phi_{wt})$ converges}{
    %$\tilde n_{wt} := 0$ for all $w \in W$ and $t \in T$\;
    %\ForAll{documents $d \in D$} {
    %    $(\tilde n_{wt}) := (\tilde n_{wt}) + \kw{ProcessDocument}(d, \Phi)$\;
    %}
    $(n_{wt}) := \mathlarger\sum\limits_{d \in D} \; \kw{ProcessDocument}(d, \Phi)$\;
    $(\phi_{wt}) := \norm_{w \in W} (n_{wt} + \phi_{wt} \frac{\partial R}{\partial \phi_{wt}})$\;
}
\end{algorithm2e}
\begin{algorithm2e}[t]
\caption{\kw{ProcessDocument}($d, \Phi$)}
\label{alg:ProcessDocument}
\BlankLine
\KwIn{document $d \in D$, matrix $\Phi=(\phi_{wt})$;}
\KwOut{matrix $(\tilde n_{wt})$;}
\BlankLine
initialize $\theta_{td} := \frac{1}{|T|}$ for all $t \in T$\;
\Repeat{$\theta_d$ converges}{
    $p_{tdw} := \norm_{t\in T} \bigl(\phi_{wt}\theta_{td}\bigr)$ for all $w\in d$ and $t \in T$\;
    $\theta_{td} := \norm_{t\in T}
        \bigl(
            \sum_{w\in d} n_{dw} p_{tdw} + \theta_{td} \frac{\partial R}{\partial \theta_{td}}
        \bigr)$ for all $t \in T$\;
}
$\tilde n_{wt} := n_{dw} p_{tdw}$ for all $w \in d$ and $t \in T$\;
\end{algorithm2e}


Offline EM-Algorithm
(see Algorithm \ref{alg:Offline} and \ref{alg:ProcessDocument}).
Key step of the algorithm is $\kw{ProcessDocument}$ subroutine
that generates a matrix $\hat n_{wt}$ of size $n_d \times |T|$.
These values describe contribution of the tokens from the document into the final $\Phi$ matrix.
Then the offline algorithm aggregates $\hat n_{wt}$ values across all documents in the collection.
After normalization they form a new $\Phi$ matrix for the next iteration.
Note that $\theta_{td}$ values appear only within $\kw{ProcessDocument}$ subroutine.
This makes the algorithm efficient in its memory usage,
allowing implementation to not store the entire theta matrix at any given time.
Instead, $\theta_{td}$ values are recalculated from scratch on every pass through the collection.

To improve convergence rate of the algorithm
the collection is split into ``batches'',
$D := D_1 \sqcup D_2 \sqcup \dots \sqcup D_B$,
and the algorithm is adjusted so that
matrix $\Phi$
is re-calculated after every $\eta$ batches.
This leads to more frequent updates of $\Phi$ matrix, which are performed as the algorithm scans through the collection.
To simplify the notation
we introduce a trivial subroutine
\[
\kw{ProcessBatches}(\{D_b\}, \Phi) = \mathlarger\sum\limits_{D_b} \mathlarger\sum\limits_{d \in D_b} \; \kw{ProcessDocument}(d, \Phi) \\
\]
that aggregates the output of $\kw{ProcessDocument}$ across specific batches at a constant $\Phi$ matrix.
The algorithm is then given by listing \ref{alg:Online}.

Key difference is that the cumulative sum $n_{wt}$ is discounted by a factor $\rho_i < 1$,
which depends on the iteration. Typical strategy is to use $\rho_i = (\tau_0 + i)^{-\kappa}$,
where typical values for $\tau_0$ are between $64$ and $1024$, for $\kappa$ --- between $0.5$ and $0.7$.

\SetAlgoSkip{}
\begin{algorithm2e}[h]
\caption{Online algorithm} %\mbox{regularized} topic modeling
\label{alg:Online}
\BlankLine
\KwIn{collection $D$, parameters $\eta, \tau_0, \kappa$;}
\KwOut{matrix $\Phi = (\phi_{wt})$;}
\BlankLine
form batches $D := D_1 \sqcup D_2 \sqcup \dots \sqcup D_B$\;
initialize $(\phi^0_{wt})$\;
\ForAll{update $i = 1, \dots, \lfloor B / \eta \rfloor$} {
    $(\hat n^i_{wt}) := \kw{ProcessBatches}(\{D_{\eta (i - 1) + 1}, \dots, D_{\eta i}\}, \Phi^{i - 1})$\;
    $\rho_i := (\tau_0 + i)^{-\kappa}$\;
    $(n^{i}_{wt}) := \rho_i \cdot (n^{i-1}_{wt}) + (1 - \rho_i) \cdot (\hat n^{i}_{wt})$\;
    $(\phi^{i}_{wt}) := \norm_{w \in W} (n^{i}_{wt} + \phi^{i - 1}_{wt} \frac{\partial R}{\partial \phi_{wt}})$\;
}
\end{algorithm2e}

A reasonable approach to speedup the online algorithm is to introduce concurrency in the $\kw{ProcessBatches}$ subroutine.
Indeed, multiple threads can simply process batches $D_{\eta (i - 1) + 1}, \dots, D_{\eta i}$ in parallel,
given that they synchronize writes $\hat n_{wt}$ values into the resulting matrix.
The problem with this approach is that all threads will have no useful work to do during steps $5$, $6$ and $7$ of the algorithm.
The threads can not start processing the next batches because a new version of $\Phi$ matrix is not ready yet.
In the next section we present a simple modification of the online algorithm that turns it into an asynchronous non-blocking algorithm.

\section{Asynchronous online algorithm}
\label{sec:Algorithm}

\SetAlgoSkip{}
\begin{algorithm2e}[h]
\caption{Asynchronous online algorithm} %\mbox{regularized} topic modeling
\label{alg:Online}
\BlankLine
\KwIn{collection $D$, parameters $\eta, \tau_0, \kappa$;}
\KwOut{matrix $\Phi = (\phi_{wt})$;}
\BlankLine
form batches $D := D_1 \sqcup D_2 \sqcup \dots \sqcup D_B$\;
initialize $(\phi^0_{wt})$\;
$(\hat n^1_{wt}) := \kw{AsyncProcessBatches}(\{D_{1}, \dots, D_{\eta}\}, \Phi^0)$\;
\ForAll{update $i = 1, \dots, \lfloor B / \eta \rfloor - 1$} {
    $(\hat n^{i+1}_{wt}) := \kw{AsyncProcessBatches}(\{D_{\eta i + 1}, \dots, D_{\eta i + \eta}\}, \Phi^{i-1})$\;
    $\kw{Await}(\hat n^i_{wt})$\;
    $\rho_i := (\tau_0 + i)^{-\kappa}$\;
    $(n^{i}_{wt}) := \rho_i \cdot (n^{i-1}_{wt}) + (1 - \rho_i) \cdot (\hat n^{i}_{wt})$\;
    $(\phi^{i}_{wt}) := \norm_{w \in W} (n^{i}_{wt} + \phi^{i-1}_{wt} \frac{\partial R}{\partial \phi_{wt}})$\;
}
\end{algorithm2e}

Adding delay and grouping several batches into a batchset negatively impact convergence,
but let utilize full CPU capacity of the machine.
In the experiments we demonstrate that on large datasets the resulting algorithm
is able to achieve better performance quicker that non-parallel or synchronous algorithm.

\section{New architecture}
\label{sec:Architecture}

TBD: explain how \ref{Online} is parallelized in the previous BigARTM version.
Explain why it was non-deterministic.
Explain that non-deterministic version has negative impact on convergence.

In the new architecture we removed DataLoader thread,
which previously was responsible for loading batches from disk.
In the new architecture data loading happens directly from each processor thread.
This simplified the architecture without any lose in performance.

We also removed Merger thread, which previously was responsible
for merging $\hat n_{wt}$ --- model increments, produced on individual batches.
In a new architecture all increments are added to the final $\hat n_{wt}$ matrix
concurrently from processor threads.
To synchronize write access to this data structure we require that
no threads simultaneously update the same row in $\hat n_{wt}$ matrix.
Thus, the data for distinct words could be updated in parallel.
To enforce this behaviour we create one spin lock, $l_w$, for each word in global dictionary $W$.
After processing a batch the processor threads loops through local batch dictionary,
and for each $w \in W_b$ acquire the corresponding lock $l_w$.
%ToDo: measure collision ratio.
His approach of aggregating results across threads is taken from \cite{smola10architecture},
where the same pattern was used to update a shared stated in distributed topic modeling architecture.
In our case the same idea is applied to aggregating data in shared memory.

\includegraphics[natwidth=501bp,natheight=235bp,width=270bp]{old_arch.png}

\includegraphics[natwidth=394bp,natheight=206bp,width=270bp]{new_arch.png}

\section{Experiments}
\label{sec:Experiments}

\section{Conclusions}
\label{sec:Conclusions}

TBD

\bigskip
\subsubsection*{Acknowledgements.}

TBD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{splncs03}
%\bibliography{MachLearn}

\begin{thebibliography}{10}
%\providecommand{\url}[1]{\texttt{#1}}
%\providecommand{\urlprefix}{URL }

TBD

\end{thebibliography}

\end{document}

