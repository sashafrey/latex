
<!-- saved from url=(0038)http://www.rulequest.com/see5-win.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>See5: An Informal Tutorial</title>

<style type="text/css">
<!--
.out {	background:#FFFFDD;
	font-size:smaller;
	padding:2ex;
	margin-left:8ex;
	margin-right:8ex;
	margin-top:2ex;
	margin-bottom:2ex;
	border-style:solid;
	border-width:1px;
	border-color:#000000 }
-->
</style>

<script>window["_GOOG_TRANS_EXT_VER"] = "1";</script></head>

<body bgcolor="white">

<h1> <font color="navy"> See5: An Informal Tutorial </font></h1>

<p>
Welcome to See5, a system that extracts informative patterns from data.
The following sections show how to prepare data files for See5
and illustrate the options for using the system.

</p><p>
In this tutorial, file names and See5 input appear in
<font color="#0000AA"><code>blue fixed-width font</code></font>
while file extensions and other general forms
are shown <font color="green" style="background: #EEEEEE">highlighted in green</font>.
Buttons and options on the Windows GUI are in <font color="maroon">maroon</font>.

</p><ul>
<li><b><a href="http://www.rulequest.com/see5-win.html#DATA">Preparing Data for See5</a></b>
  <ul>
  <li><a href="http://www.rulequest.com/see5-win.html#files">Application files</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#.names">Names file</a>
    <ul>
    <li><a href="http://www.rulequest.com/see5-win.html#name">What's in a name?</a>
    </li><li><a href="http://www.rulequest.com/see5-win.html#classes">Specifying the classes</a>
    </li><li><a href="http://www.rulequest.com/see5-win.html#explicit">Explicitly-defined attributes</a>
    </li><li><a href="http://www.rulequest.com/see5-win.html#implicit">Attributes defined by formulas</a>
    </li><li><a href="http://www.rulequest.com/see5-win.html#dates">Dates, times, and timestamps</a>
    </li><li><a href="http://www.rulequest.com/see5-win.html#select">Selecting the attributes that can appear in classifiers</a>
    </li></ul>
  </li><li><a href="http://www.rulequest.com/see5-win.html#.data">Data file</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#.test">Test and cases files (optional)</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#.costs">Costs file (optional)</a>
  </li></ul>
</li><li><b><a href="http://www.rulequest.com/see5-win.html#GUI">User Interface</a></b>
</li><li><b><a href="http://www.rulequest.com/see5-win.html#CLASSIFIERS">Constructing Classifiers</a></b>
  <ul>
  <li><a href="http://www.rulequest.com/see5-win.html#TREES">Decision trees</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#EVALUATION">Evaluation</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#SUBSETS">Discrete value subsets</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#RULES">Rulesets</a>
    <ul>
    <li><a href="http://www.rulequest.com/see5-win.html#utility">Rule utility ordering</a>
    </li></ul>
  </li><li><a href="http://www.rulequest.com/see5-win.html#BOOSTING">Boosting</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#WINNOWING">Winnowing attributes</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#FUZZY">Softening thresholds</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#OTHER">Advanced pruning options</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#SAMPLE">Sampling from large datasets</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#XVAL">Cross-validation trials</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#COSTS">Differential misclassification costs</a>
  </li><li><a href="http://www.rulequest.com/see5-win.html#CASEWEIGHT">Weighting individual cases</a>
  </li></ul>
</li><li><b><a href="http://www.rulequest.com/see5-win.html#USE">Using Classifiers</a></b>
</li><li><b><a href="http://www.rulequest.com/see5-win.html#XREF">Cross-Referencing Classifiers and Data</a></b>
</li><li><b><a href="http://www.rulequest.com/see5-win.html#CONSOLE">Generating Classifiers in Batch Mode</a></b>
</li><li><b><a href="http://www.rulequest.com/see5-win.html#HOOKS">Linking to Other Programs</a></b>
</li></ul>

<hr>

<p>
</p><h2><a name="DATA">Preparing Data for See5</a></h2>

<p>
We will illustrate See5 using a medical application -- mining a database
of thyroid assays from the Garvan Institute of Medical Research,
Sydney, to construct diagnostic rules for hypothyroidism.
Each case concerns a single referral and contains information on the
source of the referral, assays requested, patient data, and
referring physician's comments.
Here are three examples:

</p><pre style="font-size:smaller">        <b><u>Attribute</u>                 <u>Case 1</u>    <u>Case 2</u>    <u>Case 3</u>    .....</b>

	age                       41        23        46
	sex                       F         F         M
	on thyroxine              f         f         f
	query on thyroxine        f         f         f
	on antithyroid medication f         f         f
	sick                      f         f         f
	pregnant                  f         f         not applicable
	thyroid surgery           f         f         f
	I131 treatment            f         f         f
	query hypothyroid         f         f         f
	query hyperthyroid        f         f         f
	lithium                   f         f         f
	tumor                     f         f         f
	goitre                    f         f         f
	hypopituitary             f         f         f
	psych                     f         f         f
	TSH                       1.3       4.1       0.98
	T3                        2.5       2         unknown
	TT4                       125       102       109
	T4U                       1.14      unknown   0.91
	FTI                       109       unknown   unknown
	referral source           SVHC      other     other
	<font color="red">diagnosis                 negative  negative  negative</font>
	ID                        3733      1442      2965
</pre>

<p>
This is exactly the sort of task for which See5 was designed.
Each case belongs to one of a small number of mutually exclusive classes
(negative, primary, secondary, compensated).
Properties of every case that <i>may</i> be relevant to its
class are provided, although some cases may
have unknown or non-applicable values for some attributes.
There are 24 attributes in this example, but See5 can deal with
any number of attributes.

</p><p>
See5's job is to find how to predict a case's class from
the values of the other attributes.
See5 does this by constructing a <i>classifier</i> that makes
this prediction.  As we will see, See5 can construct classifiers expressed
as <i>decision trees</i> or as sets of <i>rules</i>.

</p><h3><a name="files">Application files</a></h3>

Every See5 application has a short name called
a <font color="green" style="background: #EEEEEE">filestem</font>;
we will use the filestem <font color="#0000AA"><code>hypothyroid</code></font>
for this illustration.
All files read or written by See5 for an application
have names of the form
<font color="green" style="background: #EEEEEE">filestem</font><font color="#0000AA"><code>.</code></font><font color="green" style="background: #EEEEEE">extension</font>,
where <font color="green" style="background: #EEEEEE">filestem</font> identifies the application and
<font color="green" style="background: #EEEEEE">extension</font> describes the contents of the file.

<p>
Here is a summary table of the extensions used by See5 (to
be described in later sections):

</p><p>
</p><ul><ul>
<table cellpadding="5">
<tbody><tr>
  <td><font color="green" style="background: #EEEEEE">names</font>
  </td><td>description of the application's attributes
  </td><td>[required]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">data</font>
  </td><td>cases used to generate a classifier
  </td><td>[required]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">test</font>
  </td><td>unseen cases used to test a classifier
  </td><td>[optional]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">cases</font>
  </td><td>cases to be classified subsequently
  </td><td>[optional]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">costs</font>
  </td><td>differential misclassification costs
  </td><td>[optional]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">tree</font>
  </td><td>decision tree classifier produced by See5
  </td><td>[output]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">rules</font>
  </td><td>ruleset classifier produced by See5
  </td><td>[output]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">out</font>
  </td><td>report produced when a classifier is generated
  </td><td>[output]
</td></tr>
<tr>
  <td><font color="green" style="background: #EEEEEE">set</font>
  </td><td>settings used for the last classifier
  </td><td>[output]
</td></tr>
</tbody></table>
</ul></ul>

The case of letters in both the filestem and extension is important --
file names <font color="#0000AA"><code>APP.DATA</code></font>,
<font color="#0000AA"><code>app.data</code></font>, and <font color="#0000AA"><code>App.Data</code></font>, are all different.
The extensions must be written
in lower case as shown above, otherwise See5 will not recognize
the files for your application.

<p>
If See5 cannot seem to find your files even though the filestem
and extensions are correct, please
check that file extensions are not hidden on your computer.
(If extensions are hidden and you write a text file from Wordpad,
it automatically adds an extension <font color="#0000AA"><code>.txt</code></font> that makes the file
invisible to See5.)
Here's what to do:
</p><ul>Double click "My Computer," select "Tools" (or "View" for Windows 98),
then "Folder Options" and then the "View" tab.
The box "Hide file extensions for known file types" should not be checked.
If it is, uncheck it and click "Apply."</ul>

<h3><a name=".names">Names file</a></h3>

Two files are essential for all See5 applications and there are three
further optional files, each identified by its extension.
The first essential file is
the <font color="green" style="background: #EEEEEE">names</font> file (e.g.
<font color="#0000AA"><code>hypothyroid.names</code></font>) that
describes the attributes and classes.
There are two important subgroups of attributes:
<ul>
<li>
The value of an <font color="green" style="background: #EEEEEE">explicitly-defined attribute</font> is given directly
in the data.
A <em>discrete</em> attribute has a value drawn from
a set of nominal values, a <em>continuous</em> attribute has a
numeric value, a <em>date</em> attribute holds a calendar date,
a <em>time</em> attribute holds a clock time,
a <em>timestamp</em> attribute holds a date and time,
and a <em>label</em> attribute serves only to identify a particular case.
</li><li>
The value of an <font color="green" style="background: #EEEEEE">implicitly-defined attribute</font>
is specified by a formula.
(Most attributes are explicitly defined, so you may never need
implicitly-defined attributes.)
</li></ul>

<p>
The file <font color="#0000AA"><code>hypothyroid.names</code></font>
looks like this:

</p><pre class="out">diagnosis.                     | the target attribute

age:                           continuous.
sex:                           M, F.
on thyroxine:                  f, t.
query on thyroxine:            f, t.
on antithyroid medication:     f, t.
sick:                          f, t.
pregnant:                      f, t.
thyroid surgery:               f, t.
I131 treatment:                f, t.
query hypothyroid:             f, t.
query hyperthyroid:            f, t.
lithium:                       f, t.
tumor:                         f, t.
goitre:                        f, t.
hypopituitary:                 f, t.
psych:                         f, t.
TSH:                           continuous.
T3:                            continuous.
TT4:                           continuous.
T4U:                           continuous.
FTI:=                          TT4 / T4U.
referral source:               WEST, STMW, SVHC, SVI, SVHD, other.

diagnosis:                     primary, compensated, secondary, negative.

ID:                            label.
</pre>

<h4><a name="name">What's in a name?</a></h4>

Names, labels, classes, and discrete values are represented by arbitrary
strings of characters, with some fine print:
<ul>
<li> Tabs and spaces are permitted inside a name or value, but See5
collapses every sequence of these characters to a single space.
</li><li> Special characters (comma, colon, period, vertical bar `<font color="#0000AA"><code>|</code></font>')
can appear
in names and values, but must be prefixed by the escape character
`<font color="#0000AA"><code>\</code></font>'.
For example, the name "Filch, Grabbit, and Co." would be written
as <nobr>`<font color="#0000AA"><code>Filch\, Grabbit\, and Co\.</code></font>'</nobr>.
(Colons in times and periods in numbers do not need to be escaped.)
</li></ul>
Whitespace (blank lines, spaces, and tab characters) is ignored
except inside a name or value and can be used to improve legibility.
Unless it is escaped as above,
the vertical bar `<font color="#0000AA"><code>|</code></font>'
causes the remainder of the line to be ignored and is handy for
including comments.
This use of `<font color="#0000AA"><code>|</code></font>' should not occur inside a value.


<h4><a name="classes">Specifying the classes</a></h4>

The first entry in the <font color="green" style="background: #EEEEEE">names</font> file specifies the classes
in one of three formats:
<ul>
<li> A list of class names separated by commas, e.g.
  <ul>
	<font color="#0000AA"><code>primary, compensated, secondary, negative.</code></font>
  </ul>
</li><li> The name of a discrete attribute (the <i>target</i> attribute)
that contains the class value, e.g.:
  <ul>
	<font color="#0000AA"><code>diagnosis.</code></font>
  </ul>
</li><li> The name of a continuous target attribute followed by a colon and
one or more thresholds in increasing order and separated by commas.
If there are <var>t</var> thresholds
<var>X1</var>, <var>X2</var>, ..., <var>Xt</var>
then the values of the attribute are divided into
<var>t+1</var> ranges:
  <ul>
  <li> less than or equal to <var>X1</var>
  </li><li> greater than <var>X1</var> and less than or equal to <var>X2</var>
  </li><li> . . .
  </li><li> greater than <var>Xt</var>.
  </li></ul>
Each range defines a class, so there are <var>t+1</var> classes.  For example,
a hypothetical entry<br>
  <ul>
	<font color="#0000AA"><code>age: 12, 19.</code></font><br>
  </ul>
would define three
classes: <font color="#0000AA"><code>age &lt;= 12</code></font>, <font color="#0000AA"><code>12 &lt; age &lt;= 19</code></font>, and
<font color="#0000AA"><code>age &gt; 19</code></font>.
</li></ul>

<p>
This first entry defining the classes is followed by definitions of
the attributes in the order that they will be given for each case.

</p><h4><a name="explicit">Explicitly-defined attributes</a></h4>

The name of each explicitly-defined attribute is followed by a colon
`<font color="#0000AA"><code>:</code></font>' and a description of the values taken by the attribute.
The attribute name is arbitrary, except that each attribute must have
a distinct name, and <font color="#0000AA"><code>case weight</code></font>
is reserved for setting weights for individual cases.
There are eight possibilities for the description of attribute values:

<dl>
<dt><font color="#0000AA"><code>continuous</code></font>
</dt><dd>The attribute takes numeric values.

</dd><dt><font color="#0000AA"><code>date</code></font>
</dt><dd>The attribute's values are dates in the form <font color="green" style="background: #EEEEEE">YYYY/MM/DD</font>
or <font color="green" style="background: #EEEEEE">YYYY-MM-DD</font>,
e.g. <font color="#0000AA"><code>2005/09/30</code></font> or <font color="#0000AA"><code>2005-09-30</code></font>.

</dd><dt><font color="#0000AA"><code>time</code></font>
</dt><dd>The attribute's values are times in the form <font color="green" style="background: #EEEEEE">HH:MM:SS</font>
with values between <font color="#0000AA"><code>00:00:00</code></font> and <font color="#0000AA"><code>23:59:59</code></font>.

</dd><dt><font color="#0000AA"><code>timestamp</code></font>
</dt><dd>The attribute's values are times in the form
<font color="green" style="background: #EEEEEE">YYYY/MM/DD HH:MM:SS</font> or
<font color="green" style="background: #EEEEEE">YYYY-MM-DD HH:MM:SS</font>,
e.g. <font color="#0000AA"><code>2005-09-30 15:04:00</code></font>.
(Note that there is a space separating the date and time.)

</dd><dt>a comma-separated list of names
</dt><dd>The attribute takes discrete values, and these are the allowable values.
The values may be prefaced by <font color="#0000AA"><code>[ordered]</code></font> to indicate
that they are given in a meaningful ordering, otherwise they will
be taken as unordered.  For instance, the values <font color="#0000AA"><code>low, medium, high</code></font>
are ordered, while
<font color="#0000AA"><code>meat, poultry, fish, vegetables</code></font> are not.
The former might be declared as <font color="#0000AA"><code><pre>grade: [ordered] low, medium, high.
</pre></code></font>
If the attribute values have a natural order, it is better to declare them
as such so that See5 can exploit the ordering.
(<em>NB:</em> The target attribute should not be declared as ordered.)

</dd><dt><font color="#0000AA"><code>discrete</code></font> <font color="green" style="background: #EEEEEE">N</font> for some integer <var>N</var>
</dt><dd>The attribute has discrete, unordered values,
but the values are assembled from the data itself; <var>N</var> is
the maximum number of such values.
This form can be handy for unordered discrete attributes with many values,
but its use means that the data values cannot be checked.
(<em>NB:</em> This form cannot be used for the target attribute.)

</dd><dt><font color="#0000AA"><code>ignore</code></font>
</dt><dd>The values of the attribute should be ignored.

</dd><dt><font color="#0000AA"><code>label</code></font>
</dt><dd>This attribute contains an identifying label for each case,
such as an account number or an order code.
The value of the attribute is ignored when classifiers are constructed,
but is used when referring to individual cases.
A label attribute can make it easier to locate errors in the data
and to cross-reference results to individual cases.
If there are two or more label attributes, only the last is used.
</dd></dl>

<h4><a name="implicit">Attributes defined by formulas</a></h4>

The name of each implicitly-defined attribute is followed by
`<font color="#0000AA"><code>:=</code></font>'
and then a formula defining the attribute value.  The formula is
written in the usual way, using parentheses where needed, and
may refer to any attribute defined up to this point.
Constants in the formula can be
numbers written in decimal notation, dates, times,
and discrete attribute values enclosed in string quotes `<font color="#0000AA"><code>"</code></font>'.
The operators and functions that can be used in the formula are

<ul>
<li> <font color="#0000AA"><code>+</code></font>, <font color="#0000AA"><code>-</code></font>, <font color="#0000AA"><code>*</code></font>,
<font color="#0000AA"><code>/</code></font>, <font color="#0000AA"><code>%</code></font> (mod),
<font color="#0000AA"><code>^</code></font> (meaning `raised to the power')
</li><li> <font color="#0000AA"><code>&gt;</code></font>, <font color="#0000AA"><code>&gt;=</code></font>, <font color="#0000AA"><code>&lt;</code></font>,
<font color="#0000AA"><code>&lt;=</code></font>, <font color="#0000AA"><code>=</code></font>, <font color="#0000AA"><code>&lt;&gt;</code></font> or
<font color="#0000AA"><code>!=</code></font> (both meaning `not equal')
</li><li> <font color="#0000AA"><code>and</code></font>, <font color="#0000AA"><code>or</code></font>
</li><li> <font color="#0000AA"><code>sin(</code></font>...<font color="#0000AA"><code>)</code></font>,
<font color="#0000AA"><code>cos(</code></font>...<font color="#0000AA"><code>)</code></font>,
<font color="#0000AA"><code>tan(</code></font>...<font color="#0000AA"><code>)</code></font>,
<font color="#0000AA"><code>log(</code></font>...<font color="#0000AA"><code>)</code></font>,
<font color="#0000AA"><code>exp(</code></font>...<font color="#0000AA"><code>)</code></font>,
<font color="#0000AA"><code>int(</code></font>...<font color="#0000AA"><code>)</code></font> (meaning `integer part of')
</li></ul>

The value of such an attribute is either continuous or true/false
depending on the formula.  For example, the attribute
<font color="#0000AA"><code><pre>	FTI:=  TT4 / T4U.
</pre></code></font>
is continuous since its value is obtained by dividing one number by
another.  The value of a hypothetical attribute such as
<font color="#0000AA"><code><pre>	strange := referral source = "WEST" or age &gt; 40.
</pre></code></font>
would be either <font color="#0000AA"><code>t</code></font> or <font color="#0000AA"><code>f</code></font>
since the value given by the formula is either true or false.

<p>
If the value of the formula cannot be determined for a particular
case because one or more of the attributes appearing in the formula
have unknown or non-applicable values,
the value of the implicitly-defined attribute is unknown.

</p><h4><a name="dates">Dates, times, and timestamps</a></h4>

Dates are stored by See5 as the number of days since a particular
starting point
so some operations on dates make sense.
Thus, if we have attributes
<font color="#0000AA"><code><pre>	d1: date.
	d2: date.
</pre></code></font>
we could define
<font color="#0000AA"><code><pre>	interval := d2 - d1.
	gap := d1 &lt;= d2 - 7.
	d1-day-of-week := (d1 + 1) % 7 + 1.
</pre></code></font>
<font color="#0000AA"><code>interval</code></font> then represents the number of days from
<font color="#0000AA"><code>d1</code></font> to <font color="#0000AA"><code>d2</code></font> (non-inclusive) and
<font color="#0000AA"><code>gap</code></font> would have a true/false value signaling whether
<font color="#0000AA"><code>d1</code></font> is at least a week before <font color="#0000AA"><code>d2</code></font>.
The last definition is a slightly non-obvious way of determining
the day of the week on which <font color="#0000AA"><code>d1</code></font> falls, with values
ranging from 1 (Monday) to 7 (Sunday).

<p>
Similarly, times are stored as the number of seconds since midnight.
If the <font color="green" style="background: #EEEEEE">names</font> file includes
<font color="#0000AA"><code></code></font></p><pre><font color="#0000AA"><code>	start: time.
	finish: time.
	elapsed := finish - start.
</code></font></pre>
the value of <font color="#0000AA"><code>elapsed</code></font> is the number of seconds
from <font color="#0000AA"><code>start</code></font> to <font color="#0000AA"><code>finish</code></font>.

<p>
Timestamps are a little more complex.  A timestamp is rounded to
the nearest minute, but limitations on the precision of floating-point
numbers mean that the values stored for timestamps from more than
thirty years ago are approximate.
If the <font color="green" style="background: #EEEEEE">names</font> file includes
<font color="#0000AA"><code></code></font></p><pre><font color="#0000AA"><code>	departure: timestamp.
	arrival: timestamp.
	flight time := arrival - departure.
</code></font></pre>
the value of <font color="#0000AA"><code>flight time</code></font> is the number of minutes
from <font color="#0000AA"><code>departure</code></font> to <font color="#0000AA"><code>arrival</code></font>.

<h4><a name="select">Selecting the attributes that can appear in classifiers</a></h4>

An optional final entry in the <font color="green" style="background: #EEEEEE">names</font> file affects the
way that See5 constructs classifiers.
This entry takes one of
the forms
<font color="#0000AA"><code><pre>	attributes included:
	attributes excluded:
</pre></code></font>
followed by a comma-separated list of attribute names.  The first
form restricts the attributes used in classifiers to those specifically
named;
the second form specifies that classifiers must not use any of the named
attributes.

<p>
Excluding an attribute from classifiers is not the same as ignoring the
attribute (see `<font color="#0000AA"><code>ignore</code></font>' above).
As an example, suppose that numeric attributes <font color="#0000AA"><code>A</code></font>
and <font color="#0000AA"><code>B</code></font>
are defined in the data, but background knowledge suggests that
only their difference is important.
The <font color="green" style="background: #EEEEEE">names</font> file might then contain the following entries:
</p><pre>	. . .
	<font color="#0000AA"><code>A: continuous.</code></font>
	<font color="#0000AA"><code>B: continuous.</code></font>
	<font color="#0000AA"><code>Diff := A - B.</code></font>
	   . . .
	<font color="#0000AA"><code>attributes excluded: A, B.</code></font>
</pre>
In this example the attributes <font color="#0000AA"><code>A</code></font> and <font color="#0000AA"><code>B</code></font>
could not be defined
as <font color="#0000AA"><code>ignore</code></font> because the definition of <font color="#0000AA"><code>Diff</code></font>
would then be invalid.

<h3><a name=".data">Data file</a></h3>

The second essential file,
the application's <font color="green" style="background: #EEEEEE">data</font> file
(e.g. <font color="#0000AA"><code>hypothyroid.data</code></font>)
provides information on the
<i>training</i>
cases from which See5 will extract patterns.
The entry for each case consists of one or more lines that give
the values for all explicitly-defined attributes.  If the classes are listed
in the first line of the <font color="green" style="background: #EEEEEE">names</font> file,
the attribute values are followed by the case's class value.
Values are separated by commas and the entry is optionally terminated by
a period.
Once again, anything on a line after a vertical bar `<font color="#0000AA"><code>|</code></font>'
is ignored.
(If the information for a case occupies more than one line, make sure
that the line breaks occur after commas.)

<p>
For example,
the first three cases from file
<font color="#0000AA"><code>hypothyroid.data</code></font> are:

<font color="#0000AA"><code></code></font></p><pre><font color="#0000AA"><code>	41,F,f,f,f,f,f,f,f,f,f,f,f,f,f,f,1.3,2.5,125,1.14,SVHC,negative,3733
	23,F,f,f,f,f,f,f,f,f,f,f,f,f,f,f,4.1,2,102,?,other,negative,1442
	46,M,f,f,f,f,N/A,f,f,f,f,f,f,f,f,f,0.98,?,109,0.91,other,negative,2965
</code></font></pre>

<font color="red">Don't forget the commas between values!</font>
If you leave them out,
See5 will not be able to process your data.

<p>
Notice that
`<font color="#0000AA"><code>?</code></font>' is used to denote a value that is missing or unknown.
Similarly, `<font color="#0000AA"><code>N/A</code></font>' denotes a value that is not applicable for
a particular case.
Also note
that the cases do not contain values for the attribute <font color="#0000AA"><code>FTI</code></font>
since its values are computed from other attribute values.

</p><h3><a name=".test">Test and cases files (optional)</a></h3>

Of course, the value of predictive patterns lies in their ability to make
accurate predictions!
It is difficult to judge the accuracy of a classifier by measuring
how well it does on the cases used in its construction;
the performance of the classifier on
new cases is much more informative.
(For instance, any number of gurus tell us about patterns that `explain'
the rise/fall behavior of the stock market in the past.  Even though
these patterns may appear plausible, they are only valuable to the extent that
they make useful predictions about future rises and falls.)

<p>
The third kind of file used
by See5 consists of new <font color="green" style="background: #EEEEEE">test</font>
cases (e.g. <font color="#0000AA"><code>hypothyroid.test</code></font>) on which the classifier
can be evaluated.
This file is optional and, if used, has
exactly the same format as the <font color="green" style="background: #EEEEEE">data</font> file.

</p><p>
Another optional file, the <font color="green" style="background: #EEEEEE">cases</font> file
(e.g. <font color="#0000AA"><code>hypothyroid.cases</code></font>),
differs from a <font color="green" style="background: #EEEEEE">test</font> file only in allowing the cases'
classes to be unknown (`<font color="#0000AA"><code>?</code></font>').
The <font color="green" style="background: #EEEEEE">cases</font> file is used primarily with
the cross-referencing procedure and public source code,
both of which are
described later on under <a href="http://www.rulequest.com/see5-win.html#HOOKS">linking to other programs</a>.

</p><h3><a name=".costs">Costs file (optional)</a></h3>

The last kind of file, the <font color="green" style="background: #EEEEEE">costs</font> file
(e.g. <font color="#0000AA"><code>hypothyroid.costs</code></font>),
is also optional and sets out
differential misclassification costs.
In some applications
there is a much higher penalty for certain types of mistakes.
In this application, a prediction that hypothyroidism is not present
could be very costly if in fact it is.
On the other hand, predicting incorrectly that a patient is
hypothyroid
may be a less serious error.
See5 allows different misclassification
costs to be associated with each combination of real class and
predicted class.  We will return to this topic near the end of the
tutorial.

<p>
</p><h2><a name="GUI">User Interface</a></h2>

<p>
It is difficult to see what is going on in an interface without
actually using it.  As a simple illustration, here is the main window
of See5 after the hypothyroid application has been selected.

</p><ul>
<img src="./See5  An Informal Tutorial_files/see5-main.gif" alt="See5 main window" height="400" width="600">
</ul>

<p>
The main window of See5 has six buttons on its toolbar.  From left to
right, they are
</p><dl>
<dt> <font color="maroon">Locate Data</font>
</dt><dd> invokes a browser to find the files
for your application, or to change the current application;
</dd><dt> <font color="maroon">Construct Classifier</font>
</dt><dd> selects the type of classifier
to be constructed and sets other options;
</dd><dt> <font color="maroon">Stop</font>
</dt><dd> interrupts the classifier-generating process;
</dd><dt> <font color="maroon">Review Output</font>
</dt><dd> re-displays the output from the last classifier construction (if any);
</dd><dt> <font color="maroon">Use Classifier</font>
</dt><dd> interactively applies the current classifier
to one or more cases; and
</dd><dt> <font color="maroon">Cross-Reference</font>
</dt><dd> shows how cases in training or test data relate to (parts of)
a classifier and vice versa.
</dd></dl>
These functions can also be initiated from the <font color="maroon">File</font> menu.

<p>
The <font color="maroon">Edit</font> menu facilities changes to the <font color="green" style="background: #EEEEEE">names</font> and <font color="green" style="background: #EEEEEE">costs</font>
files after an application's files have been located.
On-line help is available through the <font color="maroon">Help</font> menu.

</p><h2><a name="CLASSIFIERS">Constructing Classifiers</a></h2>

Once the <font color="green" style="background: #EEEEEE">names</font>, <font color="green" style="background: #EEEEEE">data</font>, and optional
files have been set up, everything is ready to use See5.

<p>
The first step is to locate the date using the <font color="maroon">Locate Data</font>
button on the toolbar (or the corresponding selection from
the <font color="maroon">File</font> menu).
We will assume that the hypothyroid data above has been
located in this manner.
</p><p>
There are several options that affect the type of classifier that
See5 produces and the way that it is constructed.
The <font color="maroon">Construct Classifier</font> button on the toolbar 
(or selection from the <font color="maroon">File</font> menu)
displays a dialog box that sets out these classifier construction options:
</p><ul>
<img src="./See5  An Informal Tutorial_files/see5-dialog.gif" alt="Main dialog box" height="468" width="238">
</ul>
Many of the options have default values that should be satisfactory
for most applications.

<h3><a name="TREES">Decision trees</a></h3>

When See5 is invoked with
the default values of all options,
it constructs a decision tree and generates output
like this:

<pre class="out"><u>See5 [Release 2.09]</u>     <font color="#000080">Mon Jan 23 16:20:13 2012</font>

<font color="#000080">Class specified by attribute `diagnosis'</font>

<font color="#000080">Read 2772 cases (24 attributes) from hypothyroid.data</font>

<font color="#000080">Decision tree:</font>

TSH &lt;= 6: negative (2472/2)
TSH &gt; 6:
<font color="#000080">:...</font>FTI &lt;= 65:
    <font color="#000080">:...</font>thyroid surgery = t:
    <font color="#000080">:   :...</font>FTI &lt;= 36.1: negative (2.1)
    <font color="#000080">:   :   </font>FTI &gt; 36.1: primary (2.1/0.1)
    <font color="#000080">:   </font>thyroid surgery = f:
    <font color="#000080">:   :...</font>TT4 &lt;= 61: primary (51/3.7)
    <font color="#000080">:       </font>TT4 &gt; 61:
    <font color="#000080">:       :...</font>referral source in {WEST,SVHD}: primary (0)
    <font color="#000080">:           </font>referral source in {STMW,SVHC,SVI}: primary (4.9/0.8)
    <font color="#000080">:           </font>referral source = other:
    <font color="#000080">:           :...</font>TSH &lt;= 22: negative (6.4/2.7)
    <font color="#000080">:               </font>TSH &gt; 22: primary (5.8/0.8)
    FTI &gt; 65:
    <font color="#000080">:...</font>on thyroxine = t: negative (37.7)
        on thyroxine = f:
        <font color="#000080">:...</font>thyroid surgery = t: negative (6.8)
            thyroid surgery = f:
            <font color="#000080">:...</font>TT4 &gt; 153: negative (6/0.1)
                TT4 &lt;= 153:
                <font color="#000080">:...</font>TT4 &lt;= 37: primary (2.5/0.2)
                    TT4 &gt; 37: compensated (174.6/24.8)


<font color="#000080">Evaluation on training data (2772 cases):</font>

            <font color="#000080">Decision Tree   </font>
          <font color="#000080">----------------  </font>
          <font color="#000080">Size      Errors  </font>

            12    7( 0.3%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            60     <font color="red">3</font>                <font color="#000080">(a): class primary</font>
                 153           <font color="red">1</font>    <font color="#000080">(b): class compensated</font>
                               <font color="red">2</font>    <font color="#000080">(c): class secondary</font>
                   <font color="red">1</font>        2552    <font color="#000080">(d): class negative</font>


        <font color="#000080">Attribute usage:</font>

             90%  TSH
             18%  thyroid surgery
             17%  on thyroxine
             14%  TT4
             13%  T4U
             13%  FTI
              7%  referral source


<font color="#000080">Evaluation on test data (1000 cases):</font>

            <font color="#000080">Decision Tree   </font>
          <font color="#000080">----------------  </font>
          <font color="#000080">Size      Errors  </font>

            12    4( 0.4%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            31                 <font color="red">1</font>    <font color="#000080">(a): class primary</font>
             <font color="red">1</font>    39                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
                   <font color="red">2</font>         926    <font color="#000080">(d): class negative</font>


<font color="#000080">Time: 0.0 secs</font>
</pre>

<p>
The first line identifies the version of See5 and the run date.
See5 constructs a decision tree from the 2772 training cases
in the file <font color="#0000AA"><code>hypothyroid.data</code></font>, and this appears next.
Although it may not look much like a tree, this output can be
paraphrased as:
</p><p>
</p><pre>	if TSH is less than or equal to 6 then <em>negative</em>
	else
	if TSH is greater than 6 then
	    if FTI is less than or equal to 65 then
		if thyroid surgery equals t then
		    if FTI is less than or equal to 36.1 then <em>negative</em>
		    else
		    if FTI is greater than 36.1 then <em>primary</em>
		else
		if thyroid surgery equals f then
		    if TT4 is less than or equal to 61 then <em>primary</em>
		    else
		    if TT4 is greater than 61 then
		    . . . .
</pre>
and so on.

<p>
The tree employs a case's attribute values to map it
to a <i>leaf</i> designating one of the classes.
Every leaf of the tree is followed by a cryptic (<var>n</var>) or
(<var>n</var>/<var>m</var>).
For instance, the last leaf of the decision tree
is <font color="#0000AA"><code>compensated (174.6/24.8)</code></font>, for which <var>n</var> is 174.6 and
<var>m</var> is 24.8.
The value of <var>n</var> is the number of cases in the file
<font color="#0000AA"><code>hypothyroid.data</code></font>
that are mapped to this leaf, and <var>m</var> (if it appears) is the number of
them that are classified incorrectly by the leaf.
(A non-integral number of cases can arise because, when the value of
an attribute in the tree is not known, See5 splits the case
and sends a fraction down each branch.)

</p><p>
The next section covers the evaluation of this decision tree shown in the
second part of the output.
Before we leave this output, though, its final line
states the <em>elapsed</em> time for the run.  (This differs
from early releases of See5 which gave the CPU time.)
The construction of a decision tree is usually completed quickly, even
when there are many thousands of cases.  Some of the options described later,
such as ruleset generation and boosting, can slow things down considerably.


</p><h3><a name="EVALUATION">Evaluation</a></h3>

Classifiers constructed by See5 are evaluated on the training data
from which they were generated, and also on a separate file of
unseen test cases if available;  evaluation by cross-validation is
discussed elsewhere.

<p>
Results of the decision tree on the cases in
<font color="#0000AA"><code>hypothyroid.data</code></font> are:

</p><pre class="out">            <font color="#000080">Decision Tree   </font>
          <font color="#000080">----------------  </font>
          <font color="#000080">Size      Errors  </font>

            12    7( 0.3%)   &lt;&lt;
</pre>

<font color="#0000AA"><code>Size</code></font> is the number of non-empty leaves on the
tree and
<font color="#0000AA"><code>Errors</code></font> shows
the number and percentage of cases misclassified.
The tree, with 12 leaves, misclassifies 7 of the 2772 given cases, an error
rate of 0.3%.
This might seem inconsistent with the errors recorded at the leaves --
the leaf mentioned above shows 24.8 errors!  The discrepancy arises because
parts of a case split as a result of unknown attribute values can
be misclassified and yet, when the votes from all the parts are aggregated,
the correct class can still be chosen.

<p>
When there are no more than twenty classes,
performance on the training cases is further analyzed in a
<i>confusion matrix</i>
that pinpoints the kinds of errors made.


</p><pre class="out">           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            60     <font color="red">3</font>                <font color="#000080">(a): class primary</font>
                 153           <font color="red">1</font>    <font color="#000080">(b): class compensated</font>
                               <font color="red">2</font>    <font color="#000080">(c): class secondary</font>
                   <font color="red">1</font>        2552    <font color="#000080">(d): class negative</font>
</pre>

In this example, the decision tree misclassifies
<ul>
<li>
three of the <font color="#0000AA"><code>primary</code></font> cases as <font color="#0000AA"><code>compensated</code></font>,
</li><li>
one of the <font color="#0000AA"><code>compensated</code></font> cases as <font color="#0000AA"><code>negative</code></font>,
</li><li>
both <font color="#0000AA"><code>secondary</code></font> cases as <font color="#0000AA"><code>negative</code></font>, and
</li><li>
one <font color="#0000AA"><code>negative</code></font> case as <font color="#0000AA"><code>compensated</code></font>.
</li></ul>

<p>
When the number of classes is larger than twenty,
a summary of performance broken down by class is shown instead.
The entry for each class shows the number
of cases for that class and the numbers of <i>false positives</i> and <i>false
negatives</i>.  A false positive for class <var>C</var> is a case of
another class that is classified as <var>C</var>, while a false
negative for <var>C</var> is a case of class <var>C</var> that is
classified as some other class.  Of course, the total number of errors
must come to half the sum of the numbers of false positives and false
negatives, since each error is counted twice--as a false negative for
its true class, and as a false positive for the predicted class.

</p><p>
For some applications, especially those with many attributes, it
may be useful to know how the individual attributes contribute to
the classifier.  This is shown in the next section:

</p><pre class="out">        <font color="#000080">Attribute usage:</font>

             90%  TSH
             18%  thyroid surgery
             17%  on thyroxine
             14%  TT4
             13%  T4U
             13%  FTI
              7%  referral source
</pre>

The figure before each attribute is the percentage of training cases
in <font color="#0000AA"><code>hypothyroid.data</code></font> for which the value of that
attribute is known and is used in predicting a class.  The second
entry, for instance, shows that the decision tree uses a known
value of <font color="#0000AA"><code>thyroid surgery</code></font> when classifying 18% of the
training cases.  Attributes for which this value is less than 1%
are not shown.  Two points are worth noting here:
<ul>
<li> These values are computed for the particular classifier and training
cases; changing either would give different values.
</li><li> When a case is classified, use of an attribute such as
<font color="#0000AA"><code>FTI</code></font> that is defined by a formula also counts as
using any attributes involved in its definition (here <font color="#0000AA"><code>TT4</code></font>
and <font color="#0000AA"><code>T4U</code></font>).
</li></ul>

<p>
If there are optional unseen test cases, the
classifier's performance on these cases is summarized in a
format similar to that for the training cases.

</p><pre class="out">            <font color="#000080">Decision Tree   </font>
          <font color="#000080">----------------  </font>
          <font color="#000080">Size      Errors  </font>

            12    4( 0.4%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            31                 <font color="red">1</font>    <font color="#000080">(a): class primary</font>
             <font color="red">1</font>    39                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
                   <font color="red">2</font>         926    <font color="#000080">(d): class negative</font>
</pre>

A very simple <i>majority</i> classifier predicts
that every new case belongs to the most common class in the
training data.
In this example, 2553 of the 2772 training cases belong to class
<font color="#0000AA"><code>negative</code></font> so that
a majority classifier would always opt for <font color="#0000AA"><code>negative</code></font>.
The 1000 test cases from file <font color="#0000AA"><code>hypothyroid.test</code></font>
include 928 belonging to class <font color="#0000AA"><code>negative</code></font>, so a simple
majority classifier would have an error rate of 7.2%.
The decision tree has a lower error rate of 0.4% on the new
cases, but notice that
this is higher than its error rate on the training cases.
The confusion matrix (or false positive/false negative summary if
there are more than twenty classes) for the test cases again provides
more details on correct and incorrect classifications.


<p>
</p><h3><a name="SUBSETS">Discrete value subsets</a></h3>

<p>
By default, a test on
a discrete attributes has a separate branch for
each of its values that is present in the data.
Tests with a high fan-out can have the undesirable side-effect
of fragmenting the data during construction of the decision tree.
See5 has a <font color="maroon">Subset</font> option
that can mitigate this fragmentation to some
extent: attribute values are grouped into subsets and each subtree
is associated with a subset rather than with a single value.

</p><p>
In the hypothyroid example, invoking this option
merely simplifies part of the tree as

<font color="#0000AA"><code></code></font></p><pre><font color="#0000AA"><code>	referral source in {WEST,STMW,SVHC,SVI,SVHD}: primary (4.9/0.8)
</code></font></pre>

without affecting classification performance on either the training
or test data.

<p>
Although it does not help much for this application, the
<font color="maroon">Subset</font>
option is recommended when there are important discrete
attributes that have more than four or five values.

</p><p>
</p><h3><a name="RULES">Rulesets</a></h3>

<p>
Decision trees can sometimes be quite difficult to understand.  An
important feature of See5 is its ability to generate
classifiers called <i>rulesets</i> that consist of unordered
collections of (relatively) simple if-then rules.

</p><p>
The <font color="maroon">Rulesets</font> option
causes classifiers to be expressed as rulesets rather than decision trees,
here giving the following:

</p><pre class="out"><u>See5 [Release 2.09]</u>     <font color="#000080">Mon Jan 23 16:22:15 2012</font>

    <font color="#000080">Options:</font>
        <font color="#000080">Rule-based classifiers</font>

<font color="#000080">Class specified by attribute `diagnosis'</font>

<font color="#000080">Read 2772 cases (24 attributes) from ../hypothyroid.data</font>

<font color="#000080">Rules:</font>

<font color="#000080">Rule 1: (31, lift 42.7)</font>
        thyroid surgery = f
        TSH &gt; 6
        TT4 &lt;= 37
        -&gt;  class primary  [0.970]

<font color="#000080">Rule 2: (63/6, lift 39.3)</font>
        TSH &gt; 6
        FTI &lt;= 65
        -&gt;  class primary  [0.892]

<font color="#000080">Rule 3: (270/116, lift 10.3)</font>
        TSH &gt; 6
        -&gt;  class compensated  [0.570]

<font color="#000080">Rule 4: (2225/2, lift 1.1)</font>
        TSH &lt;= 6
        -&gt;  class negative  [0.999]

<font color="#000080">Rule 5: (296, lift 1.1)</font>
        on thyroxine = t
        FTI &gt; 65
        -&gt;  class negative  [0.997]

<font color="#000080">Rule 6: (240, lift 1.1)</font>
        TT4 &gt; 153
        -&gt;  class negative  [0.996]

<font color="#000080">Rule 7: (29, lift 1.1)</font>
        thyroid surgery = t
        FTI &gt; 65
        -&gt;  class negative  [0.968]

Default class: negative


<font color="#000080">Evaluation on training data (2772 cases):</font>

                <font color="#000080">Rules     </font>
          <font color="#000080">----------------</font>
            <font color="#000080">No      Errors</font>

             7   14( 0.5%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            60     <font color="red">3</font>                <font color="#000080">(a): class primary</font>
             <font color="red">1</font>   153                <font color="#000080">(b): class compensated</font>
                               <font color="red">2</font>    <font color="#000080">(c): class secondary</font>
             <font color="red">5     3</font>        2545    <font color="#000080">(d): class negative</font>


        <font color="#000080">Attribute usage:</font>

             90%  TSH
             20%  TT4
             14%  T4U
             14%  FTI
             11%  on thyroxine
              2%  thyroid surgery


<font color="#000080">Evaluation on test data (1000 cases):</font>

                <font color="#000080">Rules     </font>
          <font color="#000080">----------------</font>
            <font color="#000080">No      Errors</font>

             7    5( 0.5%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            32                      <font color="#000080">(a): class primary</font>
             <font color="red">1</font>    39                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
             <font color="red">1     3</font>         924    <font color="#000080">(d): class negative</font>


<font color="#000080">Time: 0.0 secs</font>
</pre>

<p>
Each rule consists of:
</p><ul>
<li>A rule number -- this is quite arbitrary and serves only to identify
the rule.
</li><li>Statistics
<font color="#0000AA"><code>(</code></font><var>n</var><font color="#0000AA"><code>, lift </code></font><var>x</var><font color="#0000AA"><code>)</code></font>
or
<font color="#0000AA"><code>(</code></font><var>n</var><font color="#0000AA"><code>/</code></font><var>m</var><font color="#0000AA"><code>,
lift </code></font><var>x</var><font color="#0000AA"><code>)</code></font>
that summarize the performance of the rule.
Similarly to a leaf, <var>n</var> is the number of training cases covered
by the rule and <var>m</var>, if it appears, shows how many of them
do not belong to the class predicted by the rule.
The rule's accuracy is estimated by the Laplace ratio
<nobr><var>(n-m+1)/(n+2)</var></nobr>.
The lift <var>x</var>
is the result of dividing the rule's estimated
accuracy by the relative frequency of the
predicted class in the training set.
</li><li> One or more conditions that must all be satisfied if the rule
is to be applicable.
</li><li> A class predicted by the rule.
</li><li> A value between 0 and 1 that indicates
the confidence with which this prediction is made.
(Note: The <em>boosting</em> option described below employs an
artificial weighting of the training cases; if it is used,
the confidence may not reflect the true accuracy of the rule.)
</li></ul>
When a ruleset like this is used to classify a case, it may happen that
several of the rules are applicable (that is, all their
conditions are satisfied).
If the applicable rules predict different classes, there is
an implicit conflict that could be resolved in several ways:
for instance,
we could believe the rule with the highest confidence, or we could
attempt to aggregate the rules' predictions to reach a verdict.
See5 adopts the latter strategy --
each applicable rule votes for its predicted class
with a voting weight equal to its confidence value, the votes
are totted up, and
the class with the highest total vote is chosen as the final
prediction.
There is also a <i>default class</i>, here <font color="#0000AA"><code>negative</code></font>,
that is used when none of the rules apply.

<p>
Rulesets are generally easier to understand than trees
since each rule describes a specific context associated with
a class.
Furthermore,
a ruleset generated from a tree usually has fewer rules than
than the tree has leaves, another plus for comprehensibility.
(In this example, the first decision tree with 12 leaves is reduced to
seven rules.)

</p><p>
Another advantage of ruleset classifiers is that they
are often more accurate predictors than decision
trees -- a point not illustrated here, since the ruleset has an
error rate of 0.5% on the test cases.
For very large datasets, however, generating rules with the
<font color="maroon">Ruleset</font> option
can require considerably more computer time.

</p><p>
For a given application, the attribute usage shown for a decision tree
and for a ruleset can be a bit different.
In the case of the tree, the attribute at the root is always
used (provided its value is known) while an attribute further down the
tree is used less frequently.
For a ruleset,
an attribute is used to classify a case if it is referenced by a
condition of at least one rule that applies to that case;
the order in which attributes appear in a ruleset is not relevant.

</p><h4><a name="utility">Rule utility ordering</a></h4>

In the example above, rules are ordered by class and sub-ordered by
confidence.  An alternative ordering by estimated contribution to
predictive accuracy can be selected using the
<font color="maroon">Sort by utility</font>
option.  Under this option, the rule that most reduces the error rate
appears first and the rule that contributes least appears last.
Furthermore, results are reported in a selected number of
<i>bands</i> so that the predictive accuracies of the more
important subsets of rules are also estimated.  For example,
if the
<font color="maroon">Sort by utility</font> option with four bands
is selected, the hypothyroid rules are reordered as

<pre class="out"><font color="#000080">Rule 1: (2225/2, lift 1.1)</font>
        TSH &lt;= 6
        -&gt;  class negative  [0.999]

<font color="#000080">Rule 2: (270/116, lift 10.3)</font>
        TSH &gt; 6
        -&gt;  class compensated  [0.570]

<font color="#000080">Rule 3: (63/6, lift 39.3)</font>
        TSH &gt; 6
        FTI &lt;= 65
        -&gt;  class primary  [0.892]

<font color="#000080">Rule 4: (296, lift 1.1)</font>
        on thyroxine = t
        FTI &gt; 65
        -&gt;  class negative  [0.997]

<font color="#000080">Rule 5: (240, lift 1.1)</font>
        TT4 &gt; 153
        -&gt;  class negative  [0.996]

<font color="#000080">Rule 6: (29, lift 1.1)</font>
        thyroid surgery = t
        FTI &gt; 65
        -&gt;  class negative  [0.968]

<font color="#000080">Rule 7: (31, lift 42.7)</font>
        thyroid surgery = f
        TSH &gt; 6
        TT4 &lt;= 37
        -&gt;  class primary  [0.970]
</pre>
The rules are divided into four bands of roughly equal sizes
and a further summary is generated for both training and test cases.
Here is the output for test cases:

<pre class="out"><font color="#000080">Evaluation on test data (1000 cases):</font>

                <font color="#000080">Rules     </font>
          <font color="#000080">----------------</font>
            <font color="#000080">No      Errors</font>

             7    5( 0.5%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            32                      <font color="#000080">(a): class primary</font>
             <font color="red">1</font>    39                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
             <font color="red">1     3</font>         924    <font color="#000080">(d): class negative</font>

<font color="#000080">Rule utility summary:</font>

        <font color="#000080">Rules         Errors</font>
        <font color="#000080">-----         ------</font>
        1-2        56( 5.6%)
        1-4        10( 1.0%)
        1-5         6( 0.6%)
</pre>
This shows that,
when only the first two rules are used,
the error rate on the test cases is 5.6%,
dropping to 1.0% when the first four rules are used,
and so on.  The performance of the entire ruleset is not
repeated since it is shown above the utility summary.

<p>
Rule utility orderings are not given for cross-validations (see below).

</p><p>
</p><h3><a name="BOOSTING">Boosting</a></h3>

<p>
Another innovation incorporated in See5 is <i>adaptive boosting</i>, based
on the work of Rob Schapire and Yoav Freund.
The idea is to generate several classifiers (either decision trees
or rulesets) rather than just one.  When a new case is to be
classified, each classifier votes for its predicted class and the votes
are counted to determine the final class.

</p><p>
But how can we generate several classifiers from a single dataset?
As the first step, a single decision tree or ruleset is constructed
as before from the training data (e.g. <font color="#0000AA"><code>hypothyroid.data</code></font>).
This classifier will usually make mistakes on some cases in the data;
the first decision tree, for instance, gives the wrong class
for 7 cases in <font color="#0000AA"><code>hypothyroid.data</code></font>.
When the second classifier is constructed, more attention is paid
to these cases in an attempt to get them right.
As a consequence, the second classifier will generally be different
from the first.  It also will make errors on some cases,
and these become the the focus of attention during construction
of the third classifier.
This process continues for a pre-determined number of iterations
or <i>trials</i>, but stops if the most recent classifiers is
either extremely accurate or inaccurate.

</p><p>
The <font color="maroon">Boost</font> option with <var>x</var>
trials instructs See5 to
construct up to <var>x</var>
classifiers in this manner.
Naturally, constructing multiple classifiers
requires more computation that building a single classifier
-- but the effort can pay dividends!
Trials over numerous datasets, large
and small, show that on average 10-classifier boosting reduces the
error rate for test cases by about 25%.

</p><p>
Selecting the
<font color="maroon">Boost</font> option with 10 trials
causes ten decision trees to be generated.
The summary of the trees'
individual and aggregated performance on the 1000 test cases is:

</p><pre class="out"><font color="#000080">Trial       Decision Tree   </font>
<font color="#000080">-----     ----------------  </font>
          <font color="#000080">Size      Errors  </font>

   0        12    4( 0.4%)
   1         7   52( 5.2%)
   2        11    9( 0.9%)
   3        16   28( 2.8%)
   4        11   40( 4.0%)
   5        22   38( 3.8%)
   6        21   16( 1.6%)
   7        21   18( 1.8%)
   8        23   76( 7.6%)
   9        25   15( 1.5%)
boost             2( 0.2%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            32                      <font color="#000080">(a): class primary</font>
                  40                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
                   <font color="red">2</font>         926    <font color="#000080">(d): class negative</font>
</pre>

<p>
The performance of the classifier constructed at each trial
is summarized on a separate line, while the line labeled
<font color="#0000AA"><code>boost</code></font>
shows the result of voting all the classifiers.

</p><p>
The decision tree constructed on Trial 0 is identical to that
produced without the <font color="maroon">Boost</font> option.
Some of the subsequent trees produced by paying more attention
to certain cases
have relatively high overall error rates.  Nevertheless, when the
trees are combined by voting,
the final predictions have a lower error rate of 0.2% on the test cases.

</p><p>
</p><h3><a name="WINNOWING">Winnowing attributes</a></h3>

<p>
The decision trees and rulesets constructed by See5 do not generally
use all of the attributes.  The hypothyroid application
has 22 predictive attributes (plus a class and a label attribute)
but only six of them appear in the tree and the ruleset.
This ability to pick and choose among the predictors is an
important advantage of tree-based modeling techniques.

</p><p>
Some applications, however, have an abundance of attributes!
For instance, one approach to text classification describes each
passage by the words that appear in it, so there is a separate attribute
for each different word in a restricted dictionary.

</p><p>
When there are numerous alternatives for each test in the tree or
ruleset, it is likely that at least one of them will appear to provide
valuable predictive information.
In applications like these it can be useful to pre-select a
subset of the attributes that will be used to construct the
decision tree or ruleset.
The See5 mechanism to do this is called "winnowing" by
analogy with the process for separating wheat from chaff (or, here,
useful attributes from unhelpful ones).

</p><p>
Winnowing is not obviously relevant for the hypothyroid application
since there are relatively few attributes.  To illustrate the idea,
however, here are the results when the
<font color="maroon">Winnowing</font>
option is invoked:

</p><pre class="out"><u>See5 [Release 2.09]</u>  	<font color="#000080">Mon Jan 23 16:24:11 2012</font>

    <font color="#000080">Options:</font>
        <font color="#000080">Winnow attributes</font>

<font color="#000080">Class specified by attribute `diagnosis'</font>

<font color="#000080">Read 2772 cases (24 attributes) from hypothyroid.data</font>

<font color="#000080">14 attributes winnowed</font>
<font color="#000080">Estimated importance of remaining attributes:</font>

    <font color="#000080">990%  TSH</font>
    <font color="#000080">270%  FTI</font>
    <font color="#000080">200%  on thyroxine</font>
     <font color="#000080">30%  thyroid surgery</font>
     <font color="#000080">&lt;1%  age</font>
     <font color="#000080">&lt;1%  T3</font>
     <font color="#000080">&lt;1%  TT4</font>
     <font color="#000080">&lt;1%  referral source</font>

<font color="#000080">Decision tree:</font>

TSH &lt;= 6: negative (2472/2)
TSH &gt; 6:
<font color="#000080">:...</font>FTI &lt;= 65:
    <font color="#000080">:...</font>thyroid surgery = t:
    <font color="#000080">:   :...</font>FTI &lt;= 36.1: negative (2.1)
    <font color="#000080">:   :   </font>FTI &gt; 36.1: primary (2.1/0.1)
    <font color="#000080">:   </font>thyroid surgery = f:
    <font color="#000080">:   :...</font>TT4 &lt;= 61: primary (51/3.7)
    <font color="#000080">:       </font>TT4 &gt; 61:
    <font color="#000080">:       :...</font>referral source in {WEST,SVHD}: primary (0)
    <font color="#000080">:           </font>referral source in {STMW,SVHC,SVI}: primary (4.9/0.8)
    <font color="#000080">:           </font>referral source = other:
    <font color="#000080">:           :...</font>TSH &lt;= 22: negative (6.4/2.7)
    <font color="#000080">:               </font>TSH &gt; 22: primary (5.8/0.8)
    FTI &gt; 65:
    <font color="#000080">:...</font>on thyroxine = t: negative (37.7)
        on thyroxine = f:
        <font color="#000080">:...</font>thyroid surgery = t: negative (6.8)
            thyroid surgery = f:
            <font color="#000080">:...</font>TT4 &gt; 153: negative (6/0.1)
                TT4 &lt;= 153:
                <font color="#000080">:...</font>TT4 &lt;= 37: primary (2.5/0.2)
                    TT4 &gt; 37: compensated (174.6/24.8)


<font color="#000080">Evaluation on training data (2772 cases):</font>

            <font color="#000080">Decision Tree   </font>
          <font color="#000080">----------------  </font>
          <font color="#000080">Size      Errors  </font>

            12    7( 0.3%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            60     <font color="red">3</font>                <font color="#000080">(a): class primary</font>
                 153           <font color="red">1</font>    <font color="#000080">(b): class compensated</font>
                               <font color="red">2</font>    <font color="#000080">(c): class secondary</font>
                   <font color="red">1</font>        2552    <font color="#000080">(d): class negative</font>


        <font color="#000080">Attribute usage:</font>

             90%  TSH
             18%  thyroid surgery
             17%  on thyroxine
             14%  TT4
             13%  T4U
             13%  FTI
              7%  referral source


<font color="#000080">Evaluation on test data (1000 cases):</font>

            <font color="#000080">Decision Tree   </font>
          <font color="#000080">----------------  </font>
          <font color="#000080">Size      Errors  </font>

            12    4( 0.4%)   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            31                 <font color="red">1</font>    <font color="#000080">(a): class primary</font>
             <font color="red">1</font>    39                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
                   <font color="red">2</font>         926    <font color="#000080">(d): class negative</font>


<font color="#000080">Time: 0.0 secs</font>
</pre>
After analyzing the training cases and before the decision
tree is built, See5 winnows 14 of the 22 predictive attributes.
This has the same effect as marking the attributes as excluded
by an entry in the <font color="green" style="background: #EEEEEE">names</font> file; winnowed attributes
can still be used in the definition of other attributes.
In this example, <font color="#0000AA"><code>T4U</code></font> is winnowed but is still available
for use in the definition of <font color="#0000AA"><code>FTI</code></font>.

<p>
The remaining attributes are then listed in order of <em>importance</em>,
See5's estimate of the factor by which the true error rate or
misclassification cost would increase if that attribute were excluded.
If TSH were excluded, for example, See5 expects the error rate on unseen
test cases to increase to 4% (990% of the current rate of 0.4%).
This estimate is intended only as a rough guide
and should not be taken too literally!

</p><p>
We then see the decision tree that is constructed from the reduced
set of attributes.  In this case it is identical to the original
tree, but winnowing will usually lead to a different classifier.

</p><p>
Since winnowing the attributes can be a time-consuming process,
it is recommended primarily for large applications (10,000 cases or more)
where there is reason to suspect that 
many of the attributes have at best marginal relevance to the
classification task.

</p><p>
</p><h3><a name="FUZZY">Softening thresholds</a></h3>

<p>
The top of our initial decision tree tests whether
the value of the attribute <font color="#0000AA"><code>TSH</code></font> is less than or
equal to, or greater than, 6.  If the former holds, we go no further
and predict that the case's class is <font color="#0000AA"><code>negative</code></font>, while
if it does not we look at other information before making a decision.
Thresholds like this are sharp by default, so that a case with
a hypothetical value of 5.99 for <font color="#0000AA"><code>TSH</code></font> is treated
quite differently from one with a value of 6.01.

</p><p>
For some domains, this sudden change is quite appropriate --
for instance, there are hard-and-fast cutoffs for bands
of the income tax table.  For other applications, though,
it is more reasonable to expect classification decisions to
change more slowly with changes in attribute values.

</p><p>
See5 contains an option
to `soften' thresholds such as <font color="#0000AA"><code>6</code></font> above.
When this is invoked, each threshold
is broken into three ranges -- let us denote them
by a lower bound <var>lb</var>, an upper bound <var>ub</var>, and a
central value <var>t</var>.  If the attribute value in question
is below <var>lb</var> or above <var>ub</var>, classification is
carried out using the single branch
corresponding to the `&lt;=' or '&gt;' result respectively.
If the value lies between <var>lb</var> and <var>ub</var>, <em>both</em>
branches of the tree are investigated and the results combined
probabilistically.
The values of <var>lb</var> and <var>ub</var> are determined by See5
based on an analysis of the apparent sensitivity of classification
to small changes in the threshold.  They need not be symmetric --
a fuzzy threshold can be sharper on one side than on the other.

</p><p>
Invoking the <font color="maroon">Fuzzy thresholds</font> option
gives the following decision tree:

</p><pre class="out">TSH &lt;= 6 (6.05): negative (2472/2)
TSH &gt;= 6.1 (6.05):
<font color="#000080">:...</font>FTI &lt;= 61 (65.35):
    <font color="#000080">:...</font>thyroid surgery = t:
    <font color="#000080">:   :...</font>FTI &lt;= 24 (38.25): negative (2.1)
    <font color="#000080">:   :   </font>FTI &gt;= 52.5 (38.25): primary (2.1/0.1)
    <font color="#000080">:   </font>thyroid surgery = f:
    <font color="#000080">:   :...</font>TT4 &lt;= 44 (61.5): primary (51/3.7)
    <font color="#000080">:       </font>TT4 &gt;= 63 (61.5):
    <font color="#000080">:       :...</font>referral source in {WEST,SVHD}: primary (0)
    <font color="#000080">:           </font>referral source in {STMW,SVHC,SVI}: primary (4.9/0.8)
    <font color="#000080">:           </font>referral source = other:
    <font color="#000080">:           :...</font>TSH &lt;= 14 (22.5): negative (6.4/2.7)
    <font color="#000080">:               </font>TSH &gt;= 44 (22.5): primary (5.8/0.8)
    FTI &gt;= 66 (65.35):
    <font color="#000080">:...</font>on thyroxine = t: negative (37.7)
        on thyroxine = f:
        <font color="#000080">:...</font>thyroid surgery = t: negative (6.8)
            thyroid surgery = f:
            <font color="#000080">:...</font>TT4 &gt;= 159 (153): negative (6/0.1)
                TT4 &lt;= 146 (153):
                <font color="#000080">:...</font>TT4 &lt;= 14 (37.5): primary (2.5/0.2)
                    TT4 &gt;= 61 (37.5): compensated (174.6/24.8)
</pre>
Each threshold is now of the form
<font color="#0000AA"><code>&lt;=</code></font> <var>lb</var> <font color="#0000AA"><code>(</code></font><var>t</var><font color="#0000AA"><code>)</code></font>
or
<font color="#0000AA"><code>&gt;=</code></font> <var>ub</var> <font color="#0000AA"><code>(</code></font><var>t</var><font color="#0000AA"><code>)</code></font>.
In this example, most
of the thresholds are still relatively tight, but notice
the asymmetric threshold values for the test <font color="#0000AA"><code>FTI &lt;= 61</code></font>.
For this application, soft thresholds slightly improve the classifier's
accuracy on both training and test data.

<p>
A final point: soft thresholds affect only decision tree classifiers --
they do not change the interpretation of rulesets.

</p><p>
</p><h3><a name="OTHER">Advanced pruning options</a></h3>

<p>
Three further options enable aspects of the classifier-generation
process to be tweaked.  These are best regarded as <i>advanced</i>
options
that should be used sparingly (if at all), so that this
section can be skipped without much loss.

</p><p>
See5 constructs decision trees in two phases.  A large tree is first
grown to fit the data closely and is then `pruned'
by removing parts that are predicted to have a relatively high error rate.
This pruning process is first applied to every subtree to decide
whether it should be replaced by a leaf or sub-branch, and then
a global stage looks at the performance of the tree as a whole.
</p><p>
Turning off the default <font color="maroon">Global pruning</font> option
disables this second pruning component and generally results in
larger decision tees and rulesets.  For the hypothyroid application,
the tree increases in size from 12 to 13 leaves.

</p><p>
Turning off global pruning can be beneficial for some applications,
particularly when rulesets are generated.

</p><p>
The <font color="maroon">Pruning CF</font> option
affects the way that error rates are estimated
and hence the severity of pruning;
values smaller than the
default (25%) cause more of the initial tree to be pruned,
while larger values result in less pruning.

</p><p>
The <font color="maroon">Minimum cases</font> option
constrains the degree to which the initial tree can fit the data.
At each branch point in the decision tree,
the stated minimum number of training cases must follow
at least two of the branches.
Values higher than the default (2 cases)
can lead to an initial tree that fits the training data only
approximately -- a form of pre-pruning.
(This option is complicated by the presence of missing attribute values,
and by the use of differential misclassification costs or weighting
of individual cases as discussed elsewhere.
All cause adjustments to the apparent number of cases following a branch.)

</p><p>
</p><h3><a name="SAMPLE">Sampling from large datasets</a></h3>

<p>
Even though See5 is relatively fast, building classifiers from
large numbers of cases can take an inconveniently long time,
especially when options such as boosting are employed.
See5 incorporates a facility to extract a random sample from
a dataset, construct a classifier from the sample, and then test
the classifier on a disjoint collection of cases.
By using a smaller set of training cases in this way, the
process of generating a classifier is expedited,
but at the cost of a possible reduction in the classifier's
predictive performance.

</p><p>
The <font color="maroon">Sample</font> option with <var>x</var>%
has two consequences.
Firstly, a random sample containing <var>x</var>% of the cases in
the application's <font color="green" style="background: #EEEEEE">data</font> file is used to construct the classifier.
Secondly, the classifier is evaluated on
a non-overlapping set of test cases consisting of
another (disjoint) sample of the same size as the training set
(if <var>x</var> is less than 50%),
or all cases that were not used in the training set
(if <var>x</var> is greater than or equal to 50%).

</p><p>
In the hypothyroid example,
using a sample of 60% would cause a classifier to be constructed
from a randomly-selected 1663 of the 2772 cases in
<font color="#0000AA"><code>hypothyroid.data</code></font>, then tested on the
remaining 1109 cases.

</p><p>
By default, the random sample changes every time that
a classifier is constructed, so that
successive runs of See5 with sampling will
usually produce different results.
This re-sampling can be avoided by selecting the
<font color="maroon">Lock sample</font> option that uses the current sample
for constructing subsequent classifiers.
If this option is selected, the sample will change only when
another application is loaded, the sample percentage is
altered, the option is unselected, or See5 is restarted.

</p><p>
</p><h3><a name="XVAL">Cross-validation trials</a></h3>

<p>
As we saw earlier, the performance of a classifier on the training
cases from which it was constructed gives a poor estimate of
its accuracy on new cases.
The true predictive accuracy of the classifier can be estimated
by sampling, as above, or by using a separate <font color="green" style="background: #EEEEEE">test</font> file;
either way, the classifier is evaluated on cases that were
not used to build it.
However, this estimate can be unreliable unless the numbers of
cases used to build and evaluate the classifier are both large.
If the cases in <font color="#0000AA"><code>hypothyroid.data</code></font> and
<font color="#0000AA"><code>hypothyroid.test</code></font> were to be shuffled
and divided into a new 2772-case training set and a 1000-case test set,
See5 might construct a different classifier with a lower or higher error
rate on the test cases.

</p><p>
One way to get a more reliable estimate of predictive accuracy
is by <i><var>f</var>-fold cross-validation</i>.  The cases
in the <font color="green" style="background: #EEEEEE">data</font> file
are divided into
<var>f</var> blocks of roughly the same size and class distribution.
For each block in turn, a classifier is constructed from the
cases in the remaining blocks and tested on the cases in the
hold-out block.  In this way, each case is used just once as
a test case.  The error rate of a classifier produced from
all the cases is estimated as the ratio of the total number of errors
on the hold-out cases to the total number of cases.

</p><p>
The <font color="maroon">Cross-validation</font> option with <var>f</var> folds
runs such a <var>f</var>-fold cross-validation.
Suppose now that we
select the <font color="maroon">Cross-validation</font> option with 10 folds
together with the <font color="maroon">Rulesets</font> option.
After giving details of the individual rulesets,
the output shows a summary like this:

</p><pre class="out"><font color="#000080">Fold            Rules     </font>
<font color="#000080">----      ----------------</font>
            <font color="#000080">No      Errors</font>

   0         7        0.5%
   1         8        0.5%
   2         8        0.8%
   3         7        0.3%
   4         7        0.8%
   5         7        0.0%
   6         7        0.8%
   7         7        0.0%
   8         7        1.3%
   9         7        0.8%

  <font color="#000080">Mean</font>     7.2        0.6%
  <font color="#000080">SE</font>       0.1        0.1%
</pre>

<p>
This estimates the error rate of the rulesets
produced from the
2772 cases in <font color="#0000AA"><code>hypothyroid.data</code></font>
at 0.5%.
The <i>SE</i> figures (the standard errors of the means)
provide an estimate of the variability of these results.


</p><p>
The cross-validation procedure can be repeated for
different random partitions of the cases into blocks.  The average
error rate from these distinct cross-validations is then an even more
reliable estimate of the error rate of the single classifier
produced from all the cases.


</p><p>
Since every cross-validation fold uses only part of the application's
data, running a cross-validation does not cause a classifier to be
saved.
To save a classifier for later use, simply
run See5 without employing cross-validation.


</p><p>
</p><h3><a name="COSTS">Differential misclassification costs</a></h3>

<p>
Up to this point, all errors have been treated as equal -- we have
simply counted the number of errors made by a classifier to
summarize its performance.
Let us now turn to the situation in which the `cost' associated
with a classification error depends on the predicted and true class
of the misclassified case.

</p><p>
See5 allows costs to be assigned to any combination of predicted and
true class via entries in the optional file
<font color="green" style="background: #EEEEEE">filestem</font><font color="#0000AA"><code>.costs</code></font>.
Each entry has the form

</p><p></p><pre>	<font color="green" style="background: #EEEEEE">predicted class</font><font color="#0000AA"><code>,</code></font> <font color="green" style="background: #EEEEEE">true class</font><font color="#0000AA"><code>:</code></font> <font color="green" style="background: #EEEEEE">cost</font>
</pre>

<p>
where <font color="green" style="background: #EEEEEE">cost</font> is any non-negative value.
The file may contain any number of entries;
if a particular combination is not specified explicitly, its
cost is taken to be 0 if the predicted class is correct and
1 otherwise.

</p><p>
To illustrate the idea, suppose that it was a much more serious
error to classify a hypothyroid patient as <font color="#0000AA"><code>negative</code></font>
than the converse.
A hypothetical <font color="green" style="background: #EEEEEE">costs</font> file <font color="#0000AA"><code>hypothyroid.costs</code></font>
might look like this:

</p><p>
</p><pre class="out">negative, primary: 5
negative, secondary: 5
negative, compensated: 5
</pre>

<p>
This specifies that the cost of misclassifying any
<font color="#0000AA"><code>primary</code></font>,
<font color="#0000AA"><code>secondary</code></font>, or
<font color="#0000AA"><code>compensated</code></font>
patient as <font color="#0000AA"><code>negative</code></font> is 5 units.
Since they are not given explicitly, all other errors
have cost 1 unit.
In other words, the first kind of error is five times more costly.

</p><p>
A <font color="green" style="background: #EEEEEE">costs</font> file is automatically read by See5 unless the
system is told to ignore it.
The output from the system using default
options now looks like this:

</p><pre class="out"><u>See5 [Release 2.09]</u>     <font color="#000080">Mon Jan 23 16:28:06 2012</font>

<font color="#000080">Class specified by attribute `diagnosis'</font>

<font color="#000080">Read 2772 cases (24 attributes) from hypothyroid.data</font>
<font color="#000080">Read misclassification costs from hypothyroid.costs</font>

<font color="#000080">Decision tree:</font>

TSH &lt;= 6: negative (2472/2)
TSH &gt; 6:
<font color="#000080">:...</font>FTI &lt;= 65:
    <font color="#000080">:...</font>thyroid surgery = f: primary (68.1/11.7)
    <font color="#000080">:   </font>thyroid surgery = t:
    <font color="#000080">:   :...</font>FTI &lt;= 36.1: negative (2.1)
    <font color="#000080">:       </font>FTI &gt; 36.1: primary (2.1/0.1)
    FTI &gt; 65:
    <font color="#000080">:...</font>on thyroxine = t: negative (37.7)
        on thyroxine = f:
        <font color="#000080">:...</font>thyroid surgery = t: negative (6.8)
            thyroid surgery = f:
            <font color="#000080">:...</font>TT4 &gt; 153: negative (6/0.1)
                TT4 &lt;= 153:
                <font color="#000080">:...</font>TT4 &lt;= 37: primary (2.5/0.2)
                    TT4 &gt; 37: compensated (174.6/24.8)


<font color="#000080">Evaluation on training data (2772 cases):</font>

               <font color="#000080">Decision Tree       </font>
          <font color="#000080">-----------------------  </font>
          <font color="#000080">Size      Errors   Cost  </font>

             9   11( 0.4%)   0.01   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            60     <font color="red">3</font>                <font color="#000080">(a): class primary</font>
             <font color="red">1</font>   153                <font color="#000080">(b): class compensated</font>
                               <font color="red">2</font>    <font color="#000080">(c): class secondary</font>
             <font color="red">4     1</font>        2548    <font color="#000080">(d): class negative</font>


        <font color="#000080">Attribute usage:</font>

             90%  TSH
             18%  thyroid surgery
             17%  on thyroxine
             14%  TT4
             13%  T4U
             13%  FTI


<font color="#000080">Evaluation on test data (1000 cases):</font>

               <font color="#000080">Decision Tree       </font>
          <font color="#000080">-----------------------  </font>
          <font color="#000080">Size      Errors   Cost  </font>

             9    4( 0.4%)   0.00   &lt;&lt;


           <font color="#000080">(a)   (b)   (c)   (d)    &lt;-classified as</font>
          <font color="#000080">----  ----  ----  ----</font>
            32                      <font color="#000080">(a): class primary</font>
             <font color="red">1</font>    39                <font color="#000080">(b): class compensated</font>
                                    <font color="#000080">(c): class secondary</font>
             <font color="red">1     2</font>         925    <font color="#000080">(d): class negative</font>


<font color="#000080">Time: 0.0 secs</font>
</pre>
The difference between this tree and the original
becomes apparent when we compare the total cost of misclassified
test cases for the two trees.
The misclassification cost is
8 (3x1 + 1x5) for the original tree and 4 (4x1) for the
new tree.

<p>
The new "Cost" column in the output shows the average misclassification
cost, i.e. the total cost divided by the number of cases.
For the new tree,  the average cost is 19/2772 for the training cases
and 4/1000 for the test cases.


</p><p>
</p><h3><a name="CASEWEIGHT">Weighting individual cases</a></h3>

<p>
It is sometimes useful to attach different weights to cases
depending on some measure of their importance.
An application predicting whether a customer is likely to "churn," for
example, might weight training cases by the size of the account.

</p><p>
See5 accommodates this by allowing a special attribute that contains
the weight of each case.  The attribute name must be
<font color="#0000AA"><code>case weight</code></font> and
it must be of type <font color="#0000AA"><code>continuous</code></font>.  The relative weight
assigned to each case is its value of this attribute divided by
the average value; if the value is undefined ("<font color="#0000AA"><code>?</code></font>"),
not applicable ("<font color="#0000AA"><code>N/A</code></font>"), or is less than or equal to zero,
the case's relative weight is set to 1.

</p><p>
<em>The case weight attribute itself is not used in the classifier!</em>

</p><p>
Our sample hypothyroid application does not have any natural case-by-case
weighting, since all patients are equal.  For the purpose of illustration,
though, we will add an implicitly-defined attribute to
<font color="#0000AA"><code>hypothyroid.names</code></font> as follows:
</p><ul>
	<font color="#0000AA"><code>case weight := 100-age.</code></font><br>
</ul>
With all default options, See5 now generates a different decision tree:
<p>

</p><pre class="out">TSH &lt;= 6: negative (2462.5/2.3)
TSH &gt; 6:
<font color="#000080">:...</font>FTI &lt;= 64.5: primary (69.6/12.8)
    FTI &gt; 64.5:
    <font color="#000080">:...</font>on thyroxine = t: negative (39.6)
        on thyroxine = f:
        <font color="#000080">:...</font>thyroid surgery = t: negative (9.2)
            thyroid surgery = f:
            <font color="#000080">:...</font>TT4 &gt; 153: negative (6.2/0.2)
                TT4 &lt;= 153:
                <font color="#000080">:...</font>TT4 &gt; 61: compensated (179/29.3)
                    TT4 &lt;= 61:
                    <font color="#000080">:...</font>TSH &lt;= 35: compensated (2.8/0.3)
                        TSH &gt; 35: primary (3.2/0.3)
</pre>
The case counts at the leaves now reflect the relative weights
of the cases.  (The counts associated with rules are affected similarly.)
However, the error counts, rates, and costs shown in the evaluations
use uniform case weighting.


<p>
A cautionary note: The use of case weighting does not guarantee that
the classifier will be more accurate for unseen cases with higher
weights.  Predictive accuracy on more important cases is likely
to be improved only when cases with similar values of the predictor
attributes also have similar values of the case weight attribute,
i.e. when relatively important cases "clump together."
Without this property, case weighting can introduce an unhelpful
element of randomness into the classifier generation process.

</p><p>
</p><h2><a name="USE">Using Classifiers</a></h2>

<p>
Once a classifier has been constructed, an <i>interactive interpreter</i>
can be used to
predict the classes to which new cases belong.

The <font color="maroon">Use Classifier</font> button invokes the interpreter, using
the most recent classifier for the current application,
and prompts for information about the case to be classified.
Since the values of all attributes may not be needed,
the attribute values requested will depend on the case itself.
When all the relevant information has been entered, the most likely
class (or classes) are shown, each with a confidence value.
For example, this is the result of analyzing a case using the
first decision tree above:

</p><ul>
<img src="./See5  An Informal Tutorial_files/see5-pred.gif" alt="Image of interpreter window" height="399" width="599">
</ul>

<p>
Classifiers can also be used in batch mode.
The sample application provided in the
<a href="http://rulequest.com/see5-win.html#HOOKS">public source code</a> 
reads cases from a <font color="green" style="background: #EEEEEE">cases</font> file and shows
the predicted class and the confidence for each.

</p><p>
</p><h2><a name="XREF">Cross-Referencing Classifiers and Data</a></h2>

<p>
See5 incorporates a unique facility that links data and the
relevant sections of (possibly boosted) classifiers.
We will illustrate this facility using the first decision tree
for the hypothyroid application and the cases in <font color="#0000AA"><code>hypothyroid.data</code></font>
from which it was constructed.

</p><p>
The <font color="maroon">Cross-Reference</font> button brings up a window showing
the most recent classifier for the current application
and how it relates to the cases in the <font color="green" style="background: #EEEEEE">data</font>, <font color="green" style="background: #EEEEEE">test</font>
or <font color="green" style="background: #EEEEEE">cases</font> file.  (If more than one of these is present,
a menu will prompt you to select the file.)

</p><p>
The window is divided into two panes, with the classifier on the
left and a list of cases on the right.
The <font color="maroon">Reset</font> button can be used at any time to restore the window to
this initial state.

</p><p>
Each case has a <b>[?]</b> tag (that is red if the case is
misclassified), an identifying number or label, and the class predicted
for the case (also red when incorrect).
Clicking on the tag <b>[?]</b>
in front of a case number or label displays that case:

</p><ul>
<img src="./See5  An Informal Tutorial_files/see5-xr3.gif" alt="X-Ref window" height="468" width="295">
</ul>
The values of label attributes and attributes excluded or ignored
are displayed in a lighter tone to indicate that they play no part
in classifying the case.

<p>
Clicking on a case's label or number shows the part(s) of the
classifier(s) relevant to that case.  For instance, clicking
on case <b>3169</b> shows the leaf to which this case is mapped:

</p><ul>
<img src="./See5  An Informal Tutorial_files/see5-xr1.gif" alt="X-Ref window" height="462" width="689">
</ul>

<p>
If a case has missing values for one or more attributes,
if it is covered by several rules, or if boosted classifiers
are used, more than one leaf or rule may be relevant to a case.
In such situations, all relevant classifier parts are shown.

</p><p>
Click on any leaf or rule, and all the cases that
map to the leaf or rule are shown.  For instance,
clicking on <font color="green" style="background: #EEEEEE">Reset</font> and then
the leaf indicated shows all cases that are
covered by that leaf:

</p><ul>
<img src="./See5  An Informal Tutorial_files/see5-xr2.gif" alt="X-Ref window" height="462" width="689">
</ul>

<p>
This last pane may be puzzling for two reasons:
</p><ul>
<li>The case pane shows thirteen cases but the count shown at the leaf is 4.9.
This happens because some of these thirteen cases have unknown values
for the attributes tested on the path to this leaf (TSH, FTI,
thyroid surgery, TT4, referral source).  Cases like this are
split into partial cases associated with each outcome of the test.
</li><li>This leaf predicts class <font color="#0000AA"><code>primary</code></font>, but some
cases belonging to other classes are not highlighted in red to
indicate an error.  As noted above,
parts of a case split as a result of unknown attribute values can
be misclassified and yet, when the votes from all the parts are aggregated,
the correct class can still be chosen.  Cases 3469, 3640, 2266,
919, 3111, and
3607 are classified correctly by the decision tree as a whole.
</li></ul>

<p>
The <font color="maroon">Save</font> button preserves the details of the displayed
classifier and case list as an ASCII file selected through
a dialog box.

</p><h2><a name="CONSOLE">Generating Classifiers in Batch Mode</a></h2>

<p>
The See5 distribution includes a program <b>See5X</b> that
can be used to produce classifiers non-interactively.
This console application resides in the same folder as See5
(usually <font color="#0000AA"><code>C:\Program Files\See5</code></font> for single-computer licences
or the <font color="#0000AA"><code>See5</code></font> folder on your desktop for network licences)
and is invoked from
an MS-DOS Prompt window.  The command to run the program is:

</p><pre>	<font color="#0000AA"><code>start /B See5X -f</code></font> <font color="green" style="background: #EEEEEE">filestem</font> <font color="green" style="background: #EEEEEE">parameters</font></pre>

where the parameters enable one or more options discussed above to
be selected:

<ul><ul>
<table cellpadding="0">
<tbody><tr>
  <td><font color="#0000AA"><code>-s</code></font>
  </td><td>use the <font color="maroon">Subset</font> option
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-r</code></font>
  </td><td>use the <font color="maroon">Ruleset</font> option
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-u</code></font> <font color="green" style="background: #EEEEEE">bands</font>
  </td><td>sort rules by their utility into bands
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-b</code></font>
  </td><td>use the <font color="maroon">Boosting</font> option with 10 trials
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-t</code></font> <font color="green" style="background: #EEEEEE">trials</font>
  </td><td>ditto with specified number of trials
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-w</code></font>
  </td><td>winnow attributes before constructing a classifier
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-S</code></font> <font color="green" style="background: #EEEEEE">x</font>
  </td><td>use the <font color="maroon">Sampling</font> option with x%
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-I</code></font> <font color="green" style="background: #EEEEEE">seed</font>
  </td><td>set the sampling seed value
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-X</code></font> <font color="green" style="background: #EEEEEE">folds</font>
  </td><td>carry out a cross-validation
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-g</code></font>
  </td><td>turn off the global tree pruning stage
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-c</code></font> <font color="green" style="background: #EEEEEE">CF</font>
  </td><td>set the <font color="maroon">Pruning CF</font> value
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-m</code></font> <font color="green" style="background: #EEEEEE">cases</font>
  </td><td>set the <font color="maroon">Minimum cases</font>
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-p</code></font>
  </td><td>use the <font color="maroon">Fuzzy thresholds</font> option
</td></tr>
<tr>
  <td><font color="#0000AA"><code>-e</code></font>
  </td><td>ignore any <font color="green" style="background: #EEEEEE">costs</font> file
</td></tr>
</tbody></table>
</ul></ul>

<p>
If desired, output from See5 can be diverted to a file in the usual way.

</p><p>
As an example (for a single-computer licensee), typing the commands

</p><pre><font color="#0000AA"><code>	cd "C:\Program Files\See5"
	start /B See5X -f Samples\anneal -r -b &gt;save.txt
</code></font></pre>

in a MS-DOS Prompt window
will generate a boosted ruleset classifier for the
<font color="#0000AA"><code>anneal</code></font> application in the Samples directory,
leaving the output in file <font color="#0000AA"><code>save.txt</code></font>.

<h2><a name="HOOKS">Linking to Other Programs</a></h2>

<p>
The classifiers generated by See5 are retained in files
<font color="green" style="background: #EEEEEE">filestem</font><font color="#0000AA"><code>.tree</code></font> (for decision trees) and
<font color="green" style="background: #EEEEEE">filestem</font><font color="#0000AA"><code>.rules</code></font> (for rulesets).
Free C source code is available
to read these classifier files and to make predictions with them,
enabling you to use See5 classifiers in other
programs.

</p><p>
As an example, the source includes a program to input new cases
and to show how each is classified
by boosted or single trees or rulesets.
The program reads the application's <font color="green" style="background: #EEEEEE">names</font> file, the
<font color="green" style="background: #EEEEEE">tree</font> or <font color="green" style="background: #EEEEEE">rules</font> file generated by See5,
and an optional <font color="green" style="background: #EEEEEE">costs</font> file.  It then reads cases from
a <font color="green" style="background: #EEEEEE">cases</font> file in a format similar to a <font color="green" style="background: #EEEEEE">data</font>
file, except that a case's class can be given as `<font color="#0000AA"><code>?</code></font>' meaning
"unknown".  For each case, the program outputs the given class, the class
predicted by the classifier, and the confidence with which this prediction
is made.

</p><p>
Please see the file <font color="#0000AA"><code>sample.c</code></font> for compilation
instructions and program options.

</p><p>
<a href="http://www.rulequest.com/see5-public.zip"><b>Click here</b></a>
to download a zip archive
containing the public source code.




</p><p>
<table width="100%">
  <tbody><tr>
    <td align="left">
      <b>© <small>RULEQUEST RESEARCH 2012</small></b>
    </td>
    <td align="right">
      Last updated January 2012
    </td>
  </tr>
</tbody></table>

</p><hr>

<table width="100%" cellpadding="0" cellspacing="0">
  <tbody><tr>
    <td align="center" bgcolor="yellow"><a href="http://www.rulequest.com/index.html">
	<small><b>home</b></small></a>
    </td><td align="center" bgcolor="yellow"><a href="http://www.rulequest.com/products.html">
	<small><b>products</b></small></a>
    </td><td align="center" bgcolor="yellow"><a href="http://www.rulequest.com/licensing.html">
	<small><b>licensing</b></small></a>
    </td><td align="center" bgcolor="yellow"><a href="http://www.rulequest.com/download.html">
	<small><b>download</b></small></a>
    </td><td align="center" bgcolor="yellow"><a href="http://www.rulequest.com/contact.html">
	<small><b>contact us</b></small></a>
  </td></tr>
</tbody></table>


</body></html>