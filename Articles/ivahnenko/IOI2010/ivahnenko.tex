\documentclass[twoside]{article}
\usepackage{iip8}
%\NOREVIEWERNOTES

\begin{document}
\title{Вероятность переобучения конъюнкций пороговых предикатов над вещественными признаками}
\author{Ивахненко~А.\,А.}
\thanks{Работа выполнена при финансовой поддержке РФФИ, проект \No\,00-00-00000.}
\email{andrej\_iv@mail.ru}
\organization{Москва, Вычислительный центр им. А. А. Дородницына РАН}
\abstract{
В~данной работе рассматривается задача классификации с помощью взвешенного голосования предикатов из семейства пороговых условий. Описывается структура семейства этих предикатов. Для данного семейства предикатов выводится верхняя оценка функционала качества предиката. Показывается эффективный способ вычисления оценки.
}
\titleEng{Overfitting probability of threshold predicate conjunctions on real feature values}
\authorEng{Ivahnenko~A.\,A.}
\organizationEng{Institution of Russian Academy of Sciences Dorodnicyn Computing Centre of RAS, Moscow, Russia}

\maketitle


В~данной работе рассматриваются алгоритмы классификации, основанные на~поиске логических закономерностей по~эмпирическим данным.

Целью работы является установление зависимости между свойствами алгоритмов на обучающей и контрольных выборках.

При решении задач классификации с~неточными, неполными, разнородными данными одной из основных проблем является обеспечение наилучшего возможного качества классификации вне обучающей выборки.

\section{Задача классификации}
\label{sec:ClassificationProblem}

В данной работе задача классификации ставится следующим образом.\\
$\mathbb{X}$~--- множество \emph{объектов};\\
$Y = \{0, 1\}$~--- множество возможных ответов;\\
$X^L=\{x_i\},\;i=1,\dots,L,\; x_i\in \mathbb{X}$~--- \emph{полная выборка};\\
$Y^L=\{y_i\},\;i=1,\dots,L,\; y_i\in Y$~--- \emph{вектор ответов}.

\emph{Признаком} называется отображение $f \colon \mathbb{X} \rightarrow D_f$, описывающее результат измерения некоторой характеристики объекта, где $D_f$~--- заданное множество. В данной работе рассматриваются признаки, для которых $D_f = \RR$.

Пусть имеется набор признаков $f_1,\dots, f_n$. Вектор $(f_1(x), \dots, f_n(x))$ называют признаковым описанием объекта $x \in \mathbb{X}$. В дальнейшем мы не будем различать объекты из $\mathbb{X}$ и их признаковые описания, полагая $\mathbb{X}= D_{f_1} \times \dots \times D_{f_n}$. Совокупность признаковых описаний всех объектов выборки $X^L$, записанную в виде таблицы размером $L\times n$, называют \emph{матрицей объектов–признаков}: $F = ||f_j(x_i)||,\; i=1,\dots,L,\; j=1,\dots,n.$

Допустим, что значения каждого признака $f_j$ на всех объектах выборки $X^L$ попарно различны. Пусть $f_j(x_j^{(i)})$~-- $i$-ый элемент в вариационном ряду значений $j$-ого признака $f_j(x_j^{(1)}) < \dots < f_j(x_j^{(L)})$. Построим по матрице $F$ целочисленную матрицу $F'$ заменив каждое значение $f_j(x_j^{(i)})$ на порядковый номер $i$. В матрице $F'$ сохранен порядок объектов по каждому признаку и она удобнее для дальнейшего рассмотрения, т.к. номер значения по порядку совпадает с самим значением. В дальнейшем будем использовать матрицу $F'$ в качестве матрицы объектов-признаков.

\emph{Алгоритмом классификации} называется функция $\phi\colon \mathbb{X}, W\rightarrow Y$, где $W$~-- множество параметров. В данной работе в качестве алгоритма классификации используется взвешенное голосование закономерностей $\phi(x) = \arg\max_{y\in Y}\sum_{a\in R_y}w_a a(x)$, где $R_y$~-- множество предикатов $a$ класса $y$, $w_a\in \RR$~-- вес голоса предиката $a$. Говорят что предикат $a\in R_y$ относит объект $x\in X$ к классу $y\in Y$, если $a(x)=1$. Будем рассматривать предикаты из семейства:
\begin{align}
    A=\Bigl\{a(x;c_1,\dots,c_n)=\bigwedge\limits_{j=1}^n[x^j \lessgtr_j c_j]\colon
    \notag
    \\
    x=(x^1, \dots,x^n)\in X^L,\;c_j\in\RR,\; \lessgtr_j\in\{\leq,\geq\} \Bigr\},
    \notag
\end{align}
где $x^j\equiv f_j(x)$~--- значение $j$-го признака объекта $x$. Каждому предикату $a$ соответствует $L$-мерный бинарный вектор ошибок $\bar a=\bigl(a(x_i)\neq y_i\bigr)_{i=1}^L$. Имеет смысл рассматривать только предикаты, для которых пороги $c_j$ берутся из множества значений $j$-ого признака ($1,\dots,L$) и нуля, поэтому число различных векторов ошибок для данной матрицы $X^L$ не превысит $(L+1)^n$. Введем отношение эквивалентности на множестве предикатов $a\sim a' \leftrightarrow \bar a=\bar a'$. Структура классов эквивалентности будет рассмотрена дальше. Множество векторов ошибок будем обозначать так:
\[
\bar A = \{\bar a,\; a\in A\}.
\]

Процесс подбора предикатов и настройки их порогов по обучающей выборке $X^\ell\subset X^L$ называют \emph{настройкой} или \emph{обучением}.
\emph{Методом обучения предиката} называется отображение $\mu\colon (X \times Y)^\ell \rightarrow a$, которое произвольной конечной выборке $X^{\ell}$ ставит в соответствие предикат $a\colon X \rightarrow Y$. Будем говорить также, что метод $\mu$ строит предикат $a$ по выборке $X^{\ell}$ и записывать $a=\mu(X)$ или $a=\mu X$.

Как правило, обучение сводится к поиску параметров модели, доставляющих оптимальное значение заданному функционалу качества.

\emph{Число ошибок} предиката $a$ на произвольной выборке $X\subseteq X^L$ есть $n(a,X) = \sum_{x_i\in X}[a(x_i)\neq y_i]$.

\emph{Частота ошибок} предиката $a$ на произвольной выборке $X\subseteq X^L$ есть $\nu(a,X) = \frac{n(a,X)}{|X|}$.

Если предикат $a$ доставляет минимум функционалу $\nu(a,X^\ell)$ на заданной обучающей выборке $X^{\ell}\subset X^L$, то это ещё не гарантирует, что он будет иметь малую частоту ошибок на контрольной выборке $X^k = X^L\backslash X^\ell$. Для построения хорошего алгоритма $\phi(x)$ нужно уметь подбирать предикаты имеющие малую частоту ошибок на контроле. Поиск способа отбора таких предикатов и является главной целью исследования в данной работе.

Когда частота ошибок на контроле $\nu(a,X^k)$ оказывается существенно больше частоты ошибок на обучении $\nu(a,X^\ell)$, говорят об эффекте \emph{переобучения} или переподгонки. Также говорят, что данный предикат обладает плохой \emph{обобщающей способностью}.

Разобъём полную выборку $X^L = \{x_i\},\; i=1,\dots,L$ всеми $C_L^\ell$ способами на две части: $X^L=X^{\ell} \sqcup X^k$~--- обучающую подвыборку длины $\ell$ и контрольную длины $k$, где $\ell+k=L$. Договоримся, что $\sum\limits_{X^\ell,X^k}$ обозначает сумму по всем $C_L^\ell$ разбиениям. В данной работе для определения обобщающей способности будем использовать функционал характеризующий вероятность переобучения:

\begin{equation}
\label{eq:Qeps}
Q_\varepsilon(\mu, X^L) = \frac{1}{C_L^\ell}\sum_{X^\ell,X^k}\bigl[\nu(\mu X^\ell, X^k) - \nu(\mu X^\ell,X^\ell) > \varepsilon\bigr].
\end{equation}


\section{Постановка задачи}

Для решения задачи классификации обычно используется поиск оптимального в смысле некоторого функционала качества представителя параметрического семейства алгоритмов. При этом оптимизация использует как минимум два механизма: отбор признаков и оптимизации параметров модели алгоритма. Нашей целью является предложение методики совмещения этих механизмов так, чтобы информация о семействе алгоритмов, полученная в процессе оптимизации, использовалась при отборе признаков.

\section{Структура классов эквивалентности семейства предикатов}
\label{sec:ClusterStructure}

Будем говорить что классы эквивалентности предикатов связаны, если векторы ошибок представителей этих классов различаются на одном объекте. Число классов эквивалентности предикатов, и связи между ними зависят от значений признаков всех объектов выборки~$x_i$, и не зависят от их классификации~$y_i, i=1,\dots,L$.

Пусть $u$ и $v$~-- произвольные объекты, будем говорить что объект $u$ \emph{доминируется} объектом $v$ по координате $j$, если $u^j<v^j$. Будем говорить что объект $u$ \emph{доминируется} множеством объектов $S \subseteq X^L$, если для каждого $j\in \{1,\dots,n\}$ существует объект $s\in S$, такой, что $u^j < s^j$ (записывается: $u\prec S$). Если множество $S$ состоит из одного элемента $s$ и $u\prec S$, то будем писать $u\prec s$.


\begin{Def}
\label{def:NonDominatedObjects}
\emph{Недоминирующимся подмножеством}~(НП) будем называть такое подмножество $S\subseteq X^L$, что любой объект~$s\in S$ не доминируется подмножеством $S\backslash s$. Обозначим множество всех таких подмножеств мощности $q$ через $M_q$,

\[
M_q=\left\{S\subseteq X^L\colon |S|=q,\; \forall s\in S\; s \nprec S\backslash s \right\}.
\]
\end{Def}

Введем искусственный НП $S_0$, который будет состоять из одного объекта, $x_0=(0,\dots,0)$. Обозначим $M_0=\{S_0\}$. Предикат $a(x;0,\dots,0)$ будем обозначать $a_0$.

Очевидно, мощность НП $|S|$ не может превышать~$n$. Если $S$ является НП, то любое подмножество $S'\subset S$ также является НП. Из этого утверждения следует эффективный способ построения всех множеств НП: для построения $M_{q+1}$ добавим к каждому множеству из $M_q$ один объект, еще не входящий в него; и если полученное множество является НП, то оно войдет в $M_{q+1}$. Также получается и грубая оценка числа множеств НП: $|M_0|=1,\; |M_1|=L,\; |M_{q+1}| \leq \frac{L-q}{q+1}|M_q|,\; q=1,\dots,n-1$.

\begin{Lemma}
Для любого объекта $x$ из НП $S$ найдется хотя бы один признак $j\in\{1,\dots,n\}$, по которому на данном $x$ достигается $\max\limits_{s\in S} s^j$:

\[
\bigcup\limits_{j=1}^{n}\mathrm{Arg}\max\limits_{s\in S}(s^j)=S.
\]
\end{Lemma}

Поставим в соответствие подмножеству $S$ предикат $a(x, S)=a(x; \max\limits_{x\in S} x^1,\dots,\max\limits_{x\in S}x^n)$. Очевидно, разным~$S$ ставятся в соответствие разные $a(x,S)$, т.\:к. значения каждого признака $x_i^j,\; i=1,\dots,L$ попарно различны, следовательно, зная $j$ и $x_i^j$, можно однозначно указать объект $x_i$. Следовательно, набор параметров $c_j=\max\limits_{x\in S}x^j,\;j=1,\dots,n$ задает подмножество $S$ однозначно, и различным $S$ не могут соответствовать одинаковые $a(x, S)$.

\begin{Example}
Рассмотрим задачу с $n=2$ признаками и $L=10$ объектами (рис.~\ref{fig:AlgClusters}) и семейство предикатов
\[
A=\{a(x;c_1, c_2)=[x^1\leq c_1 \wedge x^2\leq c_2]\colon x\in X^L\}.
\]
Поскольку каждый из параметров $c_1, c_2$ принимает значения $0,\dots,L$, $|A|=(L+1)^2$, и каждый предикат из $A$ задается парой порогов $(c_1,c_2)$. Таким образом, каждому предикату из семейства можно поставить в соответствие узел прямоугольной сетки $H=\{0,\dots,L\}^2$. Объекты выборки также лежат в $X^L\subset H$, при этом каждый объект обладает уникальным значением каждого из своих признаков в силу того, что значения признаков попарно различны. Это означает, что никакие два объекта из $X^L$ не могут лежать на одной вертикали или на одной горизонтали в сетке $H$, см.~рис.~\ref{fig:AlgClusters}.


\begin{figure}[H]
\begin{center}
    \noindent
    \includegraphics[width=85mm,height=65mm]{correctAlgorithm.eps}
    \caption[классы эквивалентности предикатов]{
        Векторы ошибок предикатов обозначены точками, координаты которых совпадают с порогами предиката ($c_1,c_2$), соответствующего данному вектору. Одинаковые векторы ошибок соединены линиями и образуют классы эквивалентности. Около каждого класса эквивалентности подписано, сколько ошибок допускают предикаты этого класса на выборке $X^L$. На рисунке также отмечены объекты выборки.
    }
    \label{fig:AlgClusters}
\end{center}
\end{figure}

Рассмотрим множество объектов на~рис.~\ref{fig:AlgClusters}. Подмножествами взаимно не доминирующих объектов мощности $1$ являются все объекты выборки. Подмножествам взаимно не доминирующих объектов мощности $2$ является, например, подмножество объектов $(2,5)$ и $(5,1)$. Объект $(5,1)$ имеет большее значение первого признака, объект $(2,5)$~-- второго. Подмножество из объектов $(2,5)$ и $(3,6)$ не является НП, т.\:к. первый объект доминируется вторым. НП мощности больше $2$ нет, т.\:к. размерность задачи $n=2$.
\end{Example}


\begin{Lemma}
Пусть $E\subset A$~--- класс эквивалентности. Тогда предикат
\[
a_E(x)=a(x; \min\limits_{a'\in E}c_1(a'), \dots, \min\limits_{a'\in E}c_n(a'))
\]
принадлежит $E$, где $c_j(a')$ -- $j$-ый параметр предиката~$a'$
\end{Lemma}


Будем называть предикат $a_E(x)$ \emph{стандартным представителем} класса эквивалентности~$E$. В частности, при размерности $n=2$ на рис.~\ref{fig:AlgClusters} у каждого класса эквивалентности стандартным представителем является предикат $a(x;c_1,c_2)$, соответствующий левой нижней точке с координатами $(c_1,c_2)$. Например $(5,5); (7,3); (0,0)$.

\begin{Theorem}
\label{th:AlgClusterCount}
Существует взаимно однозначное соответствие между множеством всех классов эквивалентности и множеством всех подмножеств недоминирующих объектов.
\end{Theorem}

\begin{Corollary}
Для каждого класса эквивалентности $E$ существует и притом единственное НП $S$, такое что $a_E(x)=a(x,S)$
\end{Corollary}

\begin{Corollary}
Число классов эквивалентности предикатов равно $\sum\limits_{q=0}^n |M_q|$.
\end{Corollary}

\section{Оценка вероятности переобучения}

Для оценки функционала $Q_\varepsilon(\mu, X^L)$ будем использовать методологию, описанную в \cite{voronMMRO14}. В этой работе выдвигается гипотеза о том, что предикат $a$ будет выбран на разбиении $X^L=X^\ell \sqcup X^k$ тогда и только тогда, когда объекты, называемые \emph{порождающими}, принадлежат обучающей выборке $X^\ell$, а объекты, называемые \emph{запрещающими}, принадлежат контрольной выборке $X^k$:

\begin{equation}
\label{eq:muX}
[\mu X^\ell=a]=\sum_{v\in V_a}c_{av}[X_{av}\subseteq X^\ell][X'_{av}\subseteq X^k],
\end{equation}
где $X_{av}$~-- множество порождающих объектов, $X'_{av}$~-- множество запрещающих объектов, $V_a$~-- множество индексов пар порождающих и запрещающих множеств для предиката $a$.

Рассмотрим множества пар порождающих и запрещающих множеств. Будем называть множества объектов $X_a = \bigcap_{v\in V_a} X_{av}$ и $X'_a = \bigcap_{v\in V_a} X'_{av}$ \emph{фиксированными для предиката $a$}. Это объекты, которые необходимо должны присутствовать во всех порождающих или запрещающих множествах для того, чтобы метод обучения $\mu$ выбрал предикат $a$. Тогда гипотезу \ref{eq:muX} можно переписать в виде оценки:

\[
[\mu X^\ell=a]\leq [X_a \subseteq X^\ell][X'_a \subseteq X^k]
\]

Пусть для предиката $a$, множества $X_a$ и $X'_a$ являются фиксированными на обучении и контроле соответственно. Число ошибок предиката $a$ на объектах не из фиксированных множеств $m_a$. Число не фиксированных объектов на обучении $\ell_a=\ell-|X_a|$, число не фиксированных объектов на контроле $k_a=k-|X'_a|$, число не фиксированных объектов на всей выборке $L_a = \ell_a + k_a$. Наибольшее число ошибок предиката $a$ на выборке без фиксированных объектов $s_a(\varepsilon) = \frac{\ell_a}{L_a}(n(a,X^L\backslash(X_a\cup X'_a))-\varepsilon k_a)$. Доля разбиений для которых $X_a\subseteq X^\ell$ и $X'_a\subseteq X^k$ равна $P_a = C_{L_a}^{\ell_a}/C_L^\ell$. Справедлива следующая оценка:

\begin{Theorem}
\label{th:FixEstimation}
\[
Q_\varepsilon(\mu, X^L) \leq \sum_{a\in A} P_a H^{\ell_a,m_a}_{L_a}(s_a(\varepsilon)).
\]
\end{Theorem}

\begin{Lemma}
\label{lem:Fix1}
Объекты из НП $S$, такого, что $a(x; S)\sim a(x)$ являются фиксированными для предиката $a$. Причем объекты класса $1$ принадлежат $X_a$, а объекты класса $0$~-- $X'_a$.
\end{Lemma}

\begin{Lemma}
\label{lem:Fix2}
Пусть векторы ошибок предикатов $u$ и $v$ различаются на одном объекте $x$ класса $y$. Пусть $u(x)=y$, а $v(x)\neq y$. Тогда для того что бы при выборе предикатов с помощью пессимистичного метода минимизации эмпирического риска был выбран предикат $v$ необходимо что бы объект $x$ оказался в контроле. Таким образом $x\in X'_v$. А для того что бы был выбран предикат $u$ необходимо что бы $x\in X_u$.
\end{Lemma}

С помощью данных лемм строится часть фиксированных объектов. Рассмотрим граф связей между предикатами. Каждая вершина этого графа это предикат, а связь между вершинами присутствует если векторы ошибок предикатов различаются на одном объекте. Упорядочим все предикаты по числу ошибок на полной выборке $X^L$, и посетим каждую из них. Допустим мы находимся в вершине соответсвущей предикату $u$, тогда по лемме \ref{lem:Fix2} мы можем указать какие объекты отнести к контрольным для всех предикатов $\{v\}$ имеющих на одну ошибку больше и в то же время расширим число фиксированных объектов на обучении для предиката $u$. По лемме \ref{lem:Fix1} мы так же можем зафиксировать несколько объектов для предиката $u$.

\begin{figure}[H]
\begin{center}
    \noindent
    \includegraphics[width=80mm,height=45mm]{links.eps}
    \caption[граф связей между предикатами]{
        На рисунке изображен граф связей предикатов. Узлами графа являются стандартные представители классов эквивалентности. Предикаты связаны если их векторы ошибок различаются на одном объекте.
    }
    \label{fig:Links}
\end{center}
\end{figure}

В классической теории Вапника \cite{vapnik} есть оценка с такой же структурой $Q_\varepsilon\leq\sum_{a\in A}H^{\ell,m_a}_{L}(\frac{\ell}{L}(m_a-\varepsilon k))$. Оценка в теореме \ref{th:FixEstimation} более точна за счет учета структуры семейства предикатов. С другой стороны она отличается от точной формулы только вероятностью $P_a$. Чем точнее будет вычислена вероятность, тем точнее будет оценка.

\section{Эксперимент}

Возьмем $3$ выборки $L=100, n=2$. Будем условно называть их <<Correct>>, <<Noise>> и <<Random>>. Все выборки имеют одинаковые объекты и разную их классификацию. Выборка <<Correct>> имеет единственный полностью разделяющий классы предикат $a$ такой что $n(a, X^L)=0$. Выборка <<Noise>> получается из <<Correct>> небольшим зашумлением: $4$-ём объектам рядом с границей классов меняют класс на противоположный так, что бы не существовало предиката с $n(a,X^L)=0$. Выборка <<Random>> получается случайным назначением объектам классов. Для всех выборок число объектов класса $1$ равно числу объектов класса $0$.

Рассмотрим точную оценку функционала \ref{eq:Qeps} с помощью метода Монте-Карло. Для этого проведем $100$ случайных разбиений выборки $X^L=X^\ell\sqcup X^k$. Для каждого разбиения посчитаем вероятность переобучения предиката выбранного с помощью метода минимизации эмпирического риска при заданном $\varepsilon$, см.рис\ref{fig:Presize}.

\begin{figure}[H]
\begin{center}
    \noindent
    \includegraphics[width=85mm,height=65mm]{Presize.eps}
    \caption[точная оценка вероятности переобучения]{
        На графике изображена зависимость функционала $Q_\varepsilon(\mu,X^L)$ от $\varepsilon$. Разные линии соответствуют разным выборкам.
    }
    \label{fig:Presize}
\end{center}
\end{figure}

Рассмотрим оценку функционала \ref{eq:Qeps} с помощью фиксированных объектов. Некоторые фиксированные объекты выписываются с помощью двух лемм.

\begin{figure}[H]
\begin{center}
    \noindent
    \includegraphics[width=85mm,height=65mm]{Fix.eps}
    \caption[оценка вероятности переобучения с учетом фиксированных точек]{
        На графике изображена зависимость оценки функционала $Q_\varepsilon(\mu,X^L)$ от $\varepsilon$ с учетом фиксированных точек. Разные линии соответствуют разным выборкам.
    }
    \label{fig:Fix}
\end{center}
\end{figure}

Из графика видно, что завышенность оценки тем больше, чем хуже закономерность содержащаяся в выборке. Сравним оценку с помощью фиксированных точек и точную оуенку методом Монте-Карло для выборок <<Correct>> и <<Noise>>.

\begin{figure}[H]
\begin{center}
    \noindent
    \includegraphics[width=85mm,height=65mm]{PresizeVsFix.eps}
    \caption[оценка вероятности переобучения с учетом фиксированных точек]{
        На графике изображено сравнение оценки функционала $Q_\varepsilon(\mu,X^L)$ от $\varepsilon$ с учетом фиксированных точек и точной оценки этого функционала.
    }
    \label{fig:PresizeVsFix}
\end{center}
\end{figure}


\section{Выводы}
В данной работе рассмотрена структура семейства предикатов пороговых условий. Выведена грубая оценка числа предикатов в семействе в зависимости от состава выборки. Предложен эффективный алгоритм перечисления всех классов эквивалентности предикатов. Выведена верхняя оценка функционала вероятности переобучения, которая может быть эффективно посчитана и лучше чем оценка Вапника за счет учета структуры семейства предикатов. Предложена постановка задачи использования структуры семейства предикатов для более точного решения задачи классификации. Дальнейшие исследования будут направлены на увеличение точности оценки и разработки методики применения результатов данной работы для решения задачи классификации.

\begin{thebibliography}{1}
\bibitem{voronMMRO14}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Комбинаторный подход к проблеме переобучения.}
\bibitem{vapnik}
    \BibAuthor{Вапник\;В.\,Н.}
    Восстановление зависимостей по эмпирическим данным."---
    М.:~Наука, 1979.
\end{thebibliography}

\end{document}
