\documentclass[12pt]{article}

\usepackage[cp1251]{inputenc}
\usepackage[russian]{babel}
\usepackage{color}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage[noend]{algorithmic}
\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
\def\algorithmicgoto{\textbf{перейти к шагу}}
\def\algorithmicreturn{\textbf{вернуть}}

\newcommand{\LINEIF}[2]{%
    \STATE\algorithmicif\ {#1}\ \algorithmicthen\ {#2} %
}
\newcommand{\GOTO}[1]{%
    \algorithmicgoto\ {\ref{#1};} %
}


\textheight=26cm
\textwidth=17cm
\oddsidemargin=0mm
\topmargin=-20mm
\parindent=24pt
\tolerance=500
%\renewcommand{\baselinestretch}{1.3} %для печати с большим интервалом

\newcommand{\Expect}{\mathsf{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\X}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\def\brop#1{#1\discretionary{}{\hbox{$#1$}}{}} % перенос знака операции на следующую строку
\renewcommand{\em}{\it}
\newcommand{\sign}{\mathop{\rm sign}\limits}

\def\CCfont#1{\ifmmode{\mathsf{#1}}\else\mbox{\rm\textsf{#1}}\fi}
\def\P{\CCfont{P}}

\newcommand{\hypergeom}[5]{{#1}_{#2}^{#4,#3}(#5)}
\newcommand{\hyper}[4]{\hypergeom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\hypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\hypergeom{\bar{H}}{#1}{#2}{#3}{#4}}
\newcommand{\Binom}[2]{C_{#1}^{#2}}
%\newcommand{\Binom}[2]{\binom{#1}{#2}}
\newcommand{\CLl}{\Binom{L}{\ell}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}

\newcommand{\mur}[3]{\mu({#1}, {#2}, {#3})}
\newcommand{\todo}{\textcolor{red}{[ToDo]} }

\theoremstyle{plain}
\newtheorem{Theorem}{Теорема}
\newtheorem{Lemma}[Theorem]{Лемма}
%\newtheorem{State}[Theorem]{Утверждение}
\theoremstyle{definition}
\newtheorem{Def}{Определение}
\newtheorem{Definition}[Def]{Определение}
\newtheorem{Corollary}{Следствие}
\newtheorem{Hypothesis}{Гипотеза}
\newtheorem{Task}{Задача}
\newtheorem{Example}{Пример}

\newcommand{\vkEndProof}{\hfill$\scriptstyle\blacksquare$\par\medskip}
\newenvironment{vkProof}[1][. ]%
    {\par\noindent{\bf Доказательство#1}}%
    {\vkEndProof}

% Проглотить следующий пробел
\makeatletter
\def\gobblespace{\@ifnextchar\ {\hspace{-1ex}}\relax}
\makeatother
% Вставка замечания рецензента
\newcommand\REVIEWERNOTE[1]{{%
    \itshape\bfseries%\color{red}%
    \marginpar{%\raisebox{-1ex}{%\color{red}%
        $\checkmark$%\!_{\themmroReviewerNote}
    }%}%
    \{#1\}
}\gobblespace}

\begin{document}

\begin{center}
{\Large \bf Обобщение оценки расслоения-связности на случай произвольного графа Хассе}

\vspace{0.2cm}

{\bf Фрей Александр, Решетняк Илья.}

\vspace{0.1cm}

\end{center}

%\abstract {
%}

\tableofcontents

\section{Введение}

\todo{Добавить введение.}

Альтернативное название статьи: учёт верхней связности на основе слабого замыкания множества алгоритмов.

\section{Основные обозначения}

Пусть задана генеральная выборка $\XX=\bigl( x_1, \ldots, x_L)$, состоящая из~$L$ объектов.
Произвольный алгоритм классификации, примененный к~данной выборке, порождает бинарный вектор
ошибок $a \equiv \bigl( I(a, x_i) \bigr){}_{i=1}^L$,
где $I(a, x_i) \in \{0, 1\}$ "--- индикатор ошибки алгоритма $a$ на~объекте $x_i$.
В~дальнейшем алгоритмы будут отождествляться с~векторами их~ошибок на~выборке $\XX$.

Обозначим
через $\AA = \{0, 1\}^L$ множество всех возможных векторов ошибок длины~$L$.
Через~$\XXell$ обозначим множество всех разбиений генеральной выборки~$\XX$
на обучающую выборку~$\Xl$ длины~$\ell$ и~контрольную выборку~$\Xk$ длины $k=L-\ell$.
\emph{Число ошибок} алгоритма~$a$ на~выборке $U \subseteq \XX$
обозначим через $n(a, U) = \sum \limits_{x \in U}I(a, x)$.
Величину $\nu(a, U) = n(a, U) / |U|$ будем называть \emph{частотой ошибок}
алгоритма~$a$~на~выборке $U$.
\emph{Уклонение частот} на~разбиении $\XX = \Xl \sqcup \Xk$ определим как разность
частот ошибок на~контроле и~на обучении:
$\delta(a, \Xl) = \nu(a, \Xk) - \nu(a, \Xl)$.

Пусть~$A \subset \AA$ "--- множество алгоритмов с~попарно различными векторами ошибок.
Обозначим через $A(\Xl)$ множество алгоритмов с~минимальным числом ошибок на~обучающей выборке~$\Xl$:
\begin{equation}
\label{eqERM-A(X).sym}
    A(X) = \Argmin_{a\in A} n(a,\Xl).
\end{equation}

Частоту ошибок на~обучающей выборке называют \emph{эмпирическим риском}.
\emph{Минимизация эмпирического риска $\mu$} "--- это метод обучения,
который из~заданного множества $A \subset \AA$ выбирает алгоритм $a \in A$,
допускающий наименьшее число ошибок на~обучающей выборке $\Xl$.
Таким образом, для всех $\Xl \in \XXell$ выполнено $\mu \Xl \in A(\Xl)$.
В дальнейшем будет рассматриваться \emph{пессимистическая} минимизация эмпирического риска,
удовлетворяющая дополнительному условию $\mu \Xl \in A(\XX)$ "--- то есть
среди алгоритмов в~$A(\Xl)$ выбирается алгоритм с~наибольшим числом ошибок на полной выборке.

Говорят, что метод $\mu$ переобучен на~разбиении $\Xl \sqcup \Xk$, если
уклонение частот $\delta(a, \Xl)$ превышает фиксированный порог $\epsilon$.
Переобучение может быть следствием <<неудачного>> разбиения
генеральной выборки на~обучение и~контроль.
Поэтому вводится функционал \emph{вероятности переобучения},
равный доле разбиений выборки, при которых возникает переобучение~\cite{voron09dan, voron09mmro}:
\[
    Q_\eps(A) = \Expect [\delta(\mu \Xl, \Xl) \geq \epsilon],
    \text{ где } \Expect = \frac 1{\CLl} \sum\limits_{\Xl \in \XXell}.
\]
Тут и~далее квадратные скобки "--- нотация Айверсона,
переводящая логическое выражение в~число $0$ или $1$ по~правилам
$[\text{истина}] = 1$, $[\text{ложь}] = 1$.

Функционал $Q_\eps(A)$ уже не~зависит от~выбора разбиения
и~характеризует качество данного метода обучения на~данной генеральной выборке.

\section{Теорема о порождающих и запрещающих объектах}

Первый подход, позволивший получать точные оценки вероятности переобучения
в~рамках слабой вероятностной аксиоматики, основан на выделении порождающих
и~запрещающих объектов \cite{voron10pria}.

\begin{Hypothesis}
\label{hyp1}
    Пусть множество~$A$, выборка~$\XX$ и~детерминированный метод обучения~$\mu$ таковы, что
    для каждого алгоритма $a\in A$
    можно указать
    конечное множество индексов $V_a$, и для каждого индекса $v \in V_a$ можно указать
    порождающее множество $X_{a v} \subset \XX$,
    запрещающее множество $X'_{a v} \subset \XX$ и
    коэффициент $c_{a v} \in \RR$, удовлетворяющие условиям
    \begin{equation}
    \label{eq1muX}
        \bigl[ \mu X{=}a \bigr]
        =
        \sum_{v \in V_a}
        c_{a v}
        \bigl[  X_{a v}\subseteq  X \bigr]
        \bigl[ X'_{a v}\subseteq \X \bigr],
        \quad
        \forall X\in \XXell.
    \end{equation}
\end{Hypothesis}

Введем для каждого алгоритма $a \in A$ и каждого индекса $v \in V_a$ обозначения:
\begin{align*}
    L_{a v} &= L - |X_{a v}| - |X'_{a v}|;
\\
    \ell_{a v} &= \ell - |X_{a v}|;
\\
    m_{a v} &= n(a, \XX) - n(a, X_{a v}) - n(a, X'_{a v});
\\
    s_{a v}(\eps) &= \frac \ell L \left( n(a, \XX) - \eps k \right) - n(a, X_{a v}).
\end{align*}

В условиях гипотезы~\ref{hyp1} справедливы следующие утверждения о вероятностях получения алгоритмов и вероятности переобучения:

\begin{Theorem}
\label{th1}
    Если гипотеза~\ref{hyp1} справедлива,
    то для всех $a \in~A$ вероятность получить в~результате обучения алгоритм~$a$ равна
    \begin{align*}
        P_a &= \P[ \mu X{=}a ] = \sum_{v \in V_a} c_{a v} P_{a v}; \\
        P_{a v} &= \P[ X_{a v} \subset \X ][X'_{a v} \subset \Xk] =
        \frac{C_{L_{a v}}^{\ell_{a v}}}{C_{L}^{\ell}},
    \end{align*}
    а вероятность переобучения~$Q_\eps(A)$ выражается по~формуле
    \begin{equation}
    \label{eq1pzo}
    Q_\eps(A)
    =
    \sum_{a\in A}
    \sum_{v \in V_a}
        c_{a v} P_{a v}
        \Hyper{L_{a v}}{m_{a v}}{\ell_{a v}}{s_{a v}(\eps)},
    \end{equation}
    где
$
    \Hyper{L}{m}{\ell}{z}
    =
    \sum\limits_{s=0}^{\lfloor z \rfloor} \frac{\Binom{m}{s} \Binom{L-m}{\ell-s}}{\CLl}
$ "--- гипергеометрическая функция распределения.

\end{Theorem}

Отметим, что в ряде случаев удается подобрать лишь такие множества порождающих и запрещающих
объектов, для которых \eqref{eq1muX} выполнено лишь в виде неравенства:
\begin{equation}
\label{leq1muX}
    \bigl[ \mu X{=}a \bigr]
    \leq
    \sum_{v \in V_a}
    c_{a v}
    \bigl[  X_{a v}\subseteq  X \bigr]
    \bigl[ X'_{a v}\subseteq \X \bigr],
    \quad
    \forall X\in \XXell.
\end{equation}
Очевидно, что в данном случае выражения \eqref{eq1pzo} будет давать верхнюю оценку для вероятности переобучения $Q_\eps(A)$. В следующем параграфе данный факт будет использован для вывода верхней оценки вероятности переобучения.

\section{Оценка расслоения-связности для связных семейств алгоритмов}

Для пары алгоритмов $a, b \in A$ обозначим через $\rho(a, b)$
хэммингово расстояние между векторами ошибок алгоритмов $a$ и~$b$.
Введем на $A$ естественное отношение порядка: $a \leq b$ тогда и только тогда, когда
$I(a, x) \leq I(b, x)$ для всех $x \in \XX$. Определим $a < b$ есть $a \leq b$ и $a \neq b$.
Если $a < b$ и при этом $\rho(a, b) = 1$, то будем говорить, что $a$ \emph{предшествует} $b$,
и записывать $a \prec b$.

\begin{Definition}
\emph{Графом расслоения-связности} множества алгоритмов $A$ будем называть направленный граф $(A, E)$
с~множеством ребер $E = \{ (a, b) \colon a \prec b \}$.
\end{Definition}

Каждому ребру $a \prec b$ графа расслоения-связности соответствует один и только один объект $x_{a b} \in \XX$, такой что $I(a, x_{a b}) = 0$ и $I(b, x_{a b}) = 1$.

%Введем понятия \emph{верхней связности $u(a)$}, \emph{нижней связности $d(a)$} и
% \emph{неполноценности $q(a)$} алгоритма $a \in A$.

\begin{Definition}[Верхняя связность]
Обозначим через $X_u(a) = \{ x_{a b} \in \XX \colon a \prec b \}$ множество объектов, соответствующих ребрам графа расслоения-связности, \emph{исходящим} из вершины $a$. \emph{Верхней связностью $u(a) = |X_u(a)|$} назовем мощность множества $X_u(a)$.
\end{Definition}

\begin{Definition}[Нижняя связность]
Обозначим через $X_d(a) = \{ x_{b a} \in \XX \colon b \prec a \}$ множество объектов, соответствующих ребрам графа расслоения-связности, \emph{входящим} в вершину $a$. \emph{Нижней связностью $d(a) = |X_d(a)|$} назовем мощность множества $X_d(a)$.
\end{Definition}

Связность $u(a)$ (или $d(a)$) есть число способов изменить алгоритм $a$ так, чтобы он стал делать на одну ошибку больше (или меньше). Связность можно интерпретировать как число степеней свободы семейства $A$ в локальной окрестности алгоритма $a \in A$.

\begin{Definition}[Неполноценность алгоритма]
Обозначим через $X_q(a) = \{x_{c b} \colon c \prec b < a\}$ множество объектов, соответствующих всевозможным ребрам $(c, b)$ на путях, ведущих к вершине $a$. \emph{Неполноценностью $q(a) = |X_q(a)|$} алгоритма $a \in A$ будем называть мощность множества $X_q(a)$.
\end{Definition}

Легко доказать, что если метод $\mu$ является пессимистической минимизацией эмпирического риска, то $X_a = X_u(a)$ и $\bar X_a = X_q(a)$ можно использовать в качестве порождающего и~запрещающего множества:
\begin{equation}
    \bigl[ \mu X{=}a \bigr]
    \leq
    \bigl[ X_u(a)\subseteq  X \bigr]
    \bigl[ X_q(a)\subseteq \X \bigr],
    \quad
    \forall X\in \XXell.
\end{equation}
Следовательно, имеет место следующая верхняя оценка:
\begin{Theorem}[оценка расслоения-связности]
Для произвольной выборки $\XX$, пессимистического метода минимизации эмпирического риска $\mu$ и
произвольного $\eps \in (0, 1)$
\begin{equation}
    \label{eqConSplit}
    Q_\eps(A) = \sum_{a \in A} \frac{\Binom{L-u-q}{\ell - u}}{\CLl}\Hyper{L-u-q}{\ell-u}{m - q}{\tfrac{\ell}{L}(m-\eps k)},
\end{equation}
где $u \equiv u(a)$ "--- верхняя связность, $q \equiv q(a)$ "--- неполноценность, $m = n(a, \XX)$ "--- число ошибок алгоритма $a$ на генеральном множестве объектов.
\end{Theorem}

Благодаря комбинаторному сомножителю $\frac{\Binom{L-u-q}{\ell - u}}{\CLl}$
вклад каждого алгоритма $a$ в оценку $Q_\eps$ экспоненциально убывает
с ростом неполноценности $q$ и связности $u$.

\section{Разреженная монотонная сеть}

В данном параграфе рассматривается \emph{разреженная монотонная сеть}
"--- модельное семейство алгоритмов,
демонстрирующее один из недостатков полученной выше оценки \eqref{eqConSplit}.

Мы начнём с плотной (не-разреженной) многомерной сети алгоритмов.
Данное семейство является моделью параметрического \emph{связного семейства алгоритмов},
предполагающего, что при непрерывном удалении каждой компоненты вектора параметров
от~оптимального значения число ошибок на~полной выборке только увеличивается.
Известно, что рассмотренная выше оценка расслоения-связности \eqref{eqConSplit}
дает точное значение вероятности переобучения для многомерной сети алгоритмов.
Тем не менее, для разреженной монотонной сети, полученной удалением некоторой части алгоритмов из плотной монотонной сети, оценка расслоения-связности вырождается.

Введём целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \ldots, h} |d_j|$,\:
$|\vec d| = |d_1| + \ldots + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из~неравенств строгое.

\begin{Definition}
    \label{eq:monotonicSet}
    Множество алгоритмов
    $A = \bigl\{a_{\vec d}\bigr\}$, где~$\vec d \geq 0$ и~$\|\vec d\|\leq D$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов длины $D$},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j = \{x_j^1, \ldots, x_j^D\} \subset \XX$, для всех $j = 1, \ldots, h$,
    а~так~же множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что:
    \begin{enumerate}
        \item[1)] набор $\bigl\{ U_0, U_1,\{X_j\}_{j=1}^h \bigr\}$ является разбиением множества~$\XX$
              на~непересекающиеся подмножества;
        \item[2)] $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item[3)] $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item[4)] $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Definition}

Обозначим $|U_1| = m$.
Из определения следует, что $n(a_{\vec d}, \XX) = m + |d|$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в~сетке}.
Множество алгоритмов с~равным числом ошибок $t+m \brop= n(a_{\vec d}, \XX)$ называются \emph{$t$-слоем} сетки.

\begin{Example}
    Монотонная двумерная сетка при $m = 0$ и~$L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}



\begin{Definition}
Пусть $\kappa \in \NN$ "--- целочисленный параметр;
${A = \{a_{\vec d}\}}$ "--- $h$"~мерная монотонная сетка длины $\kappa D$;
$m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной монотонной сеткой $\ddot{A}$ разреженности~$\kappa$ и~длины $D$}
будем называть подмножество $A$, заданное условием:
\[
\begin{aligned}
    \ddot{A} =
        %\left\{a_{\vec d} \in A \, \big| \, \forall j = 1, \ldots, h \; \exists d'_j \in \ZZ \colon d_j = \kappa d'_j \right\}.
        \bigl\{a_{\vec d} \in A \, \big| \, \vec d \in \left( \tiny \kappa \ZZ \tiny \right)^h \bigr\}.
\end{aligned}
\]
\end{Definition}

Отметим, что при $\kappa > 1$ граф смежности разреженной монотонной сетки состоит из~изолированных точек.
Следовательно, для всех алгоритмов семейства $u(a) = q(a) = 0$, и оценка расслоения-связности вырождается.

\todo{отразить картинку по вертикали и переделать её векторно}
\begin{Example}
На~рисунке \ref{fig:sparseMonotonicNetExample}
выделено подмножество двумерной монотонной сетки с~параметром $D = 8$,
соответствующее разреженной монотонной сетке с~параметрами $\kappa = 2$, $D = 4$.
\begin{figure}[h]
    \label{fig:sparseMonotonicNetExample}
    \begin{centering}
    \includegraphics[height=64mm]{monot_net_rarefield.eps}
    \caption{Двумерная разреженная монотонная сетка при $\kappa = 2$, $D = 4$.}
    \end{centering}
\end{figure}
\end{Example}

Тем не менее, для разреженной монотонной сети по-прежнему можно ввести систему
порождающих и запрещающих множеств. Для этого нам потребуется следующее естественное обобщение
графа расслоения-связности:
\begin{Definition}
\emph{Диаграммой Хассе} множества алгоритмов $A$ называется ориентированный граф транзитивной редукции отношения $<$ частичного порядка на алгоритмах.
\end{Definition}

Легко проверить, для разреженной монотонной сети $\ddot A$ ребра диаграммы Хасса $E = \{(a, b) \colon a < b \text { и } \rho(a, b) = \kappa\}$ соединяют те и только те пары алгоритмов, что находятся друг от друга на хэмминговом расстоянии $\kappa$, и~идут в~сторону увеличения числа ошибок алгоритмов на полной выборке.

В отличии от графа расслоения-связности, каждому ребру $(a, b)$ диаграммы Хасса соответствует не один, а множество алгоритмов $X_{a b} = \{x \in \XX \colon I(a, x) = 0 \text { и } I(b, x) = 1\}$. Это позволяет обобщить понятия неполноценности в терминах диаграммы Хасса.

\begin{Definition}[Неполноценность алгоритма]
Обозначим через $X_q(a) = \bigcup_{b \leq a}X_{b a}$ множество объектов, соответствующих всевозможным ребрам диаграммы Хассе на всевозможных путях, ведущих к вершине $a$. \emph{Неполноценностью $q(a) = |X_q(a)|$} алгоритма $a \in A$ будем называть мощность множества $X_q(a)$.
\end{Definition}

Обозначим через $I_a = \{b \in A \colon (a, b) \in E\}$ множество концов ребер, исходящих из $a$.
через $I^a = \{b \in A \colon (a, b) \in E\}$ "--- множество алгоритмов, из которых в $a$ ведет ребро. Число исходящих из $a$ ребер обозначим через $u(a) = |I_a|$.

\begin{Lemma}
\label{lem:randomizedlearn}
Пусть $A$ "--- произвольное множество алгоритмов, а
метод обучения $\mu$ является пессимистической минимизацией эмпирического риска.
Тогда необходимое условие получения произвольного алгоритма $a \in A$ в результате обучения
записывается в терминах графа Хасса следующим образом:
\begin{equation}
    \label{randomizedlearn}
    \bigl[ \mu X{=}a \bigr]
    \leq
    \bigl[ X_u(a)\subseteq  \X \bigr]
    \prod_{b \in I_a} [X_{a b} \cap \Xl \neq \emptyset],
    \quad
    \forall X\in \XXell.
\end{equation}
\end{Lemma}
\begin{vkProof}
TBD
\end{vkProof}

Это условие означает следующее: для того, что бы алгоритм $a$ был выбран методом обучения, необходимо и достаточно, что бы множество $X_u(a)$ целиком содержалось в контроле, а для каждого $b \in I_a$ хотя бы один объект из $X_{a b}$ попал в обучение. В частном случае разреженной монотонной сети \eqref{randomizedlearn} обращается в равенство.

Обратим внимание, что в случае разреженной монотонной сети множества $\{X_{a b} \colon b \in I_a\}$, соответствующие различным $b$, попарно не пересекаются. Это важное свойство позволяет записать условие \eqref{randomizedlearn} на языке порождающих и запрещающих объектов.

Зафиксируем алгоритм $a \in A$, и пронумеруем элементы множества $I_a$ произвольным способом: $I_a = \{b_1, \dots, b_{u(a)}\}$. Для каждого $b_i \in I_a$ пронумеруем элементы множества $X_{a b_i} = \{x_{i1}, \dots, x_{i\kappa}\}$ (тоже произвольным способом).
Возьмем $V_a = \prod\limits_{b \in I_a} X_{a b}$ в качестве индексного множества,
фигурирующего в~гипотезе о~порождающих и~запрещающих объектах.
Положим все $c_{a v} = 1$.
Элементы $v \in V_a$ будем записывать в виде вектора чисел:
$v = (v_1, \dots, v_{u(a)})$,
где все $v_i = 1, \dots, |X_{b_i}|$.
Определим систему порождающих и запрещающих множеств следующим образом:
\begin{align*}
    \bar X_{a v} &= \{x_{ij} \colon i = 1, \dots, u(a), j = 1, \dots, (v_i - 1)\} \cup X_q(a), \\
    X_{a v} &= \{x_{i j} \colon i = 1, \dots, u(a), j = v_i\}.
\end{align*}

\begin{Lemma}
\label{lem13}
Пусть метод $\mu$ является пессимистической минимизацией эмпирического риска,
а~множество алгоритмов $A$ таково, что для каждого $a \in A$
множества $\{X_{a b} \colon b \in I_a\}$, соответствующие различным $b$, попарно не пересекаются.
Тогда определенная выше система порождающих и запрещающих множеств даёт необходимое условие
получение алгоритмов $a \in A$ в результате обучения:
\begin{align*}
    \bigl[ \mu X{=}a \bigr]
    & \leq
    \sum_{v \in V_a}
    \bigl[  X_{a v}\subseteq  X \bigr]
    \bigl[ X'_{a v}\subseteq \X \bigr],
    \quad
    \forall X\in \XXell.
\end{align*}
\end{Lemma}
\begin{vkProof}
TBD
\end{vkProof}

Следует отметить, что в лемме \ref{lem13} условие о попарно-непересекающихся множествах $X_{a b}$ является абсолютно искусственным техническим приемом.
В следующем параграфе мы рассматриваем процедуру, достраивающую произвольное семейство $A$ до $A^* \supset A$ так, что к $A^*$ уже можно применять лемму \ref{lem13}.

\section{Слабое замыкание семейства алгоритмов}

Для алгоритма $a$ обозначим через $\XX_{[a]} \subset \XX$ множество ошибок алгоритма $a$ на выборке $\XX$.
Рассмотрим произвольное множество алгоритмов $A$ и его диаграмму Хассе $(A, E)$ "--- граф транзитивной редукции отношения $<$,
определенного на парах алгоритмов условием <<$a\leq b$ если $\XX_{[a]} \subset \XX_{[b]}$>>.
Напомним также, что через $I_a = \{b \in A \colon (a, b) \in E\}$ обозначалось множество концов ребер, исходящих из $a$, через $u(a) = |I_a|$ "--- количество таких ребер, через
$X_{a b} = \XX_{[b]} \backslash \XX_{[a]}$, где $a < b$ "--- множество объектов, соответствующих ребру $(a, b)$.
По аналогии с $I_a$ и $I^a$ обозначим через $I_a^\infty$ множество алгоритмов $b$, таких что существует путь из $a$ в $b$, проходящий по рёбрам графа Хассе; через $I^a_\infty$ "--- множество алгоритмов $b$, таких что существует путь из $b$ в $a$. Для любой пары алгоритмов $a, b \in A$ определим алгоритм $a \cap b$ условием $\XX_{[a \cap b]} = \XX_{[a]} \cap \XX_{[b]}$.

Будем говорить, что множество алгоритмов $A$ является \emph{слабо замкнутым}, если для любого $a \in A$ множества $\{X_{a b} \colon b \in I_a\}$ попарно не пересекаются. Наша задача "--- дополнить произвольное множество $A$ алгоритмами до $A^{*}$, такого что $A \subset A^*$, и $A^*$ "--- слабо замкнуто. Очевидно, что такое множество существует "--- достаточно взять $A^* = \{0, 1\}^L$. Следует отметить, что в данном случае неверно говорить о наименьшем по включению множестве. Действительно, легко построить пример двух слабо замкнутых множеств $A_1, A_2$, пересечение которых $A_1 \cap A_2$ уже не является слабо замкнутым. Именно этим неприятным свойством и объясняется термин \emph{слабая} замкнутость.

Множество алгоритмов $A$ назовём \emph{замкнутым}, если для любого $a \in A$, и для любой пары $b_1, b_2 \in A$, такой что $a < b_1, a < b_2$, выполнено $(b_1 \cap b_2) \in A$. Легко показать, для любой пары замкнутых множеств их пересечение вновь является замкнутым. Это позволяет определить замыкание множества алгоритмов $\bar A$, как наименьшее по включению замкнутое множество, содержащее $A$.

Утверждается, что из замкнутости следует слабая замкнутость. В дальнейшем для произвольного множества $A$ мы будем определять его слабое замыкание $A^* \subset \bar A$ с помощью описанной ниже алгоритмической процедуры.

\begin{algorithm}
\label{weakClosureAlg}
\caption{Слабое замыкание множества алгоритмов}
\begin{algorithmic}[1]
\REQUIRE множество алгоритмов $A$, множество ребер графа Хассе $E$;
\ENSURE слабое замыкание $A^*$, новое множество ребер графа Хассе $E^*$;

\STATE Сгенерировать очередь заданий на обработку:

$Q := \{(a, b_1, b_2) \colon (a, b_1) \in E, (a,b_2) \in E\}$;
\WHILE{$Q \neq \emptyset$} \label{whileQ}
    \STATE $(a, b_1, b_2) := $ взять следующее задание из очереди $Q$;
    \LINEIF{$(a, b_1) \not\in E$ или $(a, b_2) \not\in E$}{\GOTO{whileQ}}
    \STATE $c := b_1 \cap b_2$;
    \LINEIF{$c \in A$}{\GOTO{whileQ}}
    \STATE $A := A \cup \{c\}$;
    \STATE Найти множества $I^{a}_\infty, I^{b_1}_\infty, I^{b_2}_\infty, I_{a}^\infty, I_{b_1}^\infty, I_{b_2}^\infty$ для текущей пары $(A, E)$.
    \STATE $I^c$ := Т-Редукция $((I^{b_1}_\infty \cap I^{b_2}_\infty) \backslash I^{a}_\infty, <, \{a\})$;
    \STATE $I_c$ := Т-Редукция $(I_{a}^\infty \backslash (I_{b_1}^\infty \cup I_{b_2}^\infty), >, \{b_1, b_2\})$;
    \STATE $E := E \cup \{(x, c) \colon x \in I^c)\} \cup \{(c, y) \colon y \in I_c)\}$;
    \FORALL{$x \in I^c$, $y \in I_c$}
        \IF{$(x, y) \in E$}
            \STATE $E := E \backslash (x, y)$;
        \ENDIF
    \ENDFOR
    \FORALL{$\{x, y\} \subset I_c$}
        \STATE $Q = Q \cup \{(c, x, y)\}$;
    \ENDFOR
    \FORALL{$x \in I^c$}
        \FORALL{$y \in I_x$}
            \IF{$y \neq c$}
                \STATE $Q = Q \cup \{(x, c, y)\}$;
            \ENDIF
        \ENDFOR
    \ENDFOR
\ENDWHILE
\STATE Положить $A^* := A$, $E^* := E$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Т-Редукция($Q, \phi, R$)}
\begin{algorithmic}[1]
\REQUIRE очередь кандидатов $Q$, предикат $\phi$ на парах из $Q$, начальное приближение $R$;
\ENSURE множество $\bar R \subset R \cup Q$, транзитивно-замкнутое относительно $\phi$;
\WHILE{$Q \neq \emptyset$} \label{whileQ2}
    \STATE $x := $ взять следующее задание из очереди $Q$;
    \STATE $D := \emptyset$;
    \FORALL{$y \in R$}
        \LINEIF{$\phi(y, x)$}{\GOTO{whileQ2}}
        \LINEIF{$\phi(x, y)$}{$D := D \cup \{y\}$;}
    \ENDFOR
    \STATE $R := (R \backslash D) \cup \{x\}$;
\ENDWHILE
\RETURN $R$
\end{algorithmic}
\end{algorithm}

Следующая лемма показывает, что вероятность переобучения для
слабого замыкания семейства $Q_\eps(A^*)$ всегда не меньше
$Q_\eps(A)$ и поэтому любую верхнюю оценку для $Q_\eps(A^*)$ можно
использовать как верхнюю оценку для $Q_\eps(A)$.
\begin{Lemma}
\label{lem:boundrelation} Пусть $A$ --- произвольное множество
алгоритмов, $b$ - некоторый алгоритма, не принадлежащий $A$, но
такой, что $\exists a \in A: a \leq b$, а метод обучения $\mu$
является пессимистической минимизацией эмпирического риска. Тогда
$Q_\eps(A \cup b) \geq Q_\eps(A)$
\end{Lemma}
\begin{vkProof}
    Рассмотрим множество разбиений $T(A)$ на которых ПМЭР переобучается.
    При добавлении в   cемейство нового алгоритма $b$ на части из этих разбиений он может быть выбран методом
    обучения. На остальных разбиениях из $T(A)$ будут выбраны те же алгоритмы, и эти разбиения останутся в $T(A \cup b)$.

    Рассмотрим произвольное разбиение $(X, \X) \in T(A)$, такое что
    при минимизации эмпирического риска для множества $A$ был выбран
    алгоритм $c = \mu A$, и он оказался переобучен, а при минимизации
    эмпирического риска для множества $A \cup b$ был выбран алгоритм $b$.

    Для доказательства леммы нам достаточно показать, что на данном разбиении $(X, \X)$ $b$ переобучается.
    Для числа ошибок алгоритмов $a$, $b$, $c$ на обучающей выборке $X$ верны следующие соотношения:

    $n(c, X) \leq n(a, X)$  (иначе из семейства $A$ был бы выбран $a$).

    $n(a, X) \leq n(b, X)$ ($a \leq b$).

    Следовательно $n(c, X) = n(b, X)$ (иначе из$A \cup b$ был бы выбран $c$).

    Так как мы рассматриваем пессимистическую минимизацию эмпирического риска, то
$n(b) >= n(c)$, если $b$ был выбран вместо $c$.

    Из неравенств $n(c, X) = n(b, X)$,   $n(b) \geq n(c)$ следует что $b$ переобучается, если
переобучается $c$.

    Лемма доказана.
\end{vkProof}

В алгоритме построения слабого замыкания (\ref{weakClosureAlg}) на
каждом шаге в семейство добавляется алгоритм $b$ c непустым
$X_q(b)$. Поэтому, согласно доказанной выше лемме,при построении
слабого замыкания семейства, на каждом шаге вероятность
переобучения только увеличивается, следовательно $Q_\eps(A^*) \geq Q_\eps(A)$.
%В качестве практической рекомендации можно посоветовать строить слабое замыкание не всего множества алгоритмов, а лишь некоторого количества нижних слоёв.

\section{Общая оценка расслоения-связности для разреженных семейств алгоритмов}

Пусть множество алгоритмов $A$ является слабо замкнутым.

Рассмотрим алгоритм $a \in A$ и систему множеств $\{X_{ab} \colon b \in I_a \}$, состоящую из наборов объектов, соответствующих выходящих из $a$ ребрам диаграммы Хасса. Пронумеруем элементы множества $I_a$ произвольным способом: $I_a = \{b_1, \dots, b_{u(a)}\}$, и рассмотрим вектор $w_a = (|X_{a b_i}|)_{i = 1}^{u(a)}$. Напомним, что под модулем вектора $|w_a|$ мы понимаем сумму его координат. Рассмотрим также вектор $1_a = (1, \dots, 1)$, той же размерности что и $w_a$, но заполненный единицам.
Рассмотрим функцию $S_a(u)$, определенную для всех $u$ из $|1_a|, \dots, |w_a|$ выражением $S_a(u) = |\{w \in \ZZ^{u(a)} \colon |w| = u, 1_a \leq w \leq w_a\}|$. Значение $S_a(u)$ соответствует количеству векторов с целочисленными координатами, ограниченных снизу вектором $1_a$, сверху - $w_a$, и с суммой координат $u$.

\begin{Theorem}
\label{theorem5eq}
Пусть $A$ "--- произвольное множество алгоритмов, $A^*$ "--- его слабое замыкание.
Тогда справедлива следующая оценка вероятности переобучения $Q_\eps(A^*)$ и среднего значения числа ошибок на контроле $C(A^*)$:
\begin{align*}
    Q_\eps(A^*) &\leq
        \sum_{a \in A}
        \sum_{u = |1_a|}^{|w_a|}
            S_a(u)
            \frac{\Binom{L_a - u}{\ell_a}}{\CLl}
            \Hyper{L_a - u}{m_a}{\ell_a}{s_a(\eps)}, \\
    C(A^*) &\leq
        \sum_{a \in A}
        \sum_{u = |1_a|}^{|w_a|}
            S_a(u)
            \frac{\Binom{L_a - u}{\ell_a}}{\CLl}
            \left(n(a, \XX) - \frac{\ell_a}{L_a - u}m_a\right), \\
\end{align*}
где введены следующие обозначения:
$L_a = L - q(a)$,
$\ell_a = \ell - u(a)$,
$m_a = n(a, \XX) - q(a)$,
$s_a(\eps) = \frac{\ell}{L} \left(n(a, \XX) - \eps k\right)$,
$q(a)$ "--- неполноценность алгоритма $a$, определенная в терминах диаграммы Хассе.
\end{Theorem}
\begin{vkProof}
Напомним, что согласно лемме \ref{lem13} множества порождающих и запрещающих объектов можно выбрать следующим способом:
\begin{align*}
    \bar X_{a v} &= \{x_{ij} \colon i = 1, \dots, u(a), j = 1, \dots, (v_i - 1)\} \cup X_q(a), \\
    X_{a v} &= \{x_{i j} \colon i = 1, \dots, u(a), j = v_i\}.
\end{align*}
Следовательно, мощности множеств $\bar X_{a v}$ и $X_{a v}$ выражаются следующим способом:
$
    |\bar X_{a v}| = |v| - u(a) + q(a),
    |X_{a v}| = u(a).
$
Тогда, в обозначениях теоремы \ref{th1}, получим:
\begin{align*}
    L_{a v} &= L - |v| - q(a) = L_a - |v|, \text{ где } L_a \equiv L - q(a), \\
    \ell_{a v} &= \ell - u(a) \equiv \ell_a, \\
    m_{a v} &= n(a, \XX) - q(a) \equiv m_a, \\
    s_{a v} &= \frac{\ell}L (n(a, \XX) - \eps k) - 0 \equiv s_a,
\end{align*}
где выражения $L_a, \ell_a, m_a, s_a$ не зависят от $v$.
Это позволяет записать сумму вида $\sum\limits_{v \in V_a}f(|v|)$ по декартовому произведению $V_a = \prod\limits_{b \in I_a} X_{a b}$ следующим образом:
\[
    \sum\limits_{v \in V_a}f(|v|) = \sum\limits_{u = |u(a)|}^{|w_a|}s_a(u)f(u),
\]
где $s_a(u)$ "--- определенный выше коэффициент.
\end{vkProof}

\section{Оценка для случая пересекающихся рёбер}
В данном параграфе мы рассмотрим альтернативный подход к учёту
рёбер диаграммы Хассе в оценке расслоения-связности  - прямое
вычисление оценки для случая пересекающихся рёбер.
Обозначим через $X_{u(a)} = \bigcup_{b \in I_a} X_{ab}$ - множество объектов, принадлежащих исходящим из $a$ рёбрам.

Перепишем условие (\ref{randomizedlearn}) в виде, необходимом для применения
теоремы \ref{th1}. Пусть $R(a) = \{r | r \subset X_u(a), \forall b \in I_a, X_{ab} \cap r \neq \emptyset \}$
\begin{equation}
    \label{randomizedlearn}
    \bigl[ \mu X{=}a \bigr]
    \leq \sum_{r \in R(a)}
    \bigl[ X_q(a) \cup (R(a)\backslash r) \subseteq  \X \bigr]
    [r \subseteq X]
\end{equation}
Тогда в обозначениях теоремы \ref{th1}
\begin{align*}
    L_{a r} &= L - |X_u(a)| - q(a);
\\
    \ell_{a r} &= \ell - |r|;
\\
    m_{a r} &= n(a, \XX) - q(a);
\\
    s_{a r}(\eps) &= s_a
\\
    P_{a r} &= \frac{C_{L_{a r}}^{\ell_{a r}}}{C_L^{\ell}}
\end{align*}
Запишем выражение для вклада одного алгоритма в оценку переобучения:
\begin{equation}
\sum_{r \in R(a)}
       P_{a r}
        \Hyper{L_{a r}}{m_{a r}}{\ell_{a r}}{s_{a r}(\eps)}
\end{equation}
Заметим, что параметры   $L_{a r}, \ell_{a r}, m_{a r}, s_{a r}, P_{a r}$
зависят только от мощности множества r, поэтому выражение можно переписать,
сгруппировав слагаемые с одинаковой мощностью $|r|$:
\begin{equation}
\sum_{v = 0}^{|X_u(a)|}
        T(v)
        \frac{C_{L - |X_u(a)| - q(a)}^{\ell - v}}{C_L^{\ell}}
        \Hyper{L - |X_u(a)| - q(a)}{n(a, \XX) - q(a)}{\ell - v}{s_a(\eps)}
\end{equation}
,где $T(v) = \#\{r | r \in R(a), |r| = v\}$.

Задача, вычисления $T(v)$, вообще говоря, NP-трудна(она является обобщением задачи о покрытии множества), но
эксперименты показывают, что число  исходящих рёбер неединичной мощности у алгоритма обычно невелико, поэтому для
вычисления $T(v)$ можно применять практически любой алгоритм.
Мы предлагаем следующий алгоритм:

Пусть $x_1, \dots , x_{|X_u(a)|}$ - пронумерованные произвольным образом объекты из $X_u(a)$.
Пусть $T(v, n, \alpha)$ - число подмножеств мощности $v$ из объектов $x_1, \dots, x_n$, покрывающих множество рёбер
$\alpha \subset I(a)$. Тогда для $T(v, n, \alpha)$ справедлива рекуррентная формула:
\begin{equation}
    T(v, n, \alpha \cup \beta_n) = T(v, n - 1, \alpha \cup \beta_n) + T(v - 1, n - 1, \alpha),
\end{equation}
где $\beta_n = \{b | b \in I_a, x_n \in X_{a b} \}$.

Используя эту рекуррентную формулу можно рассчитать $T(v) \equiv T(v, |X_u(a)|, I(a))$ для всех значений $v$

\section{Численный эксперимент}
В данном параграфе рассматривается вопрос практической применимости полученных выше оценок вероятности переобучения.
Оценка теоремы \eqref{theorem5eq} применима лишь к слабо замкнутому множеству алгоритмов,
поэтому в~первую очередь требуется сравнить вероятности переобучения исходного множества алгоритмов $A$ и его слабого замыкания $A^*$, построенного алгоритмом \ref{weakClosureAlg}.

Отметим, что слабая замкнутость множества алгоритмов необходима лишь для учёта верхней связности.
Положив $u(a) = 0$ и $w_a = 0$ в оценке теоремы \ref{theorem5eq} мы получим новую оценку, которая учитывает лишь неполноценность $q(a)$ каждого алгоритма.
Такая оценка будет менее точной, но в то же время она применима к произвольному множеству алгоритмов.
Следовательно, необходимо сравнить два эффекта: увеличение вероятности переобучения при слабом замыкании множества $A$, и улучшение оценки при учете верхней связности.
Так же представляет интерес сравнение оценки \ref{theorem5eq} и простой оценки, полученной с помощью неравенства Буля:
\begin{align*}
    Q_\eps(A) &\leq
        \sum_{a \in A}
            \Hyper{L}{m}{\ell}{\tfrac{\ell}{L} \left(m - \eps k\right)},
\end{align*}
где $m = n(a, \XX)$ "--- число ошибок алгоритма $a$ на полной выборке.

Для проведения эксперимента было выбрано $8$ задач из репозитория UCI.
К каждой из восьми задач применялось четыре метода поиска логических закономерностей, реализованных в библиотеке Forecsys-LogicPro:
случайный поиск ($RND$),
случайный поиск с адаптацией весов признаков ($RSA$),
генетический алгоритм ($sGA$)
и поиск правил путём отбора признаков алгоритмом $TEMP$.
Каждый метод поиска в процессе своей работы генерирует большое количество логических правил и вычисляет для них информативность.
Следует подчеркнуть, что взаимодействие метода поиска с обучающей выборкой происходит только в момент вычисления информативности.
Поэтому представляется разумным исследовать вероятность переобучения лишь для \emph{наблюдаемой части семейства } "--- то есть для множества логических закономерностей, сгенерированных методом поиска в процессе своей работы. Все прочие логические закономерности предлагается объявить ненаблюдаемыми, и исключить из рассмотрения.

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=72mm,height=48mm]{ErrorProfile_Liverdis.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=72mm,height=48mm]{ErrorProfile_Echocard.eps}
    \hfill
    \end {multicols}
    \caption{Распределение алгоритмов по числу ошибок,
    задача Liver Disorders (слева) и Echo Cardiogram (справа).}
    \label{fig2}
\end{figure}
\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=72mm,height=48mm]{Comparison_LiverDisordersRSA.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=72mm,height=48mm]{Comparison_EchocardRSA.eps}
    \hfill
    \end {multicols}
    \caption{Сравнение 0.5-квантили различных оценок вероятности переобучения,
    задача Liver Disorders (слева) и Echo Cardiogram (справа). Пунктирная кривая
    на левом рисунке соответствует семейству, полученному случайным перемешиванием
    векторов ошибок в $A^*_r$.}
    \label{fig3}
\end{figure}

Ошибкой логических закономерностей считалось как покрытие объекта противоположного класса, так и непокрытие объекта своего класса.
На~рис.\,\ref{fig2} приведены профили расслоения полученных семейств.
Очевидно, что при минимизации числа ошибок на обучении вероятность выбрать алгоритм в результате обучения быстро падает с ростом числа его ошибок.
Поэтому в данном эксперименте рассматривались подмножества $A_r = \{a \in A \colon n(a, \XX) \leq m_0 {+} r\}$, где $m_0  = \min\limits_{a \in A} n(a, \XX)$.
Значения $r$ перебирались от $0$ до $50$.
Для каждого $A_r$ строилось внутреннее замыкание $A_r^*$ и вычислялась $0.5$-квантиль распределения вероятности переобучения,
построенного методом Монте-Карло (по $10000$ случайным разбиениям выборки на обучения и контроль).
Также вычислялась $0.5$-квантиль распределения, полученного по формулам расслоения связности --- с учетом и без учета верхней связности.

Результаты сравнения приведены на~рис.\,\ref{fig3}. Во-первых, видно что внутреннее замыкание множества алгоритмов может вести себя принципиально по-разному.
Так, для задачи Liver Disorder вероятности переобучения $A_r$ и $A^*_r$ практически совпадают при всех значениях $r$.
Для задачи Echo Cardiogram вероятность переобучения $A^*_r$ оказывается заметно выше.
Оценки вероятности переобучения также ведут себя по-разному.
Для задачи Liver Disorder учёт связности не даёт улучшения по сравнению с оценкой Буля.
Однако на задаче Echo Cardiogram улучшение становится очевидным.

Завышенность оценки \ref{theorem5eq} в задаче Liver Disorders можно объяснить плохим учетом связности (при малых $r$) и расслоения (при больших $r$).
На левом рисунке пунктиром изображена кривая, соответствующая семейству с $A^*_r$ со случайно переставленными ошибками.
Данная процедура разрушает связность между алгоритмами. Видно, что при малых значениях $r$ данная кривая хорошо приближает оценки теоремы \ref{theorem5eq}.
Плохой учет связности возникает из-за того, что в оценке рассматриваются лишь пары алгоритмов с вложенными векторами ошибок.
Это, в частности, не позволяет учитывать связи между алгоритмами с равным числом ошибок.
В то же время, теоретически доказано что эффект связности имеет место и в этом случае.
В работах Ильи Толстихина и Александра Фрея была показана принципиальная разница между поведением вероятности переобучения для двух модельных семейств:
максимально-компактного (центральное сечение шара), и максимально-разреженного (случайные подмножества слоя).

Вместе с тем, при больших значениях $r$ оценка для перемешенного $A^*_r$ стремится к горизонтальной асимптоте, в то время как оценка расслоения-связности продолжает расти.
Это можно объяснить недостаточным учетом эффекта расслоения. В работах Евгения Соколова показано, что для произвольной пары алгоритмов $a, b$ с различным числом ошибок алгоритм $a$ с меньшим числом
всегда уменьшает вероятность алгоритма $b$. Данный эффект наблюдается даже в тех случаях, когда вектор ошибок $a$ не вложен в вектор ошибок $b$.

\todo{Причесать текст и добавить замечание о высокой толерантности обращения оценки к ошибке по сравнению с непосредственной оценкой среднего числа ошибок на контроле.}

%$P(\delta(x, A_n) > \eps) \leq n \Hyper{L}{m}{\ell}{s(\eps)} \simeq n e^{- \lambda \eps}.$
%Приравнивая правую часть к единице, получим $\eps \simeq \frac{\ln n}{\lambda}$.

\section{Заключение}
\todo{Добавить заключение. Краткие выводы:}
\begin{itemize}
  \item Предложен способ учёта верхней связности, основанный на слабом замыкании множества алгоритмов;
  \item Показано, что учёт верхней связности может оказывать существенное влияние на качество оценки;
  \item Показано, что в ряде случаев оценки всё еще остаются завышенными. Проанализированы возможные причины завышенности и предложены дальнейшей способы борьбы с ними.
\end{itemize}

\def\BibAuthor#1{\emph{#1}}
\def\BibTitle#1{#1}
\def\BibUrl#1{{\small\url{#1}}}
\def\BibHttp#1{{\small\url{http://#1}}}
\def\BibFtp#1{{\small\url{ftp://#1}}}
\def\typeBibItem{\small\sloppy}

%\begin{thebibliography}{1}

%\end{thebibliography}


\end{document}
