\documentclass{ccaspreprint}
\usepackage{multicol}
\usepackage{color}
\RequirePackage[all,poly]{xy}

\newtheorem{hypothesis}{Гипотеза}

% ============================================================================
% Шаманские пляски
% ============================================================================

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\X}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}

\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\sign}{\mathop{\rm sign}\limits}
\renewcommand{\epsilon}{\varepsilon}\newcommand{\eps}{\varepsilon}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\hypergeom}[5]{{#1}_{#2}^{#4,#3}\left(#5\right)}
\newcommand{\Bhypergeom}[5]{{#1}_{#2}^{#4,#3}\bigl(#5\bigr)}
\newcommand{\hyper}[4]{\hypergeom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\hypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\BHyper}[4]{\Bhypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\hypergeom{\bar{H}}{#1}{#2}{#3}{#4}}
\newcommand{\Binom}[2]{C_{#1}^{#2}}
%\newcommand{\Binom}[2]{\binom{#1}{#2}}
\newcommand{\CLl}{\Binom{L}{\ell}}
\newcommand{\ToDo}[1]{\textbf{\textcolor{red}{[ToDo] #1}}}

\newcommand{\vkEndProof}{\hfill$\scriptstyle\blacksquare$\par\medskip}
\newenvironment{vkProof}[1][. ]%
    {\par\noindent{\bf Доказательство#1}}%
    {\vkEndProof}

\newcommand{\IncludeHalfPicture}[1]{\includegraphics[width=54mm,height=38mm]{#1}}

\def\brop#1{#1\discretionary{}{\hbox{$#1$}}{}}

\def\RR{\mathbb{R}}
\def\DD{\mathbb{D}}
\def\fF{\mathfrak{F}}
\def\fI{\mathfrak{I}}
\def\fM{\mathfrak{M}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\def\afterlabel#1{\renewcommand\labelenumi{\theenumi #1}}


\newcounter{mmroReviewerNote}
\setcounter{mmroReviewerNote}{0}%
\newcommand{\REVIEWNOTE}[1]
{
    \refstepcounter{mmroReviewerNote}%
    \textbf{ REVIEWNOTE \themmroReviewerNote: } #1
}

\begin{document}
% ============================================================================
% Оформление титульной страницы и~аннотации
% ============================================================================
% Заглавие и~автор
\title{Комбинаторные оценки вероятности переобучения: теоретико-групповой подход}
\author{Воронцов~К.\,В., Фрей А.\,И., Толстихин~И.\,О.}

% Номер УДК
\udk{004.852}

% Ответственный редактор
\editor{чл.-корр. РАН К.\,В.~Рудаков}

% Рецензенты: внешний и~внутренний
\reviewer[1]{В.\,В.~Стрижов}
\reviewer[2]{А.\,Г.~Дьяконов}

% Год публикации
\year{2011}

% Ключевые слова через запятую с~маленькой буквы. Точка в~конце не~нужна.
\keywords{%
    теория статистического обучения,
    обобщающая способность,
    вероятность переобучения,
    теория групп
}

% Аннотация
\anno{
    В~рамках комбинаторной теории переобучения
    развивается теоретико"~групповой подход,
    позволяющий получать точные оценки вероятности переобучения
    для симметричных и~рандомизированных семейств алгоритмов.
}

\maketitle

\clearpage
\tableofcontents

% ============================================================================
% Основной текст препринта
% ============================================================================
\section{Проблема переобучения при~восстановлении зависимостей по~эмпирическим данным}

При решении задач
распознавания образов, восстановления регрессии, прогнозирования
всегда возникает проблема выбора по~неполной информации.
Имея лишь конечную обучающую выборку объектов,
требуется из~заданного множества алгоритмов выбрать алгоритм, который
ошибался~бы как можно реже
не~только на~объектах наблюдаемой обучающей выборки,
но~и~на~объектах скрытой контрольной выборки,
которая в~момент выбора алгоритма ещё неизвестна.
Если частота ошибок на~контрольной выборке
оказывается значительно выше, чем на~обучающей,
то~говорят, что произошло
<<переобучение>> (overtraining) или
<<переподгонка>> (overfitting) алгоритма "---
он~слишком хорошо описывает конкретные данные,
но~не~обладает способностью к~обобщению этих данных,
не~восстанавливает порождающую их~зависимость
и~не~пригоден для построения прогнозов.

На~практике обучающая выборка формируется раньше, чем контрольная.
Таким образом, обучающая и~контрольная выборки могут иметь различные статистические свойства.
Низкое качество на~контроле может быть обусловлено не~только
переобучением алгоритма, но~и~нестационарностью данных во~времени.

Процесс формирования обучающей выборки также является важным фактором, влияющим на~переобучение.
В~частности, большое значение имеет представительность (репрезентативность) обучающей выборки.
Данная проблема хорошо известна при проведении социалогических опросов.
Как ограничить круг респондетнов, чтобы тем не~менее представить весь спектр общественного мнения?
Аналогичный вопрос возникает и~при формировании обучающей выборки.

В~дальнейшем выборка данных будет предполагаться репрезентативной и~стационарной.
Главной целью ставится исследование свойств метода обучения как такового.

На~практике переобучение оценивается количественно с~помощью процедуры скользящего контроля (кросс"=валидации).
Фиксируется некоторое множество разбиений исходной выборки на~две подвыборки~--- обучающую и~контрольную.
Для каждого разбиения выполняется настройка алгоритма по~обучающей подвыборке,
затем оценивается его средняя ошибка на~объектах контрольной подвыборки.
\emph{Оценкой скользящего контроля} называется средняя по~всем разбиениям величина ошибки на~контрольных подвыборках.
Аналогичным образом определяется и~\emph{оценка вероятности переобучения}~---
это доля разбиений, при которых
средняя ошибка на~контрольной подвыборке превышает
среднюю ошибку на~обучающей подвыборке более чем на заданную величину~$\eps$.
Главный недостаток данного подхода~--- большая вычислительная сложность,
связанная с~многократной настройкой алгоритма классификации.
Точность оценки скользящего контроля зависит от~стабильности метода обучения
и~от~числа разбиений, по~которым производилось усреднение.
В~комбинаторной теории переобучения рассматривается множество всех возможных разбиений
и~ставится задача получения вычислительно эффективных формул
для оценок скользящего контроля и~вероятности переобучения~\cite{voron09dan,voron10pria}.

Теоретико-групповой подход~\cite{frey09mmro,frey10ioi,frey10pria}
позволяет получать такие формулы для случаев, когда
семейство алгоритмов обладает некоторой симметрией,
а~методом обучения является \emph{рандомизированная минимизация эмпирического риска}.
Рандомизация означает, что если в~семействе существует несколько алгоритмов,
допускающих одинаковое минимальное число ошибок на~обучающей выборке,
то~из них равновероятно выбирается любой.

\subsection{Задача обучения по~прецедентам}

Пусть задана генеральная выборка $\XX=\bigl( x_1, \ldots, x_L)$, состоящая из~$L$ объектов.
Произвольный алгоритм классификации, примененный к~данной выборке, порождает бинарный вектор
ошибок $a \equiv \bigl( I(a, x_i) \bigr){}_{i=1}^L$,
где $I(a, x_i) \in \{0, 1\}$ "--- индикатор ошибки алгоритма $a$ на~объекте $x_i$.
В~дальнейшем алгоритмы будут отождествляться с~векторами их~ошибок на~выборке $\XX$.

Обозначим
через $\AA = \{0, 1\}^L$ множество всех возможных векторов ошибок длины~$L$.
Через~$\XXell$ обозначим множество всех разбиений генеральной выборки~$\XX$
на обучающую выборку~$\Xl$ длины~$\ell$ и~контрольную выборку~$\Xk$ длины $k=L-\ell$.
\emph{Число ошибок} алгоритма~$a$ на~выборке $U \subseteq \XX$
обозначим через $n(a, U) = \sum \limits_{x \in U}I(a, x)$.
Величину $\nu(a, U) = n(a, U) / |U|$ будем называть \emph{частотой ошибок}
алгоритма~$a$~на~выборке $U$.
\emph{Уклонение частот} на~разбиении $\XX = \Xl \sqcup \Xk$ определим как разность
частот ошибок на~контроле и~на обучении:
$\delta(a, \Xl) = \nu(a, \Xk) - \nu(a, \Xl)$.

Пусть~$A \subset \AA$ "--- множество алгоритмов с~попарно различными векторами ошибок.
Обозначим через $A(\Xl)$ множество алгоритмов с~минимальным числом ошибок на~обучающей выборке~$\Xl$:
\begin{equation}
\label{eqERM-A(X).sym}
    A(X) = \argmin_{a\in A} n(a,\Xl).
\end{equation}

Частоту ошибок на~обучающей выборке называют \emph{эмпирическим риском}.
\emph{Минимизация эмпирического риска $\mu$} "--- это метод обучения,
который из~заданного множества $A \subset \AA$ выбирает алгоритм $a \in A$,
допускающий наименьшее число ошибок на~обучающей выборке $\Xl$.
Таким образом, для всех $\Xl \in \XXell$ выполнено $\mu \Xl \in A(\Xl)$.

В~следующей таблице показан пример, когда минимизация эмпирического риска приводит к~переобучению.
Столбцы таблицы соответствуют алгоритмам,
строки "--- объектам обучающей выборки $\{x_1,\ldots,x_\ell\}$
и~контрольной выборки $\{x_{\ell+1},\ldots,x_L\}$.
%Единица в~$[i,d]$-й ячейке таблицы означает, что
%алгоритм~$a_d$ допускает ошибку на~объекте~$x_i$.
\[
    \bordermatrix{
          & a_1 & a_2 & \cdots & a_d & \cdots & a_D \cr
    x_1 & 0 & 1 & \cdots & {0} & \cdots & 1 \cr
    \cdots & 1 & 1 & \cdots & {0} & \cdots & 0 \cr
    x_\ell & 0 & 0 & \cdots & {0} & \cdots & 0 \vspace{-2ex}\cr\cline{2-7}
    x_{\ell + 1} & 1 & 1 & \cdots & {1} & \cdots & 1 \cr
    \cdots & 1 & 0 & \cdots & {1} & \cdots & 0 \cr
    x_L & 0 & 0 & \cdots & {1} & \cdots & 0
    }
\]
В~данном примере переобучение могло быть следствием <<неудачного>> разбиения
генеральной выборки на~обучение и~контроль.
Поэтому вводится функционал \emph{вероятности переобучения},
равный доле разбиений выборки, при которых возникает переобучение~\cite{voron09dan, voron09mmro}:
\[
    Q_{\mu,\eps}(A) = \Expect [\delta(\mu \Xl, \Xl) \geq \epsilon],
    \text{ где } \Expect = \frac 1{\CLl} \sum\limits_{\Xl \in \XXell}\!\!.
\]
Тут и~далее квадратные скобки "--- нотация Айверсона \cite{knuth98concrete},
переводящая логическое выражение в~число $0$ или $1$ по~правилам~$[\text{истина}] = 1$, $[\text{ложь}] = 1$.

Для~краткости в~тех случаях, когда из~контекста ясно, о~каком методе обучения идет речь, мы будем опускать индекс~$\mu$ и писать просто $Q_{\varepsilon}(A)$.

Функционал $Q_\eps(A)$ уже не~зависит от~выбора разбиения
и~характеризует качество данного метода обучения на~данной генеральной выборке.

При минимизации эмпирического риска может возникать неоднозначность "---
несколько алгоритмов из~$A(\Xl)$ могут иметь одинаковое число ошибок на~обучающей выборке.
В~\cite{voron09mmro} для устранения неоднозначности и~получения точных верхних
оценок вероятности переобучения использовалась \emph{пессимистичная}
минимизация эмпирического риска "--- предполагалось, что в~случае неоднозначности
выбирается алгоритм с~наибольшим числом ошибок на~генеральной выборке~$\XX$.
Это не~устраняет неоднозначность окончательно.
Возможны ситуации, когда несколько алгоритмов имеют
наименьшее число ошибок на~обучающей выборке~$\Xl$
и~одинаковое число ошибок на~генеральной выборке~$\XX$.
\mbox{В~таких} случаях на~множестве алгоритмов вводился линейный порядок,
и~среди неразличимых алгоритмов выбирался алгоритм с~б\'ольшим порядковым номером.
Введение приоритетности алгоритмов является искусственным приемом,
не~имеющим адекватных аналогов среди известных методов обучения.

\subsection{Рандомизированная минимизация эмпирического риска}

\emph{Рандомизированный метод минимизации эмпирического риска}
выбирает произвольный алгоритм из~множества~$A(X)$ случайно и~равновероятно
\cite{frey:mmro}.
Поскольку в~задаче статистического обучения появляется второй независимый источник случайности,
определение вероятности переобучения~$Q_\eps(A)$ приходится модифицировать.
Наиболее естественный вариант модификации "--- усреднение по~множеству $A(X)$:
\begin{equation}
\label{QepsRERM.sym}
    Q_\eps(A)
    =
    \Expect
    \frac1{|A(X)|} \sum_{a\in A(X)}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\end{equation}

Переставим местами знаки суммирования, ${\Expect\sum=\sum\Expect}$.
Получим сумму по~всем алгоритмам ${a\in A}$,
каждое слагаемое которой обозначим через $Q_\eps(a, A)$
и~назовём \emph{вкладом алгоритма}~$a$ в~вероятность переобучения:
\[
    Q_\eps(A)
    =
    \sum_{a\in A}
    Q_\eps(a,A),
    \quad
    Q_\eps(a,A)
    =
    \Expect
    \frac{\bigl[a\in A(X)\bigr]}{|A(X)|}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\]

Аналогичным образом введём \emph{вероятность реализации алгоритма}~$a$:
\begin{equation}
\label{eq:PaRERM}
    P(a,A)
    =
    \Expect \frac
        {\bigl[a\in A(X)\bigr]}
        {|A(X)|}.
\end{equation}

В~частном случае, когда множество~$A$ состоит из~единственного алгоритма,
вероятность переобучения~\eqref{QepsRERM.sym} и~вероятность реализации~\eqref{eq:PaRERM}
принимают привычный вид:
\[
    Q_\eps(A) = \Expect
    \bigl[
        \delta(\mu X,X) \geq \eps
    \bigr],
    \quad
    P(a,A) = \Expect
    \bigl[
        a\!=\!\mu X
    \bigr].
\]

Рандомизированный метод минимизации эмпирического риска~$\mu_r$
занимает промежуточное положение между
\emph{оптимистичным} и~\emph{пессимистичным} методами:
\begin{align*}
    \mu_o \Xl &= \arg\!\!\min\limits_{a\in A(\Xl)} n(a,\Xk) \text{ "--- оптимистичный ММЭР}; \\
    \mu_p \Xl &= \arg\!\!\max\limits_{a\in A(\Xl)} n(a,\Xk) \text{ "--- пессимистичный ММЭР.}
\end{align*}
Приводим без доказательства следующее утверждение:
для произвольного множества алгоритмов $A \subseteq \AA$
и~каждого $\epsilon \in (0, 1]$ справедлива цепочка неравенств:
\begin{equation}
    \label{eq:sortedMethods}
    Q_{\mu_o,\,\varepsilon}(A) \leq Q_{\mu_r,\,\varepsilon}(A) \leq Q_{\mu_p,\,\varepsilon}(A).
\end{equation}

\subsection {Перестановки объектов}

Введём симметрическую группу~$S_L$ всех $L!$~перестановок, действующую на~выборке ${\XX = (x_1,\ldots,x_L)}$.
Возьмём произвольную перестановку ${\pi\in S_L}$.
Обозначим через~$\pi x$ тот объект, в~который объект~${x\in\XX}$ переходит под действием перестановки~$\pi$.
Действие перестановок на~объектах естественным образом переносится
на~подмножества объектов, на~алгоритмы как бинарные векторы ошибок длины~$L$
и~на~множества алгоритмов:

\medskip
\noindent действие перестановки~$\pi$ на~подмножество объектов:
\[\pi X = \bigl\{ \pi x \colon x\in X \bigr\};\]
действие перестановки~$\pi$ на~алгоритм:
\[\pi\vec a
    = \bigl( I(\pi a, x_i) \bigr){}_{i=1}^L
    = \bigl( I(a, \pi^{-1}x_i) \bigr){}_{i=1}^L;\]
действие перестановки~$\pi$ на~множество алгоритмов:
\[\pi A = \bigl\{ \pi a \colon a\in A \bigr\}.\]

\medskip
Заметим, что действие одной и~той~же перестановки~$\pi$
сначала на~выборку~$\XX$, затем на~алгоритм~$a$,
восстанавливает исходный вектор ошибок алгоритма~$a$.
Благодаря такому определению
действие на~алгоритм обладает рядом полезных свойств.

\begin{lemm}
\label{lem:piSL}
    Свойства действия произвольной перестановки ${\pi\in S_L}$:

    1) $I(\pi a, \pi x) = I(a,x)$ для любых $a\in A$ и~$x\in\XX$;

    2) $n(\pi a, \XX) = n(a,\XX)$ для любого $a\in A$;

    3) $n(\pi a, \pi X) = n(a, X)$ для любых $a\in A$ и~$X\subseteq\XX$;

    4) $\delta(\pi a, \pi X) = \delta(a, X)$ для любых $a\in A$ и~$X\subseteq\XX$;

    5) $[a \in A(X)] = [\pi a \in (\pi A)(\pi X)]$ для любых $a\in A$ и~$X\subseteq\XX$;

    6) $|A(X)| = |(\pi A)(\pi X)|$ для любых $A$ и~$X\subseteq\XX$;

    7) $\rho(a, a') = \rho(\pi a, \pi a')$ для любых $a, a' \in A$, где
       $\rho(a, a')$ "--- расстояние Хэмминга векторами ошибок алгоритмов $a$ и~$a'$:
\[
    \rho(a, a') = \sum \limits_{x \in \XX} |I(a, x) - I(a', x)|.
\]
\end{lemm}

\begin{vkProof}
    Свойство 1) следует из~определения:
    \[
        I(\pi a, \pi x)
        = I(a, \pi^{-1} \pi x)
        = I(a,x).
    \]
    Свойство 2) следует из~свойства 1):
    \[
        n(\pi a, \XX)
        = \sum_{i=1}^L I(\pi a, x_i)
        = \sum_{i=1}^L I(\pi a, \pi x_i)
        = \sum_{i=1}^L I(a, x_i)
        = n(a,\XX).
    \]
    Свойство 3) также следует из~свойства 1):
    \[
        n(\pi a, \pi X)
        = \sum_{x\in \pi X} I(\pi a, x)
        = \sum_{x\in X} I(\pi a, \pi x)
        = \sum_{x\in X} I(a, x)
        = n(a,X).
    \]
    Свойство 4) следует из~свойства 3):
    \begin{multline*}
        \delta(a, X)
        = \frac{L-n(a,X)}k - \frac{n(a,X)}\ell ={}\\
        {}= \frac{L-n(\pi a,\pi X)}k - \frac{n(\pi a,\pi X)}\ell
        = \delta(\pi a, \pi X).
    \end{multline*}
    Свойство 5) следует из~определения \ref{eqERM-A(X).sym} и~свойства 1):

    \begin{align*}
        a_0 \in A(X) \;\Leftrightarrow \;\;
        & a_0 \in \argmin_{a \in A} n(a, X) \;\Leftrightarrow {}\\
        & \forall a \in A \rightarrow n(a_0, X) \leq n(a, X) \; \Leftrightarrow {}\\
        & \forall a \in A \rightarrow n\big(\pi a_0, \pi X\big)
        \leq n\big(\pi a, \pi X \big) \; \Leftrightarrow {}\\
        & \forall a' \in \pi A \rightarrow n\big(\pi a_0, \pi X \big)
        \leq n\big(a', \pi X \big) \; \Leftrightarrow {}\\
         & \pi a_0 \in \argmin_{a' \in \pi A} n(a', \pi X) \;\Leftrightarrow \;
            \pi a_0 \in (\pi A)(\pi X).
    \end{align*}

    Свойство 6) следует из~свойства 5):
    \begin{multline*}
       |A(X)| = \sum_{a \in A}[a \in A(X)]
            = \sum_{a \in A}[\pi a \in (\pi A)(\pi X)] = {}\\
            {}= \sum_{a' \in \pi A}[a' \in (\pi A)(\pi X)]
             = |(\pi A)(\pi X)|.
    \end{multline*}

    Свойство 7) следует из~свойства 1):
    \begin{align*}
        \rho(\pi a, \pi a')
            & = \sum_{x \in \XX} |I(\pi a, x) - I(\pi a', x)| = {}\\
            &{} = \sum_{x' \in \XX} |I(\pi a, \pi x') - I(\pi a', \pi x')| = {}\\
            &{} = \sum_{x' \in \XX} |I(a, x') - I(a', x')|
              = \rho(a, a').
    \end{align*}
\vskip-5ex
\end{vkProof}

\subsection{Группа симметрий множества алгоритмов.}
Рассмотрим множество всех перестановок, действие которых на~множество~$A$ не~меняет его:
\[
    \Sym A = \{\pi \in S_L \colon \pi A = A\}.
\]
Если подействовать любой из~перестановок $\pi\in \Sym A$ на~строки матрицы ошибок множества~$A$,
то~получится ровно то~же самое множество столбцов;
переставив столбцы, можно получить исходную матрицу ошибок.
Очевидно, множество~$\Sym A$ является группой.
Будем называть её \emph{группой симметрий} множества алгоритмов~$A$.

\begin{eq}
    Рассмотрим множество алгоритмов, заданное матрицей ошибок
    \[
        \bordermatrix{& a_1 & a_2 & a_3 & a_4 & a_5 \cr
            x_1 & 1 & 1 & 1 & 0 & 0 \cr
            x_2 & 0 & 1 & 1 & 1 & 0 \cr
            x_3 & 0 & 0 & 1 & 1 & 1 \cr
            x_4 & 1 & 0 & 0 & 1 & 1 \cr
            x_5 & 1 & 1 & 0 & 0 & 1 \cr
        }
        \hspace{45pt}
        {\catcode`"12
            \xy/r3pc/:
            {\xypolygon5"X"{~={90}~>{-}~><{}~*{x_\xypolynode}}},
            +(0,-1)="bot",
            "X1";"bot"**@{.}
            \endxy
        }
    \]

    Группа симметрий данного множества алгоритмов
    совпадает с~группой симметрий правильного пятиугольника
    и~называется \emph{диэдральной группой}.
    Образующими элементами группы являются
    циклическая перестановка $\pi_1 = (x_1, x_2, x_3, x_4, x_5)$
    и~осевая симметрия $\pi_2 = (x_2, x_5)(x_3, x_4)$.
\end{eq}

Пусть далее $G \subseteq \Sym A$ "--- произвольная подгруппа группы~$\Sym A$.

Для любой перестановки ${\pi\in G}$ и~любого алгоритма ${a\in A}$
алгоритм~$\pi a$ снова лежит в~$A$.
В~таких случаях говорят, что группа~$G$ \emph{действует} на~множестве~$A$.

\emph{Орбитой} алгоритма~$a\in A$ называется множество алгоритмов
${Ga = \bigl\{ \pi a \colon \pi \in G \bigr\}}$.
Орбита также целиком лежит~в~$A$.
Орбиты двух различных алгоритмов $Ga$ и $Ga'$ либо совпадают, либо не~пересекаются.
Следовательно, множество~$A$ разбивается на~непересекающиеся подмножества "--- орбиты:
\[
    A
    \;=\!\! \bigsqcup_{\omega\in\Omega(A)} \!\!\! \omega
    \;=\!\! \bigsqcup_{\omega\in\Omega(A)} \!\!\! Ga_\omega,
\]
где
$\Omega(A)$ "--- множество всех орбит в~$A$,
$a_\omega$ "--- произвольный представитель орбиты~$\omega$.

Из~свойства~2) леммы~\ref{lem:piSL} следует, что
алгоритмы одной орбиты обязательно лежат в~одном слое.
Обратное, вообще говоря, неверно.

\begin{lemm}
\label{lem:equalPQa}
    Алгоритмы из~одной орбиты имеют равные вероятности реализации и~равные вклады в~вероятность переобучения:
    для любой перестановки $\pi$ из~$G$
    \[
        P(a,A) = P(\pi a,A),
        \quad
        Q_\eps(a,A) = Q_\eps(\pi a,A).
    \]
\end{lemm}
\begin{vkProof}
    Воспользуемся определением вероятности реализации~\eqref{eq:PaRERM}, свойствами~5), 6)
    из~леммы~\ref{lem:piSL}, и~свойством $A = \pi A$:
    \[
        P(a,A)
        =
        \Expect \frac {\bigl[a\in A(X)\bigr]} {|A(X)|}
        =
        \Expect \frac {\bigl[\pi a\in (\pi A)(\pi X)\bigr]} {|(\pi A)(\pi X)|}
        =
        \Expect \frac {\bigl[\pi a\in A(\pi X)\bigr]} {|A(\pi X)|}.
    \]
    Под знаком $\Expect$ можно всюду заменить $\pi X$ на~$X$,
    так как результат не~зависит от~порядка суммирования разбиений:
    \[
        P(a,A)
        =
        \Expect \frac {\bigl[\pi a\in A(X)\bigr]} {|A(X)|}
        =
        P(\pi a,A).
    \]

    Воспользуемся определением вероятности реализации~\eqref{eq:PaRERM},
    свойствами~4), 5), 6)  из~леммы~\ref{lem:piSL}, и~свойством $A = \pi A$:
\begin{align*}
        Q_\eps(a,A)
        & =
        \Expect \frac {\bigl[a\in A(X)\bigr]} {|A(X)|}
                \bigl [\delta(a,X) \geq \eps \bigr] = {}\\
        & {}=
        \Expect \frac {\bigl[\pi a\in (\pi A)(\pi X)\bigr]} {|(\pi A)(\pi X)|}
                \bigl [\delta(\pi a, \pi X) \geq \eps \bigr] = {}\\
        & {}=
        \Expect \frac {\bigl[\pi a\in A(\pi X)\bigr]} {|A(\pi X)|}
                \bigl [\delta(\pi a, \pi X) \geq \eps \bigr].
\end{align*}
    Вновь заменяя $\pi X$ на~$X$ под знаком $\Expect$, получим:
    \[
        Q_\eps(a,A)
        =
         \Expect \frac {\bigl[\pi a\in A(X)\bigr]} {|A(X)|}
                \bigl[ \delta(\pi a, X) \geq \eps \bigr]
        =
        Q_\eps(\pi a,A).
    \]
    \vskip-4ex
\end{vkProof}

\paragraph{Разложение вероятности переобучения по~орбитам множества алгоритмов.}
Из~теоремы о~равном вкладе алгоритмов одной орбиты
немедленно следует формула разложения вероятности переобучения по~орбитам.
Она является основным инструментом получения точных оценок
для рандомизированного метода минимизации эмпирического риска.

\begin{theorem}
\label{th:QAorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения по~орбитам множества~$A$:
    \begin{equation}
    \label{eq:QAorbit}
        Q_\eps(A)
        =
        \sum_{\omega\in\Omega(A)} \!\! |\omega| \:
            \Expect
            \frac{\bigl[ a_\omega \in A(X) \bigr]}{|A(X)|}
            \bigl[
                \delta(a_\omega,X) \geq \eps
            \bigr],
    \end{equation}
    где
    $\Omega(A)$ "--- множество всех орбит в~$A$,
    $a_\omega$ "--- произвольный представитель орбиты~$\omega$.
\end{theorem}
\begin{vkProof}
    Перегруппируем слагаемые в~\eqref{QepsRERM.sym} по~орбитам множества~$A$,
    затем применим лемму~\ref{lem:equalPQa} о~равном вкладе алгоритмов одной орбиты:
    \begin{multline*}
        Q_\eps(A)
        =
        \sum_{\omega\in\Omega(A)} \sum_{a\in\omega}
        \Expect
        \frac{\bigl[a\in A(X)\bigr]}{|A(X)|}
        \bigl[
            \delta(a,X) \geq \eps
        \bigr]
        = {}
    \\
        {}=
        \sum_{\omega\in\Omega(A)} \!\! |\omega| \:
            \Expect
            \frac{\bigl[ a_\omega \in A(X) \bigr]}{|A(X)|}
            \bigl[
                \delta(a_\omega,X) \geq \eps
            \bigr].
    \end{multline*}
    \vskip-4ex
\end{vkProof}

\paragraph{Разложение вероятности переобучения по~орбитам множества разбиений.}
В~некоторых случаях удобнее делать группировку слагаемых
не~по~орбитам множества алгоритмов, а~по~орбитам множества разбиений.
Напомним, что через~$\XXell$ мы~обозначаем
множество всех $\ell$"~элементных подмножеств генеральной выборки~$\XX$.

Представим вероятность переобучения в~виде суммы вкладов разбиений:
\[
    Q_\eps(A)
    =
    \Expect\,
    Q_\eps(X,A),
    \quad
    Q_\eps(X,A)
    =
    \frac1{|A(X)|}
    \sum_{a\in A(X)}
    \bigl[\delta(a,X) \geq \eps \bigr],
\]
где $Q_\eps(X,A)$ "--- вклад разбиения~$\Xl\sqcup\Xk$ в~вероятность переобучения.
Поскольку разбиениям $X\sqcup\Xk$ взаимно однозначно соответствуют выборки $X\in\XXell$,
далее будем говорить также о~\emph{вкладе выборки}~$X$ в~вероятность переобучения.

\emph{Орбитой} выборки ${X\in \XXell}$ называется множество выборок
${GX = \bigl\{ \pi X \colon \pi \in G \bigr\}}$.
Множество всех выборок длины~$\ell$ разбивается на~непересекающиеся орбиты:
\[
    \XXell
    \;=\!\! \bigsqcup_{\tau\in\Omega\XXell} \!\!\! \tau
    \;=\!\! \bigsqcup_{\tau\in\Omega\XXell} \!\!\! GX_\tau,
\]
где
$\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
$X_\tau$ "--- произвольный представитель орбиты~$\tau$.

\begin{lemm}
\label{th:equalQX}
    Выборки из~одной орбиты имеют равные вклады в~вероятность переобучения:
    $Q_\eps(X,A) = Q_\eps(\pi X,A)$
    для любой перестановки $\pi$ из~$G$.
\end{lemm}
\begin{vkProof}
    Воспользуемся сначала определением вклада выборки,
    свойствами~4), 5), 6) из~леммы~\ref{lem:piSL}, и~затем свойством $A = \pi A$:
    \begin{align*}
        Q_\eps(X,A)
        & =
        \sum_{a\in A(X)} \frac{\bigl[\delta(a, X) \geq \eps \bigr]}{|A(X)|} = {}\\
        & {}=
        \sum_{a'\in (\pi A)(\pi X)} \frac{\bigl[\delta(a',\pi X) \geq \eps \bigr]}{|(\pi A)(\pi X)|} = {}\\
        & {}=
        \sum_{a'\in A(\pi X)} \frac{\bigl[\delta(a',\pi X) \geq \eps \bigr]}{|A(\pi X)|}
          =
          Q_\eps(\pi X, A).
    \end{align*}
    \vskip-4ex
\end{vkProof}

\begin{theorem}
\label{th:QXorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения по~орбитам множества~$\XXell$:
    \begin{equation}
    \label{eq:QXorbit}
        Q_\eps(A)
        =
        \frac1{\CLl}
        \sum_{\tau\in\Omega\XXell}
        \frac{|\tau|}{|A(X_\tau)|}
        \sum_{a\in A(X_\tau)}
        \bigl[\delta(a,X_\tau) \geq \eps \bigr],
    \end{equation}
    где
    $\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
    $X_\tau$ "--- произвольный представитель орбиты~$\tau$.
\end{theorem}
Доказательство аналогично доказательству теоремы~\ref{th:QAorbit}.

В~качестве примера применения полученных формул рассмотрим множество $\AA = \{0, 1\}^L$,
состоящее из~всех возможных бинарных векторов ошибок.

\begin{theorem}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к~множеству всех алгоритмов $\AA = \{0, 1\}^L$, дается формулой:
\[
    Q_\varepsilon(\AA) =
    \frac 1{2^k}\!
    \sum_{m = \lceil \epsilon k \rceil}^k
    \!\!\Binom{k}{m}.
\]
\end{theorem}

\begin{vkProof}
Для всех перестановок $\pi \in S_L$ выполнено $\pi \AA = \AA$.
Следовательно, $\Sym A = S_L$.
Заметим, что для каждой пары обучающих выборок $X$, $X'$
возможно указать перестановку $\pi \in S_L$, такую что $X' = \pi X$.
Такую ситуацию называют <<транзитивным действием группы $S_L$ на~множестве $\XXell$>>.
Для нас это означает, что имеется лишь одна орбита $\tau = \XXell$.
Выбрав произвольную выборку $\Xl$ в~качестве ее представителя,
и~воспользовавшись теоремой \ref{th:QXorbit}, получим
\[
    Q_\eps(\AA) = \frac 1{|\AA(\Xl)|} \sum_{a \in \AA(\Xl)} [\delta(a, \Xl) \geq \eps].
\]
Множество $\AA(\Xl)$ состоит из~всех алгоритмов, не~допускающих ошибок на~$\Xl$.
Следовательно, $|\AA(\Xl)| = 2^k$.
Для завершения доказательства осталось заметить,
что переобученными в~$\AA(\Xl)$ будут те~и~только те~алгоритмы,
у~которых не~менее $\lceil \epsilon k \rceil$ ошибок на~контрольной выборке.
\end{vkProof}

Из~сформулированных выше теорем~\ref{th:QAorbit} и~\ref{th:QXorbit} о~двух видах разложения вероятности переобучения по~орбитам действия группы симметрии легко следует следующее разложение,
объединяющее оба вида:
\begin{theorem}
\label{th:QAXorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения по~орбитам множества~$\XXell$:
    \[
        Q_\eps(A)
        =
        \sum_{\omega\in\Omega(A)}
        \frac{|\omega|}{\CLl}
        \sum_{\tau\in\Omega\XXell}
        |\{X\in \tau\colon a_{\omega}\in A(X)\}|
        \frac{\bigl[\delta(a_{\omega},X_\tau) \geq \eps \bigr]}{|A(X_\tau)|}.
    \]
    где
    $\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
    $\Omega(A)$ "--- множество всех орбит в~$A$,
    $a_\omega$ "--- произвольный представитель орбиты~$\omega$,
    $X_\tau$ "--- произвольный представитель орбиты~$\tau$.
\end{theorem}
Этим разложением мы воспользуемся позже при~выводе точной оценки вероятности переобучения для~хэммингова шара.

Завершая параграф, интересно рассмотреть частный случай, когда все алгоритмы из $A$
имеют равное число ошибок на полной выборке.
\begin{cor}
\label{crl:easyFormula}
Пусть все $a \in A$ имеют равное число ошибок на полной выборке: $n(a, \XX) = m$.
Тогда вероятность переобучения рандомизированного метода минимизации эмпирического риска записывается в виде
\begin{equation}
    \label{eq:easyFormula}
    Q_{\epsilon}(A)
    =
    \frac1{C_L^\ell} \sum_{\tau \in \Omega(\XXell)}
    %\textbf{E}
    \!\!|\tau| \left[\min_{a \in A} n(a, X_\tau) \leq \frac \ell L(m - \epsilon k)\right].
\end{equation}
\end{cor}

\begin{vkProof}
Отметим, что в рассматриваемом случае для~любой обучающей выборки $X$ все алгоритмы из множества~$A(X)$ либо переобучены, либо нет.
Действительно, согласно определению $A(X)$ они имеют равное число ошибок на обучении. Число ошибок на полной выборке
одинаково поскольку в силу специфики рассматриваемого случая все алгоритмы из $A$ лежат в одном слое.
Следовательно, все алгоритмы из $A$ имеют равное число ошибок на контрольной выборке, и равные уклонения частот.
Тогда, применяя формулу \eqref{eq:QXorbit}, получим следующее выражение для вероятности переобучения:
\[
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell} \sum_{\tau \in \Omega(\XXell)}
    %\textbf{E}
    \!\!|\tau| \left[\delta(a, X_\tau) > \varepsilon\right].
\]
Для получения формулы \eqref{eq:easyFormula} осталось выразить уклонение частот $\delta(a, X_\tau)$ через
число ошибок лучшего алгоритма на обучении и количество ошибок на полной выборке $m$.
\end{vkProof}

\subsection{Теорема о~порождающих и~запрещающих объектах}

Первый подход, позволивший получать точные оценки вероятности переобучения
в~рамках слабой вероятностной аксиоматики, основан на выделении порождающих
и~запрещающих объектов~\cite{voron10pria}.

\begin{hypothesis}
\label{hyp1}
    Пусть множество~$A$, выборка~$\XX$ и~детерминированный метод обучения~$\mu$ таковы, что
    для каждого алгоритма $a\in A$
    можно указать пару непересекающихся подмножеств
    ${X_a \subset \XX}$ и~${X'_a\subset \XX}$,
    %хотя~бы одно из которых не~пусто,
    удовлетворяющую условию
    \begin{equation}
    \label{eq1muX}
        \bigl[ \mu X{=}a \bigr]
        =
        \bigl[  X_a\subseteq  X \bigr]
        \bigl[ X'_a\subseteq \X \bigr],
        \quad
        \forall X\in \XXell.
    \end{equation}
\end{hypothesis}

Множество~$X_a$ называется \emph{порождающим},
$X'_a$~--- \emph{запрещающим} для алгоритма~$a$.
Гипотеза~\ref{hyp1} означает, что
метод~$\mu$ выбирает алгоритм~$a$
тогда и~только тогда, когда в~обучающей выборке~$X$
находятся все порождающие объекты и~ни одного запрещающего.
Все остальные объекты $\XX {\setminus} X_a {\setminus} X'_a$ называются
\emph{нейтральными} для~алгоритма~$a$.

Для произвольного алгоритма $a\in A$ введём следующие обозначения:

$L_a = L - |X_a| - |X'_a|$~--- число нейтральных объектов в~генеральной выборке;

$\ell_a = \ell - |X_a|$~--- число нейтральных объектов в~обучающей выборке;

$m_a = n(a,\XX {\setminus} X_a {\setminus} X'_a)$~--- число ошибок алгоритма~$a$ на нейтральных объектах;

$s_a(\eps) = \tfrac\ell L \bigl( n(a,\XX)-\eps k \bigr) - n(a,X_a)$~---
наибольшее число ошибок алгоритма~$a$ на~нейтральных обучающих объектах $X\setminus X_a$,
при~котором имеет место большое уклонение частот ошибок, $\delta(a,X) \geq \eps$.

Введём функцию гипергеометрического распределения:
\[
    \Hyper{L}{m}{\ell}{z}
    =
	\sum\limits_{s=0}^{\lfloor z \rfloor} \frac{\Binom{m}{s} \Binom{L-m}{\ell-s}}{\CLl}.
\]

\begin{theorem}
\label{th1}
    Если справедлива гипотеза~\ref{hyp1},
    то~вероятность получить в~результате обучения алгоритм~$a$ равна
    $P_a(A) = \Prob[ \mu X{=}a ] = {C_{L_a}^{\ell_a}} / {C_{L}^{\ell}}$,
    вероятность переобучения равна
    \[
    Q_\eps(A) =
    \sum_{a\in A}
        P_a
        \BHyper{L_a}{m_a}{\ell_a}{s_a(\eps)}.
    \]
\end{theorem}

Данный результат позволил получить формулы вероятности переобучения
для широкого класса модельных семейств алгоритмов, в~частности
для монотонных и~унимодальных сетей.

Теорема \ref{th1} получена для детерминированных методов обучения,
для которых результатом обучения является один алгоритм ${a\in A}$.
В~случае рандомизированного метода результатом обучения является подмножество ${A(X) \subseteq A}$.
Таким образом, множество алгоритмов~$A$ порождает
множество подмножеств алгоритмов, получающихся в~результате обучения
\[
    \fA = \bigl\{ A(X) \colon X \in \XXell \bigr\}.
\]

\begin{hypothesis}
\label{hyp2}
    Пусть множество~$A$ и~выборка~$\XX$ таковы, что
    для~каждого $\alpha \in \fA$
    можно указать пару непересекающихся подмножеств
    ${X_\alpha \subset \XX}$ и~${X'_\alpha\subset \XX}$,
    удовлетворяющую условию
    \begin{equation}
    \label{eq2muX}
        \bigl[ A(X) \!=\! \alpha \bigr]
        =
        \bigl[  X_\alpha\subseteq  X \bigr]
        \bigl[ X'_\alpha\subseteq \X \bigr],
        \;\;
        \forall X\in \XXell.
    \end{equation}
\end{hypothesis}

Следующая теорема является непосредственным обобщением теоремы~\ref{th1}
для рандомизированного метода минимизации эмпирического риска.

\begin{theorem}
\label{th2}
Если справедлива гипотеза~\ref{hyp2},
то вероятность переобучения рандомизированного метода минимизации эмпирического риска есть
\[
    Q_\eps(A) = \sum_{a \in A} \sum_{\alpha \in \fA} \frac {[a \in \alpha]}{|\alpha|}
        \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        \BHyper{L_\alpha}{m^a_\alpha}{\ell_\alpha}{s^a_\alpha(\eps)},
\]
где введены следующие обозначения:
\begin{align*}
    L_\alpha &= L - |\Xl_\alpha| - |\Xk_\alpha|;\quad
    \ell_\alpha = \ell - |\Xl_\alpha|;\\
    m^a_\alpha &= n(a, \XX \backslash \Xl_\alpha \backslash \Xl'_\alpha); \\
    s^a_{\alpha}(\eps) &= \tfrac\ell L \bigl(n(a, \XX) - \eps k\bigr) - n(a, X_\alpha).
\end{align*}
\end{theorem}

\begin{vkProof}
Рассмотрим функционал $Q_\eps(A)$. Введем под~знак суммированя по~$X$ два вспомогательных суммирования:
первое "--- по~всем $\alpha \in \fA$ при условии $\alpha = A(X)$, второе "--- по~всем значениям
$s$ числа ошибок алгоритма $a$ на~подвыборке $X \backslash X_\alpha$.
Очевидно, значение $Q_\eps(A)$ от~этого не~измениться:
\begin{align}
\label{pz1.proof}
    & Q_\eps(A)  = \Expect\!\!\sum_{a \in A(X)} \frac {1}{|A(\Xl)|} [\delta (a, \Xl) \geq \eps ] ={} \notag\\
%                & {}= \Expect \sum_{\alpha \in \fA} \sum_{a \in A(X)}
%                      \frac {[\alpha = A(X)]}{|A(\Xl)|} [\delta (a, \Xl) \geq \eps ] \\
                 & {} = \Expect \sum_{\alpha \in \fA} \sum_{a \in \alpha}
                      \frac {[\alpha = A(X)]}{|\alpha|}
                            [\delta (a, \Xl) \geq \eps] ={}\notag\\
                 & {}= \Expect \sum_{\alpha \in \fA} \sum_{a \in \alpha} \sum_{s=0}^{\ell_\alpha}
                      \frac {[\alpha = A(X)]}{|\alpha|}
                            [n(a, \Xl \backslash X_{\alpha}) = s]
                            [\delta (a, \Xl) \geq \eps].
\end{align}
Число ошибок алгоритма $a$ на~обучающей подвыборке $\Xl$ равно $s + n(a, X_\alpha)$, поэтому
отклонение частот выражается в~виде
\[
    \delta(a, \Xl) = \frac{n(a, \XX) - s - n(a, X_\alpha)}{k} - \frac{s + n(a, X_\alpha)}{\ell},
\]
следовательно
\[
    [\delta(a, \Xl) \geq \eps]
        = \bigl[s \leq \frac{\ell}L (n(a, \XX) - \eps k) - n(a, X_\alpha)\bigr]
        = [s \geq s^a_\alpha(\eps)].
\]

Подставим полученное выражение в~\eqref{pz1.proof},
затем заменим ${[\alpha = A(X)]}$ правой частью равенства \eqref{eq2muX}
и~переставим знаки суммирования (очевидно, $\Expect$ также можно рассматривать как суммирование):

 \begin{align}
    \label{eqTh1-proof2}
        &\hspace{-25pt}Q_\eps(A)
        ={}\\
        {}&\hspace{-30pt}=\!\sum_{\alpha \in \fA}
        \sum_{a\in \alpha}
        \sum_{s=0}^{\ell_a}
            \frac 1{|\alpha|}
            \underbrace{
                \Expect
                \bigl[  X_\alpha\subseteq  X \bigr]
                \bigl[ X'_\alpha\subseteq \X \bigr]
                \bigl[ n(a,X{\setminus} X_\alpha) = s \bigr]
            }_{N(\alpha, a)}
            \bigl[ s\leq s^a_\alpha(\eps) \bigr]\notag.
    \end{align}

    Выделенное в~данной формуле выражение $N(\alpha, a)$ есть
    доля разбиений генеральной выборки~$\XX=X\sqcup\X$
    таких, что
    множество объектов~$X_\alpha$ целиком лежит в~$X$,
    множество объектов~$X'_\alpha$ целиком лежит в~$\X$
    и~в~подвыборку $X{\setminus} X_\alpha$ длины~$\ell_\alpha$ попадает ровно~$s$~объектов,
    на~которых алгоритм~$a$ допускает ошибку.

    Для наглядности представим вектор ошибок~$a$ разбитым на~шесть блоков:
    \[
        \vec a = \bigl(\:
            \underbrace{
                X_\alpha;
                \underbrace{
                    \overbrace{1,\ldots,1}^{s}\,;
                    0,\ldots,0
                }_{X\setminus X_\alpha}
            }_{X}\,;
            \underbrace{
                X'_\alpha;
                \underbrace{
                    \overbrace{1,\ldots,1}^{m^a_\alpha-s}\,;
                    0,\ldots,0
                }_{\X\setminus X'_\alpha}
            }_{\X}
        \;\bigr).
    \]

    Число ошибок алгоритма~$a$ на~объектах,
    не~попадающих ни~в~$X_\alpha$, ни~в~$X'_\alpha$, равно~$m^a_\alpha$.
    Существует $\Binom{m^a_\alpha}{s}$ способов выбрать из~них $s$~объектов,
    которые попадут в~$X{\setminus} X_\alpha$.
    Для каждого из~этих способов имеется ровно $\Binom{L_a-m^a_\alpha}{\ell_\alpha-s}$ способов
    выбрать $\ell_\alpha-s$ объектов, на~которых алгоритм~$a$ не~допускает ошибку,
    и~которые также попадут в~$X{\setminus} X_\alpha$.
    Тем самым однозначно определяется состав выборки $X{\setminus} X_\alpha$,
    а,~значит, и~состав выборки  $\X{\setminus} X'_\alpha$.
    Таким образом,
    $N(\alpha, a) = {\Binom{m^a_\alpha}{s} \Binom{L_\alpha-m^a_\alpha}{\ell_\alpha-s}} / \CLl$.
    Подставим это выражение в~\eqref{eqTh1-proof2}
    и~выделим в~нём формулу гипергеометрической функции вероятности:
    \begin{align*}
        Q_\eps(A)
        & =
        \sum_{\alpha \in \fA}
        \sum_{a\in \alpha}
        \frac 1{|\alpha|}
        \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        \sum_{s=s_0}^{\ell_\alpha}
            \bigl[ s\leq s^a_\alpha(\eps) \bigr]
            \frac{\Binom{m^a_\alpha}{s} \Binom{L_\alpha - m^a_\alpha}{\ell_\alpha-s}}
                {\Binom{L_\alpha}{\ell_\alpha}} = {}\\
          & {}=
        \sum_{a \in A}
          \sum_{\alpha \in \fA}
        \frac {[a \in \alpha]}{|\alpha|}
          \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
           H_{L_\alpha}^{\ell_\alpha, m^a_\alpha} \bigl(s^a_\alpha(\eps)\bigr).
    \end{align*}

    Теорема доказана.
\end{vkProof}

\begin{cor}
\label{cor1}
    Пусть во~множестве~$A$ найдётся алгоритм~$a_0$,
    такой, что для любого ${a \in A}$ вектор ошибок
    алгоритма~$a_0$ содержится в~векторе ошибок алгоритма~$a$.
    Обозначим через $X_0$ множество объектов, на~которых ошибается алгоритм~$a_0$.
    Пусть система порождающих и~запрещающих множеств такова, что для всех~$\alpha \in \fA$ выполнено
    ${X_0 \cap X_\alpha = \emptyset}$ и~${X_0 \cap X'_\alpha = \emptyset}$.
    Тогда
    \[
        m^a_\alpha = n(a_0, \XX), \quad
        s^a_{\alpha}(\eps) = \tfrac{\ell}{L}\bigl(n(a, \XX) - \eps k\bigr).
    \]
\end{cor}
\begin{vkProof}
Зафиксируем обучающую выборку $X \in \XXell$ и~пусть $\alpha = A(X)$.
Докажем, что из~$a \in \alpha$ следует $n(a, X_\alpha) = 0$.
Пусть $a$ ошибается на~объекте $x$. Нам необходимо доказать, что $x \notin X_\alpha$.
Допустим обратное, тогда по~определению запрещающих объектов $x \in X_\alpha$ обязан лежать
в~обучении.
Условие $X_0 \cap X_\alpha = \emptyset$ означает, что $a_0$ не~ошибается на~$x$.
Следовательно, алгоритм $a$ делает как минимум на~одну ошибку больше, чем $a_0$ на~обучении.
Противоречие.

Второе утверждение заключается в~том, что из~$a \in \alpha$ следует
$n(a, \XX) = n(a_0, X_0) + n(a, X'_\alpha)$.
Запишем число ошибок алгоритма~$a$ в~виде $n(a, \XX) = n(a, X_0) + n(a, X \setminus X_0)$.
Из определения доминирующего алгоритма следует, что $n(a, X_0) = n(a_0, X_0)$.
Осталось доказать, что $n(a, X \setminus X_0) = n(a, X'_\alpha)$.
Отметим, что из~условия $X_0 \cap X'_\alpha$ следует, что $X'_\alpha \subset X \setminus X_0$,
а значит $n(a, X \setminus X_0) \geq n(a, X'_\alpha)$. Осталось доказать, что каждая ошибка
$a \in X \setminus X_0$ алгоритма $a$ принадлежит $X'_\alpha$.
Это следует из~того, что алгоритмы $a_0$ и~$a$ обязаны быть неразличимыми на~обучении.

Из доказанных выше утверждений следует, что
\begin{align*}
m^a_\alpha &= n(a, \XX \backslash \Xl_\alpha \backslash \Xl'_\alpha) = n(a_0, X_0) = n(a_0, \XX);\\
s^a_{\alpha}(\eps)
    &= \frac{\ell}{L}\bigl(n(a, \XX) - \eps k\bigr) - n(a, X_\alpha)
    = \frac{\ell}{L}\bigl(n(a, \XX) - \eps k\bigr).
    \end{align*}
\end{vkProof}

\begin{cor}
Полученная формула легко объединяется с~теоремой о~разбиении множества алгоритмов на~орбиты:
\begin{equation}
\label{fjerkgjwaxm}
\begin{aligned}
    Q_\eps(A) & =     \sum_{\omega \in \Omega(A)}
                        %\sum_{\substack{
                        %        \alpha \in \fA \colon\\
                        %        a_\omega \in \alpha
                        %    }}
                        \sum_{\alpha \in \fA}
                            [a_\omega \in \alpha]
                            \frac {|\omega|}{|\alpha|}
                              \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
                            H_{L_\alpha}^{\ell_\alpha, m^{a_\omega}_\alpha} \bigl(s^{a_\omega}_\alpha(\eps)\bigr).
\end{aligned}
\end{equation}
\end{cor}

\begin{vkProof}
Доказательство немедленно следует из~леммы~\ref{lem:equalPQa} о~равном вкладе
алгоритмов одной орбиты в~вероятность переобучения.
\end{vkProof}

\section{Точные оценки вероятности переобучения}

\subsection{Монотонная цепочка алгоритмов}

В~следующих параграфах изучаются несколько модельных параметрических семейств алгоритмов,
в которых количество ошибок монотонно возрастает
по~мере удаления вектора параметров от~оптимального значения.

\begin{df}
Множество алгоритмов $\{a_0, \ldots, a_D\}$ называется
\emph{монотонной цепочкой}, если выполнены два условия:
\begin{itemize}
    \item[1)]
        монотонность числа ошибок:
        $n(a_i, \XX) = m + i$,\: $i = 0, \ldots, D,$ при некотором фиксированном~$m$;
    \item[2)]
        поглощение ошибок предыдущего алгоритма:
        ${\rho(a_{i}, a_{i-1}) = 1}$,\: $i = 1, \ldots, D$,
        где $\rho(a, a')$ "--- расстояние Хэмминга между векторами ошибок $a$ и~$a'$.
\end{itemize}
\end{df}
Таким образом, в~монотонной цепочке каждый следующий алгоритм ошибается на~тех же объектах,
что и~предыдущий, и~допускает еще одну дополнительную ошибку.

Монотонная цепочка алгоритмов "--- это простейшая модель однопараметрического
\emph{связного семейства алгоритмов}, предполагающая,
что при непрерывном удалении некоторого параметра от~оптимального значения
число ошибок на~полной выборке только увеличивается.

\begin{eq}
\label{exLinearClassifier}
    Пусть
    $A$ "--- семейство
    \emph{линейных алгоритмов классификации} "---
    параметрических отображений из~$\XX = \RR^n$ в~$\mathbb{Y} \brop= \{-1,+1\}$ вида
    \[
        a(x,w) = \sign\left( x_1 w_1 + \ldots + x_n w_n \right),
        \quad
        x=(x_1,\ldots,x_n)\in \RR^n,
    \]
    где параметр ${w\in\RR^n}$ "--- направляющий вектор гиперплоскости,
    разделяющей пространство~$\RR^n$ на~два полупространства "--- классы~$-1$ и~$+1$.
    Пусть функция потерь имеет вид
    $I(a,x)\brop = \bigl[ a(x,w)\neq y(x) \bigr]$,
    где $y(x)$ "--- истинная классификация объекта~$x$,
    и~множество объектов~$\XX$ линейно разделимо,
    т.\,е.~существует вектор ${w^{*} \in\RR^n}$,
    при котором алгоритм~$a(x,w^{*})$ не~допускает ошибок на~$\XX$.
    Тогда множество алгоритмов
    \[
        A[\delta\,]
        =
        \bigl\{
            a(x,w^{*}+t\delta) \colon
            t\in[0,+\infty)
        \bigr\}
    \]
    порождает монотонную цепь при любом ${\delta\in\RR^n}$,
    за~исключением, быть может, некоторого конечного множества векторов.
    При~этом~$m=0$ в~силу линейной разделимости.
\end{eq}

\begin{theorem}
    \label{th:monotonicChain}
    Для монотонной цепочки из~$D + 1$ алгоритмов вероятность переобучения РМЭР равна
    \begin{equation}
        \label{formula:monotonicChain}
           Q_\eps(A) =
            %\frac{1}{\CLl}
            \sum_{d=0}^{D} \sum_{t=d}^D
            \frac 1{1 + t}
            \frac{\Binom{L'}{\ell'}}{\CLl}
            H_{L'}^{\ell\,', m}\bigl(s(\epsilon)\bigr),
    \end{equation}
    где
        $L' = L {-} t {-} F$,\:
        $\ell' = \ell {-} F$,\:
        $F = [t{\neq}D]$,\:
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m {+} d {-} \epsilon k) \bigr\rfloor$.
%        $H_{L}^{\ell, m}(z) = \frac {1}{\CLl}\sum \limits_{s=0}^{\lfloor z \rfloor} \Binom{m}{s}
% \Binom{L-m}{\ell - s}$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{theorem}

\begin{vkProof}
     в~данном случае доказательство практически полностью совпадает с~доказательством формулы
     о~вероятности переобучения пессимистического ММЭР для монотонной цепи.
     Действительно, перенумеруем объекты так, как показано в~следующей таблице:
    \[
        \begin{array}{rccccclll}
                        & x_1 & x_2 & x_3 &   & x_D &  & \overbrace{\hphantom{1,\ldots,1}}^{m} &
        \\
            \vec a_0 = ( & 0,  & 0,  & 0,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \vec a_1 = ( & 1,  & 0,  & 0,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \vec a_2 = ( & 1,  & 1,  & 0,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \vec a_3 = ( & 1,  & 1,  & 1,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \ldots\;\: &&&& \ldots && \quad\ldots & \quad\ldots
        \\
            \vec a_D = ( & 1,  & 1,  & 1,  & \ldots  & 1,  & 0,\ldots,0, &  1,\ldots,1  & );
        \end{array}
    \]
     При такой нумерации каждый из~алгоритмов~$a_t$,\, $t=1,\ldots,D,$
    допускает ошибку на~объектах $x_1,\ldots,x_t$.
    Очевидно, лучший алгоритм~$a_0$ не~ошибается ни~на~одном из~этих объектов.
    Нумерация остальных объектов не~имеет значения,
    так как алгоритмы не~различимы на~них.

    Заметим, что для любой обучающей выборки $X$ множество~$A(X)$ будет состоять
    из~алгоритмов $\alpha_t \equiv \{a_0, \ldots, a_t\}$ для~некоторого $t$.
    Отметим, что $|\alpha_t| = t + 1$.
    Зафиксируем $t$ и~рассмотрим два случая:

1.\enspace
    Если $t=D$,
    то $\alpha_t$ совпадает со всем множеством алгоритмов, следовательно
     $A(X) = \alpha_t$ тогда и~только тогда, когда
    все объекты $\{x_1,\ldots,x_D\}$ будут находиться в~контрольной подвыборке~$\X$.
    В~этом случае
    \[
        [A(X){=}\alpha_t] = [x_1,\ldots,x_D \in \X].
    \]

2.\enspace
    Во~всех остальных случаях
    $A(X) = \alpha_t$ тогда и~только тогда, когда
    все объекты $\{x_1,\ldots,x_t\}$ будут находиться в~контрольной подвыборке~$\X$,
    а~объект $x_{t+1}$ "--- в~обучающей подвыборке~$X$.
    В~этом случае
    \[
        [A(X){=}\alpha_t] = [x_{t+1} \in X][x_1,\ldots,x_t \in \X].
    \]

    Для данной системы порождающих и~запрещающих множеств можно применить
    следствие~\ref{cor1} из~теоремы~\ref{th2}.

    Множеству~$\alpha_t$ и~алгоритму $a_d$ ($d \leq t$)
    соответствуют следующие значения параметров
    (для упрощения обозначений
    вместо двойных индексов $L_{\alpha_t}$ будем использовать одинарные~$L_t$):
    \hfill$L_t =$\\ $=L - t - 1$,\:
    $\ell_t = \ell - [t = D]$,\;
    $m^d_t = m + d - d = m$,\:
    $s^d_t(\eps) \brop= \tfrac\ell L(m+d-\eps k)$.
    Подставляя эти значения в~общую формулу из~теоремы \ref{th2} о порождающих и запрещающих объектах,
    получаем утверждение нашей теоремы.
\end{vkProof}

В~приведенном доказательстве мы не~рассматривали отдельно случай $D \leq k$ и~$D > k$.
Эти эффекты уже учтены корректно благодаря доопределению нулем биномиальных коэффициентов
и~гипергеометрического распределения.
Так же отметим, что в~случае монотонной цепочки группа симметрии в~данном случае была тривиальной,
и~потому не~учитывалась при вычисления.

\subsection{Унимодальная цепочка алгоритмов}

\begin{df}
Множество алгоритмов \[\{a_0,\, a_1, \ldots, a_D,\, a'_1, \ldots, a'_D\}\] называется
унимодальной цепочкой, если выполнены два условия:
\begin{itemize}
  \item[1)] левая ветвь $\{a_0, a_1, \ldots, a_D\}$ и
        правая ветвь $\{a_0, a'_1, \ldots, a'_D\}$
        являются монотонными цепочками.
  \item[2)] пересечение множества ошибок алгоритмов $a_D$ и~$a'_D$ равно
        множеству ошибок алгоритма $a_0$.
\end{itemize}
\end{df}

Параметр $D$ будем называть \emph{длиной ветвей} унимодальной цепочки.

Унимодальная цепочка является более реалистичной моделью однопараметрического \emph{связного семейства}
по сравнению с~монотонной цепочкой. Если мы имеем лучший алгоритм $a_0$ c оптимальным значением
некоторого вещественного параметра, то отклонение значения этого параметра как в~б\'ольшую, так и~в~меньшую
стороны приводит к~увеличению числа ошибок.

\begin{theorem}
    \label{th:unimodalChain}
    Для унимодальной цепочки с~ветвями длины~$D$ вероятность переобучения
    рандомизированного метода минимизации эмпирического риска равна
    \begin{equation}
        \label{formula:unimodalChain}
           Q_\eps(A) =
            %\frac{1}{\CLl}
            \sum_{d=0}^{D} \sum_{t_1=d}^D \sum_{t_2 = 0}^{D}
            \frac{|\omega_d|}{1 + t_1 + t_2}
            \frac{\Binom{L'}{\ell\,'}}{\CLl}
            H_{L'}^{\ell\,', m}\bigl(s(\epsilon)\bigr),
    \end{equation}
    где \;
          $\omega_d = [d = 0] + 2 \cdot [d > 0]$,\:
        $L' = L - S - F$,\:
          $S = t_1 + t_2$,\:
        $F = [t_1{\neq}D] + [t_2{\neq}D]$,\:
        $\ell' = \ell {-} F$,\:
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m {+} d {-} \epsilon k) \bigr\rfloor$.
\cite{voron09mmro}.
\end{theorem}

\begin{vkProof}

Пронумеруем объекты генеральной выборки $\XX$ таким образом, как показано в~следующей таблице:
\[
    \bordermatrix{
         & a_0 & a_1 & a_2 & \cdots & a_D & a'_1 & a'_2 & \cdots & a'_D \cr
    x_1  & 0 & 1 & 1 & \cdots & 1 & 0 & 0 & \cdots & 0 \cr
    x_2  & 0 & 0 & 1& \cdots  & 1 & 0 & 0& \cdots  & 0 \cr
         & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots& \cdots  & \cdots \cr
    x_D  & 0 & 0 & 0 & \cdots & 1 & 0 & 0& \cdots  & 0 \vspace{-1.5ex}\cr\cline{2-10}
    x'_1 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 & \cdots & 1 \cr
    x'_2 & 0 & 0 & 0 & \cdots & 0 & 0 & 1& \cdots  & 1 \cr
         & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots& \cdots  & \cdots \cr
    x'_D & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 1 \cr
    }
\]
Нумерация остальных объектов не~имеет значения, так как алгоритмы не~различимы на~них.

Перестановками объектов выборки
(${x_1{\,\leftrightarrow\,}x'_1}, \ldots, x_D{\,\leftrightarrow\,}x'_D$)
можно поменять левую и~правую ветви местами.
Следовательно, алгоритмы разных ветвей с~равным числом ошибок лежат
в~одной орбите действия группы симметрии.
Орбита $\omega_0 = \{a_0\}$ содержит единственный алгоритм.
Для остальных орбит $\omega_d = \{a_d, a'_d\}$
договоримся выбирать алгоритм $a_d$ из~левой ветви в~качестве представителя орбиты.

Для произвольной обучающей выборки $X \in \XXell$ множество~$A(X)$ будет состоять из~алгоритмов
$a_0, a_1, \ldots, a_{t_1}, a'_1, \ldots, a'_{t_2}$ для некоторой пары $(t_1, t_2)$.
Следовательно, $\fA = \{\alpha_{t_1, t_2}\}$, где ${t_i = 0,\ldots,D}$.
Отметим, что $|\alpha_{t_1, t_2}| = \frac 1{1 + t_1 + t_2}$,
а $[a_d \in \alpha_{t_1, t_2}] \brop= [d \leq t_1]$.

Для $\alpha \equiv \alpha_{t_1, t_2}$ при $t_1, t_2$ строго меньше $D$,
множеством порождающих объектов будет
$X'_\alpha = \{ x_{t_1 + 1}, x_{t_2 + 1}\}$.
Условие $t_i = D$ уменьшает количество порождающих объектов в~$X'_\alpha$ на~единицу.
Множество запрещающих объектов $X_\alpha = \{x_1, \ldots, x_{t_1}, x'_1, \ldots, x'_{t_2}\}$.

Введя обозначение $F = [t_1{\neq}D] + [t_2{\neq}D]$, получим
\begin{align*}
L_\alpha &= L - t_1 - t_2 - F,\quad\hspace{10pt} \ell_\alpha = \ell - F;\\
m^a_\alpha &= m + d - d = m,\quad s^a_\alpha(\eps) = \lfloor \frac{\ell}L (m + d - \epsilon k) \rfloor.
\end{align*}

Подставляя эти значения в~общую формулу из теоремы \ref{th2} о~порождающих и запрещающих объектах,
получаем утверждение нашей теоремы.

\end{vkProof}

\subsection{Пучок монотонных цепочек}

\emph{Пучком из~$h$ монотонных цепочек} называется множество алгоритмов,
полученное объединением $h$ монотонных цепочек равной длины,
с~общим первым алгоритмом. Как и~в~случае унимодальной цепочки, предполагается,
что множества объектов, на~которых ошибаются алгоритмы ветвей, не~пересекаются.

Связка из~$2h$ монотонных цепочек является
моделью $h$"~параметрического семейства алгоритмов,
в~котором разрешено изменять любой из~$h$ параметров при фиксированных остальных,
а~одновременное изменение нескольких параметров не~допускается.
Данное семейство можно также рассматривать как обобщение трёх частных случаев,
рассмотренных в~\cite{voron09dan}:
монотонной цепочки~($h=1$),
унимодальной цепочки~($h=2$)
и~единичной окрестности лучшего алгоритма ($D=1$).

В~следующей теореме будет дана явная формула вероятности переобучения
для связки из~$h$~монотонных цепочек.
Введём \emph{комбинаторный коэффициент}
$R_{D,h}^d(S,F)$, который зависит от~параметров $S$~и~$F$,
от числа монотонных цепочек~$h$ и~от их~длины~$D$,
а также от~$d$ "--- минимального значения параметра~$S$.
Коэффициент $R_{D,h}^d(S,F)$ равен числу способов представить число~$S$
в виде суммы $h$~неотрицательных пронумерованных слагаемых, $S = t_1+ \ldots + t_h$,
каждое из~которых не~превосходит~$D$.
При этом ровно $F$~слагаемых не~должно равняться~$D$,
а на~первое слагаемое накладывается дополнительное ограничение $t_1 \geq d$.

\begin{theorem}
\label{th:pMonot}
    Пусть в~связке из~$h$ монотонных цепочек
    лучший алгоритм допускает $m$~ошибок на~полной выборке,
    длина каждой ветви без учета лучшего алгоритма равна~$D$.
    \mbox{Тогда} при обучении рандомизированным методом
    вероятность переобучения может быть записана в~виде:
    \begin{equation}
    \label{formula:pChainsUnion}
               Q_\eps(A) =
                \sum_{d=0}^{D}
                \sum_{S=d}^{h D}
                \sum_{F = 0}^{h}
                \frac{|\omega_d| R_{D, h}^d(S, F)}{1 + S}
                \frac{\Binom{L'}{\ell'}}{\CLl}
                H_{L'}^{\ell', m}\bigl(s(\epsilon)\bigr),
    \end{equation}
    где
        $L' = L {-} S {-} F$,\:
        $\ell' = \ell {-} F$,\:
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m + d - \epsilon k) \bigr\rfloor$;\:
        $|\omega_h| = 1$ при~$h = 0$ и~$|\omega_d| = h$ при~${d \geq 1}$.
        %$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{theorem}

\begin{figure}[t]
    \begin {multicols}{2}
%    \centering
    \hfill
        \IncludeHalfPicture{frey_monot.eps}
    \hfill
    \caption{Зависимость $Q_\varepsilon(A)$ от~$\epsilon$
             для монотонной цепочки при $L=100$, $\ell=60$, $D=40$, $m=20$.}
    \label{fig:Monot}
    \medskip
    \hfill
        \IncludeHalfPicture{frey_unit.eps}
    \hfill
    \caption{Зависимость $Q_\varepsilon(A)$ от~$\epsilon$
             для~единичной окрестности при $L=100$, $\ell=60$, $h=10$, $m=20$.}
    \label{fig:Unit}
    \end {multicols}
\end{figure}

\begin{vkProof}

Группа симметрии связки~из~$h$ монотонных цепочек является
симметрической группой $S_h$, действующей на~ветви связки всевозможными перестановками.
Следовательно, алгоритмы разных ветвей с~равным числом ошибок лежат
в~одной орбите действия группы симметрии.
Орбита $\omega_0 = \{a_0\}$ содержит единственный алгоритм.
Для остальных орбит $\omega_d = \{a^1_d, a^2_d, \ldots, a^h_d\}$
договоримся выбирать алгоритм $a^1_d$ из~первой ветви в~качестве представителя орбиты.

Для произвольной обучающей выборки $X \in \XXell$ множество $A(X)$ будет состоять из~алгоритмов
$\{a^i_j\}$ при $i = 1, \ldots, h$, $j = 1, \ldots, t_i,$ для некоторого
вектора $\vec t \equiv (t_1, t_2, \ldots, t_h)$, где $t_i \brop= 0,\ldots,D$.
Отметим, что $|\alpha_{\vec t}| = \frac 1{1 + t_1 + \ldots + t_h}$,
а $[a^1_d \in \alpha_{\vec t}] = [d \leq t_1]$.

Для $\alpha_{\vec t}$, где все~$t_i$ строго меньше $D$,
множеством порождающих объектов будет
$X'_\alpha = \{ x^1_{t_1 + 1}, \ldots, x^h_{t_h + 1}\}$.
Условие $t_i = D$ уменьшает количество порождающих объектов в~$X'_\alpha$ на~единицу.
Множество запрещающих объектов
$X_\alpha \brop= \{x^1_1, \ldots, x^1_{t_1}, x^2_1, \ldots, x^2_{t_2}, \ldots, x^h_1, \ldots, x^h_{t_h}\}$.

Введем обозначения
${S = \sum\limits_{i=1}^{p}t_i}$,\:
${F = \sum\limits_{i=1}^{p}[t_i \neq D]}$.
Тогда
$L_\alpha \brop= L {-} S {-} F$, $\ell_\alpha = \ell {-} F$,
$m_\alpha^a = n(a_0, \XX)$,\:
$s_\alpha^a = \lfloor \frac{\ell}L (m + d - \epsilon k) \rfloor$.

Подставляя эти значения в~общую формулу из теоремы \ref{th2} о порождающих и запрещающих объектах,
получаем следующее выражение для вероятности переобучения:
\[
    Q_\eps(A) =
        \sum_{d=0}^{D} \sum_{t_1=d}^D \sum_{t_2 = 0}^{D} \ldots \sum_{t_h = 0}^{D}
        \frac{|\omega_d|}{1 {+} S} \frac {\Binom{L'}{\ell'}} {\CLl} H_{L'}^{\ell', m}\bigl(s(\epsilon)\bigr).
\]

Теперь от~суммирования по~параметрам $t_i$
можно перейти к~суммированию по~множеству возможных значений $S$~и~$F$:
\[
    Q_\eps(A) =
        %\frac{1}{\CLl}
        \sum_{d=0}^{D} \sum_{S=d}^{h D} \sum_{F = 0}^{h}
         |\omega_h|
        \frac{R_{D,h}^d(S, F)}{1 {+} S}
        \frac {\Binom{L'}{\ell'}}{\CLl}
        H_{L'}^{\ell', m}\bigl(s(\eps)\bigr),
\]
где $R_{D,h}^d(S, F)$ "--- определенный выше комбинаторный коэффициент.

\end{vkProof}

\begin{cor}
    Для единичной окрестности из~${h}$ алгоритмов вероятность переобучения равна
    \begin{equation}
    \label{formula:unitVicinity}
        Q_\eps(A) =
            \sum_{d=0}^{1}
            \sum_{S=d}^{h}
            \frac{|\omega_d| \Binom{h-d}{S-d}}{1 {+} S}
              \frac {\Binom{L'}{\ell'}}{\CLl}
             H_{L'}^{\ell', m}\bigl(s(\eps)\bigr),
    \end{equation}
    где
    $L' = L {-} h$,\:
    $\ell' = \ell {+} S {-} h$.
\end{cor}

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
        \IncludeHalfPicture{frey_p.eps}
    \hfill
    \caption{Зависимость $Q_\varepsilon(A)$ от~$h$ для связки
             из~монотонных цепочек
             при $L=300$, $\ell=150$, $m=15$, $D = 1, 2, 3, 5, 10$, $\epsilon = 0.05$.}
    \label{fig:pBinding}
    \medskip
    \hfill
        \IncludeHalfPicture{frey_D.eps}
    \hfill
    \caption{Зависимость $Q_\varepsilon(A)$ от~$D$ для связки
            из~$h = 1, 2, 3, 5, 10$ монотонных цепочек
            при $L=300$, $\ell=150$, $m=15$, $\epsilon = 0.05$.}
    \label{fig:dBinding}
    \end {multicols}
\end{figure}

На~рис.\,\ref{fig:Monot}~и~рис.\,\ref{fig:Unit} представлены результаты численных экспериментов,
в~которых сравнивались вероятности переобучения для различных вариантов минимизации эмпирического риска.
Из~четырех кривых на~каждом графике верхняя (жирная) соответствует
пессимистической минимизации эмпирического риска~\cite{voron09dan,voron09mmro},
нижняя "--- оптимистической.
Две почти сливающиеся кривые между ними соответствуют рандомизированной минимизации эмпирического риска.
\mbox{Одна из~них} вычислена по~доказанным формулам,
вторая построена методом Монте-Карло по~$10^5$~случайных разбиений,
при равновероятном выборе лучшего алгоритма в~случаях неопределенности.
Различия этих двух кривых находятся в~пределах погрешности метода Монте"=Карло.

На~рис.\,\ref{fig:pBinding}~и~рис.\,\ref{fig:dBinding} представлены зависимости вероятности переобучения
от~числа~$h$ ветвей в~связке и~от их~длины~$D$.
Графики построены для рандомизированного метода минимизации эмпирического риска.
Рис.\,\ref{fig:dBinding} показывает, что при увеличении длин цепочек~$D$ вероятность переобучения
практически перестаёт расти уже при $D=7$.
Это~связано с~\emph{эффектом расслоения} "--- лишь алгоритмы из~нижних слоёв
имеют существенно отличную от~нуля вероятность быть выбранными
методом минимизации эмпирического риска.
Добавление <<слишком плохих>> алгоритмов не~увеличивает вероятность переобучения.
Рис.\,\ref{fig:pBinding} показывает, что
при~увеличении числа~$h$ цепочек в~связке вероятность переобучения продолжает расти.
Однако скорость роста сублинейна по~$h$, благодаря \emph{эффекту связности} "---
все алгоритмы находятся на~хэмминговом расстоянии не~более~$D$ от~лучшего алгоритма.

\subsection{Многомерная монотонная сеть алгоритмов}

Введём целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \ldots, h} |d_j|$,\:
$|\vec d| = |d_1| + \ldots + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из~неравенств строгое.

\begin{df}
    \label{eq:monotonicSet}
    Множество алгоритмов
    $A = \bigl\{a_{\vec d}\bigr\}$, где~$\vec d \geq 0$ и~$\|\vec d\|\leq D$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов длины $D$},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j = \{x_j^1, \ldots, x_j^D\} \subset \XX$, для всех $j = 1, \ldots, h$,
    а~так~же множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что:
    \begin{enumerate}
        \item[1)] набор $\bigl\{ U_0, U_1,\{X_j\}_{j=1}^h \bigr\}$ является разбиением множества~$\XX$
              на~непересекающиеся подмножества;
        \item[2)] $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item[3)] $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item[4)] $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{df}

Монотонная сетка алгоритмов "--- это модель параметрического \emph{связного семейства алгоритмов},
предполагающая, что при непрерывном удалении каждой компоненты вектора параметров
от~оптимального значения число ошибок на~полной выборке только увеличивается.

Обозначим $|U_1| = m$.
Из определения следует, что $n(a_{\vec d}, \XX) = m + |d|$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в~сетке}.
Множество алгоритмов с~равным числом ошибок $t+m \brop= n(a_{\vec d}, \XX)$ называются \emph{$t$-слоем} сетки.

\begin{eq}
    Монотонная двумерная сетка при $m = 0$ и~$L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{eq}

Число алгоритмов в~$h$-мерной монотонной сетке с~ветвями длины $D$ равно $(D + 1)^h$.
\emph{Укороченной} $h$-мерной монотонной сеткой $\tilde{A} \subset A$
назовем первые $D$ слоев из~$A$.
Таким образом \[\tilde{A} = \{a_{\vec d} \in A,\, |\vec d| \leq D\}.\]
Число алгоритмов в~$\tilde{A}$ равно $\Binom{D+h}{h}$.

\begin{figure}[t]
    \label{fig:MonotonicSets}
    \begin {multicols}{2}
    \centering
    \hfill
    \IncludeHalfPicture{netpics_M.eps}
    \hfill
    \medskip
    \hfill
    \IncludeHalfPicture{netpics_MS.eps}
    \hfill
    \end {multicols}
    \caption{Матрица ошибок монотонной сетки (слева) и~укороченной монотонной сетки (справа)
    при $D=20$, $h=2$, $m=5$, $L=60$.}
\end{figure}

Впервые монотонны сетки произвольной размерности были изучены П. Ботовым в~\cite{botov09mmro}.
Там же были получены формулы для~вероятности переобучения \emph{пессимистического} метода минимизации
эмпирического риска.

Численные эксперименты показывают, что при разумных сочетаниях параметров
вероятности переобучения для укороченной $\tilde{A}$ и~простой $A$ монотонных сеток различаются крайне мало.
Поэтому в~дальнейшем мы ограничимся исследованием не-укороченных монотонных сеток.
Для этого класса семейств алгоритмов будут получены явные формулы вероятности переобучения
рандомизированного метода минимизации эмпирического риска.

\begin{figure}[t]
    \centering
    \includegraphics[height=72mm]{monot_net_illustration.eps}
    \caption{Строение множества $A(X)$ для двумерной монотонной сети; $h=2$, $D=8$.}
    \label{fig1}
\end{figure}

\begin{theorem}
    Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
    примененного к~монотонной сети $A = \{ a_{\vec d} \}$ размерности $h$, $\|\vec d\| \leq D$,
    дается выражением:
    \begin{equation}
        \label{djkfqmrkfd}
        Q_\epsilon(A) = \sum_{\substack{\vec d \geq \vec 0, \\\|\vec d\| \leq D}}
                             \sum_{\substack{\vec t \geq \vec 0, \\\|\vec t\| \leq D}}
                             \frac {[\vec t \geq \vec d]} {V( \vec t )}
                             \frac{\Binom{L'}{\ell'}}{\CLl}
                             \BHyper{L'}{m}{\ell'}{s(\epsilon)},
    \end{equation}
    где
    ${V(\vec {t}) = \!\prod\limits_{j=1}^h (t_j \!+\! 1)}$,\:
    ${\ell' = \ell - \!\!\sum\limits_{j = 1}^h [t_j \!\neq\! D]}$,\:
    ${k' = k \!-\! |\vec t|}$,\:
    ${L' = \ell' + k'}$,\:
    ${s(\epsilon) = \tfrac \ell L \bigl[ m + |\vec d| - \epsilon k \bigr]}$.
\end{theorem}

\begin{vkProof}
Напомним, что через $\fA = \bigl\{ A(X) \colon X \in \XXell \bigr\}$
обозначалось множество подмножеств алгоритмов, получающихся в~результате обучения.
Пусть $A$ "--- монотонная сеть.
Тогда для~произвольной обучающей выборки $X$ множество $A(X)$ устроено специфическим образом.
На~рис.\,\ref{fig1} показано, что в $A(X)$ всегда найдется такой алгоритм $a_{\vec t}$,
что $A(X) = \{a_{\vec d} \,|\, \vec d \leq \vec t\}$.
Следовательно, $\fA = \{\alpha_{\vec t} \, \colon \, \vec t \in [0, \dots, D]^h \}$.

Обозначим через $J(\vec t)$ "--- множество тех индексов $j \in \{1, \dots, h\}$, для которых $t_j < D$.
Положим
\[
    \X_{\vec t} = \bigcup\limits_{j \in J(\vec t)} x^{t_j + 1}_j,
    \quad
    \X'_{\vec t} = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{t_j} x^i_j.
\]
Построенные таким образом $\X_{\vec t}$ и $\X'_{\vec t}$
являются порождающим и запрещающим множеством для $\alpha_{\vec t}$.
Применив теорему о порождающих и запрещающих множествах, получим формулу \eqref{djkfqmrkfd}.
\end{vkProof}

Вычисление вероятности переобучения по полученной формуле требует $O(h \cdot D^{2h})$ операций.
Теорема \ref{fjerkgjwaxm} позволяет сократить количество вычислений за счет учета симметрий монотонной сети.

\begin{lemm}
Группа симметрии монотонной сетки
размерности $h$ содержит в~качестве подгруппы группу $S_h$ всевозможных
перестановок множеств $X_1, \ldots, X_h$.
\end{lemm}

\begin{vkProof}

Все алгоритмы $h$"~мерной монотонной сетки длины $D$ индексированы
множеством вектор-индексов $\vec d \in \{0, \ldots, D\}^h$.
Тут число ошибок алгоритма $a_{\vec d} = m + |\vec d|$.

Рассмотрим алгоритм $a_{\vec d} \in A$ и~произвольную $\pi \in S_h$.
По~данному выше определению действия $\pi$ на~$\XX$ получаем, что
${\pi a_{\vec d} = a_{ \pi \vec d}}$, где действие $\pi$ на~вектор $\vec d$
определяется соответствующей перестановкой его координат.
Множество $\{0, \ldots, D\}^h$ сохраняется при применении к~нему
произвольной перестановки координат $\pi \in S_h$.
Поэтому $\forall \vec d \in \{0, \ldots, D\}^h$ выполнено $\pi \vec d \in \{0, \ldots, D\}^h$.
А следовательно, $a_{\pi \vec d} \in A$.
\end{vkProof}

Пусть $Y_h^D$ "--- множество целочисленных неотрицательных невозрастающих последовательностей
длины $h$ и~не~превосходящих $D$,
$|S_h \vec d|$ "--- число различных слов, состоящих из~символов $d_1, \ldots, d_h$.

\begin{lemm}
\label{eq:monotonicSetOrbits}
Множество орбит монотонной сетки $A = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$
под действием $S_h$ индексировано всевозможными векторами $\lambda \in Y_h^D$.
Число алгоритмов в~орбите $\omega_\lambda$, где $\lambda = (\lambda_1, \ldots, \lambda_h)$ равно
числу различных слов длины $h$, состоящих из~символов $\lambda_1, \ldots, \lambda_h$:
$|\omega_\lambda| = |S_h \lambda|$.
\end{lemm}

\begin{vkProof}
Напомним, что вместо действия $S_h$ на~${A = \{ a_{\vec d} \}}$ можно рассматривать действие
$S_h$ на~вектор индексов $\vec d$, заданное перестановками координат.

Рассмотрим орбиту произвольного алгоритма $a_{\vec d}$.
Возьмем перестановку $\pi \in S_h$, упорядочивающую координаты $\vec d$ в~порядке не-возрастания,
и~положим $\lambda = \pi \vec d$. Построенная таким образом~$\lambda$ лежит в множестве $Y_h^D$.
При этом различным $\lambda_1$ и~$\lambda_2$ будут соответствовать
различные орбиты действия группы $S_h$ на~$\{a_{\vec d}\}$.

Взаимно-однозначное соответствие между словами длины $h$ из~символов $\lambda_1, \ldots, \lambda_h$ и
количеством элементами орбиты $|\omega_\lambda|$ очевидно.
\end{vkProof}

\begin{theorem}
    \label{fdhfemndawd}
    С учетом симметрий монотонной сети вероятность переобучения записывается в виде
    \begin{equation}
    \label{ghjfkwlg}
            Q_\epsilon(A) = \sum_{\vec d \in Y_h^D}
                                 \sum_{\substack{\vec t \geq \vec d, \\\|\vec t\| \leq D}}
                                 \frac {|S_h \vec d|} {V( \vec t )}
                                 \frac{\Binom{L'}{\ell'}}{\CLl}
                                 \BHyper{L'}{m}{\ell'}{s(\epsilon)},
    \end{equation}
    где
    $Y_h^D$ "--- множество целочисленных неотрицательных невозрастающих последовательностей
    длины $h$ и~не~превосходящих $D$,
    $|S_h \vec d|$ "--- число различных слов, состоящих из~символов $d_1, \ldots, d_h$.
\end{theorem}

\begin{vkProof}
Для доказательства формулы \eqref{ghjfkwlg} необходимо воспользоваться теоремой \ref{fjerkgjwaxm}
и леммой \ref{eq:monotonicSetOrbits}.
\end{vkProof}

Расчет новой формулы требует $O(h \cdot D^h \cdot \Binom{D+h}{h})$ операций.
Рассмотрим отношение $D^h \slash C_{h+D}^h$, показывающее во сколько раз сокращается объем вычислений благодаря учету симметрии.
Данная величина максимальна при $D \gg h$. Это соответствует случаю сеток большой длины,
на~которых группа симметрии действует наиболее эффективно.
В~этом случае число операций сокращается в $h!$ раз, что в~точности соответствует количеству элементов в группе симметрий.
В остальных случаях (сетки больших размерностей и малой длины) выигрыш оказывается меньше.

\subsection{Многомерная унимодальная сеть алгоритмов}

Унимодальная сетка является более реалистичной моделью связного параметрического семейства
по~сравнению с~монотонной сеткой. Если мы имеем лучший алгоритм~$a_0$ c~оптимальным значением
вектора вещественных параметров, то~отклонение значений компонент этого вектора
как в~б\'ольшую, так~и~в~меньшую
сторону приводит к~увеличению числа ошибок.

\begin{df}
    Множество алгоритмов
    $A \!=\! \bigl\{a_{\vec d}\bigr\}$, где ${\|\vec d\|\!\leq\! D}$
    называется \emph{унимодальной $h$"~мерной сеткой алгоритмов},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j \brop= \{x_j^1, x_j^2, \ldots, x_j^D\} \subset \XX$,\:
    $Y_j = \{y_j^1, y_j^2, \ldots, y_j^D\} \subset \XX$,
    для всех $j = 1, \ldots, h$,
    а~также множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что выполнены условия:
    \begin{enumerate}
        \item[1)] Набор $\bigl\{ U_0, U_1,\{X_j\}_{j=1}^h, \{Y_j\}_{j=1}^h \bigr\}$
              является разбиением множества $\XX$
              на~непересекающиеся множества;
        \item[2)] $a_d(x_j^i) = [d_j > 0] \bigl[ i \leq |d_j| \bigr]$, где $x_j^i \in  X_j$;
        \item[3)] $a_d(y_j^i) = [d_j < 0] \bigl[ i \leq |d_j| \bigr]$, где $y_j^i \in  Y_j$;
        \item[4)] $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item[5)] $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{df}

Заметим, что данное определение отличается от~определения монотонной сетки отсутствием ограничения
$\vec d \geq 0$. Число алгоритмов в~$h$-мерной унимодальной сетке с~ветвями длины $D$ составляет
$(2 D + 1)^h$.
\emph{Укороченной} $h$-мерной унимодальной сеткой $\tilde{A}$ назовем множество первых $D$ слоев из~$A$:
\[\tilde{A} = \{a_{\vec d} \in A \colon n(a_{\vec d}, \XX) \leq m + D\}.\]

\begin{figure}[t]
    \label{fig:UnimodalSets}
    \begin {multicols}{2}
    \centering
    \hfill
    \IncludeHalfPicture{netpics_U.eps}
    \hfill
    \medskip
    \hfill
    \IncludeHalfPicture{netpics_US.eps}
    \hfill
    \end {multicols}
    \caption{Матрица ошибок унимодальной сетки (слева) и~укороченной унимодальной сетки (справа)
    при $D=10$, $h=2$, $m=5$, $L=60$.}
\end{figure}

Формула для вероятности переобучения \emph{пессимистического} метода минимизации эмпирического риска
на укороченных унимодальных сетках так же была получена в~\cite{botov09mmro}.
Ниже рассматриваются неукороченные унимодальные сетки и
случай \emph{рандомизированного} метода минимизации эмпирического риска.

\begin{lemm}
Группа симметрии унимодальной сетки размерности $h$~содержит в~качестве подгруппы группу $\Sym(A) = (S_2)^h \times S_h$.
Группа $S_h$ действует на~множестве пар $\left(X_j, Y_j\right)_{j=1}^h$
всеми возможными перестановками;
$j$"~тая группа $S_2$ переставляет объекты множества $X_j$ и~$Y_j$ местами,
сохраняя относительный порядок объектов.
\end{lemm}

\begin{vkProof}
Все алгоритмы $h$"~мерной унимодальной сетки длины $D$
индексированы множеством вектор-индексов $\vec d \brop\in \{-D, \ldots, D\}^h$.
Тут число ошибок алгоритма $a_{\vec d} = m + |\vec d|$.

Рассмотрим алгоритм $a_{\vec d} \in A$ и~произвольную
$\pi \brop= (z_1, \ldots, z_h) \times \pi_0 \in \Sym(A)$,
где $z_j \in S_2$, $\pi_0 \in S_h$.
По данному выше определению действия $\pi$ на~$\XX$ получаем, что
$\pi a_{\vec d} = a_{ \pi \vec d}$, где действие $\pi$ на~вектор $\vec d$
определяется перестановкой его координат с~помощью $\pi_0$ и~инверсией знаков
для всех $j$, таких что $z_j \neq id$ "--- транспозиция.
Множество $\{-D, \ldots, D\}^h$ сохраняется при применении к~нему
произвольной перестановки координат $\pi \in (S_2)^h \times S_h$.
Поэтому $\forall \vec d \in \{-D, \ldots, D\}^h$ выполнено $\pi \vec d \in \{-D, \ldots, D\}^h$.
А следовательно, $a_{\pi \vec d} \in A$.
\end{vkProof}

\begin{lemm}
\label{eq:unimodalSetOrbits}
Множество орбит унимодальной сетки $A = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
под действием $\Sym(A)$ индексировано всевозможными $Y_h^D$.
Пусть $\lambda = (\lambda_1, \ldots, \lambda_h) \in Y_h^D$. Обозначим через $|S_h \lambda|$
число различных слов длины $h$, состоящих из~символов $\lambda_1, \ldots, \lambda_h$
Пусть $|\lambda > 0|$ "--- число строго положительных компонент вектора $\lambda$.

Тогда число алгоритмов в~орбите $\omega_\lambda$ равно $|S_h \lambda| \cdot 2^{|\lambda > 0|}$.
\end{lemm}

$\square$ \; \textbf{Доказательство} полностью повторяет рассуждения леммы \ref{eq:monotonicSetOrbits}.
Множитель $2^{|\lambda > 0|}$ соответствует возможности сменить знак у~всех
не-нулевых компонент вектора $\vec d$.
$\blacksquare$

\begin{figure}[t]
    \centering
    \includegraphics[height=72mm]{unimod_net_illustration.eps}
    \caption{Строение множества $A(X)$ для двумерной унимодальной сетки.}
    \label{fig_grjklghl}
\end{figure}

\begin{theorem}
\label{th:unimodalNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к~унимодальной сетке $A = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
дается выражением:
\begin{equation}
\label{vmvmricdkd}
\begin{aligned}
    Q_\varepsilon(A) & =
                         \sum_{\vec d \in Y_h^D}
                         \sum_{\substack{\vec  t   \geq \vec d, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \frac {|S_h \vec d\,| \cdot 2^{|\vec d > 0|}}
                            {T(\vec t +  \vec {t'})}
                         \frac{\Binom{L'}{\ell'\!}}{\CLl}
                         H_{L'}^{\ell', m}(s_0),
\end{aligned}
\end{equation}
где
$\ell' = \ell - \sum \nolimits_{j = 1}^h \left( [t_j \neq D] + [t'_j \neq D] \right)$,
$k' = k - |\vec t| - |\vec t'|$,
а~остальные обозначения совпадают с~обозначениями теоремы~\ref{fdhfemndawd}.
\end{theorem}

\begin{vkProof}
Напомним, что через $\fA = \bigl\{ A(X) \colon X \in \XXell \bigr\}$
обозначалось множество подмножеств алгоритмов, получающихся в~результате обучения.
Пусть $A$ "--- унимодальная сеть.
Тогда для произвольной обучающей выборки $X$ множество $A(X)$ устроено специфическим образом.
На~рис.\,\ref{fig_grjklghl} показано, что в $A(X)$ всегда найдется такая пара алгоритмов $(a_{\vec t_1},\,a_{\vec t_2})$
что \[A(X) = {\{a_{\vec d} \,|\, \vec t_1 \leq \vec d \leq \vec t_2\}}.\]
Следовательно, $\fA = \{\alpha_{\vec t, \vec t'} \, \colon \, \vec t, \vec t' \in [0, \dots, D]^h \}$.

Обозначим через $J(\vec t)$ "--- множество тех индексов $j \in \{1, \dots, h\}$, для которых $t_j < D$.
Положим
\[
\begin{aligned}
    X_{\vec t}  = \bigcup\limits_{j \in J(\vec t)} x^{t_j + 1}_j,
    \quad
    X'_{\vec t} = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{t_j} x^i_j ;
    \\
    Y_{\vec t'} = \bigcup\limits_{j \in J(\vec t')} y^{t'_j + 1}_j ,
    \quad
    Y'_{\vec t'} = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{t'_j} y^i_j.
\end{aligned}
\]
Множества $X_{\vec t} \cup Y_{\vec t'}$ и $X'_{\vec t} \cup Y'_{\vec t'}$
являются соответственно порождающим и запрещающим множеством для $\alpha_{\vec t, \vec t'}$.
Применив теорему о порождающих и запрещающих множествах, получим формулу~\eqref{vmvmricdkd}.
\end{vkProof}

\subsection{Разреженные монотонные и унимодальные сети}
В~предыдущих параграфах рассматривались семейства алгоритмов, реализующихся при непрерывном изменении компонент вектора вещественных параметров.
На~практике возможны ситуации, при которых наблюдаемое семейство будет собственным подмножеством рассмотренных выше монотонных и~унимодальных сетей.
В~данном параграфе рассматриваются только такие подмножества, в~которых наложено ограничение на минимальное расстояние между ближайшими алгоритмами в семействе.
Такие случаи соответствует изменению каждой компоненты вектора вещественных параметров с~постоянным шагом.

%\REVIEWNOTE{Саша. Все предыдущие параграфы красиво объясняли аналогию с~реальными семействами. Тут бы тоже какое-то вводное слово не помешало.}

\begin{df}
Пусть $\rho \in \NN$ "--- целочисленный параметр;
${A = \{a_{\vec d}\}}$ "--- $h$"~мерная монотонная сетка длины $\rho D$;
$m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной монотонной сеткой $\ddot{A}$ плотности~$\rho$ и~длины $D$}
будем называть подмножество $A$, заданное условием:
\[
\begin{aligned}
    \ddot{A} =
        %\left\{a_{\vec d} \in A \, \big| \, \forall j = 1, \ldots, h \; \exists d'_j \in \ZZ \colon d_j = \rho d'_j \right\}.
        \bigl\{a_{\vec d} \in A \, \big| \, \vec d \in \left( \tiny \rho \ZZ \tiny \right)^h \bigr\}.
\end{aligned}
\]
\end{df}

Отметим, что при $\rho > 1$ граф смежности разреженной монотонной сетки состоит из~изолированных точек.

\begin{eq}
На~рисунке \ref{fig:sparseMonotonicNetExample}
выделено подмножество двумерной монотонной сетки с~параметром $D = 8$,
соответствующее разреженной монотонной сетке с~параметрами $\rho = 2$, $D = 4$.
\begin{figure}[h]
    \label{fig:sparseMonotonicNetExample}
    \begin{centering}
    \includegraphics[height=64mm]{monot_net_rarefield.eps}
    \caption{Двумерная разреженная монотонная сетка при $\rho = 2$, $D = 4$.}
    \end{centering}
\end{figure}
\end{eq}

\begin{df}
Пусть~$\rho \in \NN$ "--- целочисленный параметр;
$A=\{a_{\vec d}\}$, где $\|d\| \leq \rho D$ "--- $h$"~мерная унимодальная сетка;
$m \brop\equiv n(a_0, \XX)$.
\emph{Разреженной $h$"~мерной унимодальной сеткой $\ddot{A}$ плотности~$\rho$}
будем называть следующее подмножество $A$:
\[
\begin{aligned}
    \ddot{A} =
        \bigl\{a_{\vec d} \in A \, \big| \, \vec d \in \left( \tiny \rho \ZZ \tiny \right)^h \bigr\}.
\end{aligned}
\]
\end{df}

\begin{theorem}
\label{th:monotonicNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к разреженной монотонной сетке $\ddot{A}_M\! =\! \{ a_{\vec d} \}$ размерности $h$, ${\|d\| \!\leq\! D}$,
дается выражением:
\begin{equation}
\label{eq:monotonicNetFormula}
    Q_\varepsilon(\ddot{A}_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\end{equation}
где
$Y_h^D$ "--- множество целочисленных неотрицательных невозрастающих последовательностей
длины $h$ и не превосходящих $D$,
$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на $\lambda$,
$T(\vec {t}) = \prod_j (t_j + 1)$,\:
$\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,\:
$k' = k - |\vec t|$, $L' = \ell' + k'$,\:
$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,\:
$H_{L'}^{\ell\,', m}(s)$ "--- функция гипергеометрического распределения.
\end{theorem}

\begin{vkProof}
%\REVIEWNOTE{ilia:Разберись с лэйблом. alfrey:разобрался.}
Воспользуемся теоремой \ref{th:QAorbit} о разложении вероятность переобучения по орбитам множества алгоритмов:
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\Xl \in \XXell}
                         \frac {[a_\lambda \in A_M(\Xl)]} {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 1. }Зафиксируем $\Xl \in \XXell$.
Обозначим через $t_j$ максимальный индекс из $\{0, \dots, \rho D\}$, при котором все объекты
$\{x_j^1, \dots, x_j^{t_j}\}$ содержатся в $\Xk$, а $x_j^{t_j + 1}$\!\!, при его наличии, лежит в~$\Xl$.
Положим $\vec t = \{t_j\}_{j = 1}^h$.
Тогда условие $a_\lambda \in A_M(X)$ перепишется  как $\vec t \geq \rho \lambda$.

Действительно, заметим, что для всех $a \in A_M$ и $\Xl \in \XXell$ выполнено $n(a, \Xl) \geq n(a_0, \Xl)$.
Следовательно, алгоритм $a_\lambda$ может быть выбран, только если объекты
$x_j^i$ при всех $j = 1, \dots, h$ и $i \leq \rho \lambda_j$ лежат в контроле.
В терминах $\vec t$ это записывается как $\vec t \geq \rho \lambda$.

Обозначим множество разбиений на обучение и контроль с~фиксированным значением параметра $\vec t$
через $\XXell_{\vec t}$. Тогда
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_h^D} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 2. } Пусть $\Xl \in \XXell_{\vec t}$.
Заметим, что алгоритм $a_{\vec d} \in A_M(\Xl)$ тогда и только тогда, когда $\rho \vec d \leq \vec t$.
Следовательно, \hfill$|A_M(\Xl)| ={}$\\ \hfill ${}\!=\!(\lfloor t_1 / \rho \rfloor \!+\! 1) (\lfloor t_2 / \rho \rfloor \!+\! 1) \dots (\lfloor t_h / \rho \rfloor \!+\! 1)$. Обозначим~${T(\vec v) = \prod_j (v_j\! +\! 1)}$. Тогда $|A(\Xl)| = T\bigl(\lfloor \vec t / \rho \rfloor\bigr)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из~$U_1$, лежащих в обучении.
Тогда $\delta (a_\lambda, \Xl) = \frac{m - s + \rho |\lambda|}{k} - \frac{s}{\ell}$, и условие
$\delta (a_\lambda, \Xl) \geq \epsilon$ запишется в виде $s \leq \frac \ell L [m + \rho |\lambda| - \epsilon k] \equiv s_0$.
Множество всех разбиений из $\XXell_{\vec t}$ с фиксированным параметром $s$ обозначим
через $\XXell_{\vec t, s}$. Тогда
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_h^D} |S_h \lambda|
                         \!\!\sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \!\!\frac 1 {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         {\sum_{s = 0}^{s_0} \bigl|\XXell_{\vec t, s}\bigr|}.
\]

\textbf{Шаг 4. } Вычислим мощность множества $\XXell_{\vec t, s}$.

Введем обозначения $\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,
$k' = k - |\vec t|$, $L'\brop = \ell' + k'$. Тогда простое комбинаторное вычисление показывает, что
$\bigl|\XXell_{\vec t, s}\bigr| = C_m^s C_{L' - m}^{k' - s}$.
Следовательно,
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_h^D} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac 1 {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         {\sum_{s = 0}^{s_0} C_m^s C_{L' - m}^{k' - s}}.
\]
Напомним, что
$H_{L'}^{\ell\,', m}(z) = \sum \limits_{s=0}^{\lfloor z \rfloor} \frac{C_m^{\,s} C_{L'-m}^{\,\ell' - s}}{C_{L'}^{\,\ell'}}$
"--- функция гипергеометрического распределения \cite{voron09mmro}. Тогда
\[
    Q_\varepsilon(A_M) = \sum_{\lambda \in Y_h^D}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell\,', m}(s_0).
\]
\end{vkProof}

\begin{theorem}
\label{th:unimodalNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к~разреженной унимодальной сетке $\ddot{A} = \{ a_{\vec d} \}$ размерности $h$, ${\|d\| \!\leq \!D}$,
дается выражением:
\begin{equation}
\label{eq:unimodalNetFormula}
\begin{aligned}
    Q_\varepsilon( A) & =
                         \sum_{\lambda \in Y_h^D}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\,\,\|\vec {t'}\| \leq \rho D}}
                         \!\!\!\mathbb{S}(\lambda, \vec t, \vec {t'}); \\
                          \mathbb{S}(\lambda, \vec t, \vec {t'})
                          & =
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                            {T\bigl(\lfloor \vec t / \rho \rfloor + \lfloor \vec {t'} / \rho \rfloor\bigr)}
                         \frac{\Binom{L'}{\ell\,'}}{\CLl}
                         H_{L'}^{\ell\,', m}(s_0),
\end{aligned}
\end{equation}
где
%$Y_h^D$ "--- множество невозрастающих последовательностей длины $h$ и~не~превосходящих $D$,
%$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на~$\lambda$,
%$T(\vec t, \vec {t'}) = \prod_j (t_j + t'_j + 1)$,
$\ell' = \ell - \sum \nolimits_{j = 1}^h \bigl( [t_j \neq \rho D] + [t'_j \neq \rho D] \bigr)$,
$k' = k - |\vec t| - |\vec t'|$,
%$L' = \ell' + k'$,
а~остальные обозначения совпадают с~обозначениями теоремы~\ref{th:monotonicNetFormula}.
%$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,
%$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{theorem}

\begin{vkProof}

\textbf{Шаг 1. }
Выберем в~качестве представителя $a_\lambda$ орбиты $\omega_\lambda$
алгоритм, не~допускающий ошибок на
множестве $Y = \bigcup_{j=1}^h Y_j$.
Этого можно добиться, взяв произвольный  $a_{\vec d} \in \omega_\lambda$ и~поменяв
знаки у~всех $d_j < 0$ c помощью транспозиции $z_j$.

Введя обозначения $\vec t$ и~$\XXell_{\vec t}$ так же,
как и~на~первом шаге вывода формулы для монотонной сетки,
получим
\[
    Q_\varepsilon(\ddot{A}) = \frac1{\CLl} \sum_{\lambda \in Y_h^D} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|\ddot{A}(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

%\REVIEWNOTE{Я бы подумал над обозначениями. Громоздкие индексы. См. дальше.}
%\REVIEWNOTE{Саша: Legacy code. Пусть будут как есть, ладно?}

\textbf{Шаг 2. }
Обозначим с~помощью $t'_j$ максимальный индекс из~$\{0, \ldots, \rho D\}$, при~ко\-тором все объекты
$\{y_j^1, \ldots, y_j^{t'_j}\}$ содержатся в~$\Xk$, а~$y_j^{t\,'_j + 1}$, при его наличии, лежит в~$\Xl$.
Положим $\vec {t'} = \{t'_j\}_{j = 1}^h$. Заметим, что вектор $\vec {t'}$ играет для набора $\{Y_j\}$ ту же
роль, что $\vec t$ для $\{X_j\}$. Обозначим через $\XXell_{\vec t, \vec {t'}}$ множество разбиений с
фиксированными параметрами $\vec t$ и~$\vec {t'}$.

Пусть $\Xl \in \XXell_{\vec t, \vec {t'}}$.
Заметим, что $[a_{\vec d} \in \ddot{A}(\Xl)] = [-\vec {t'} \leq \rho \vec d \leq \vec t].$
Следовательно, $|\ddot{A}(\Xl)| = T\bigl(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor\bigr)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из~$U_1$, лежащих в~обучении.
Пусть $s_0 \equiv \frac \ell L [m + \rho |\lambda| - \epsilon k]$.
Повторяя рассуждения аналогичного шага доказательства для разреженной монотонной сетки получим
\[
    Q_\varepsilon(\ddot{A}) = \frac1{\CLl} \sum_{\lambda \in Y_h^D} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec  t   \geq \lambda, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \!\!\frac 1 {T(\vec t, \vec {t'})}
                         {\sum_{s = 0}^{s_0} \bigl|\XXell_{\vec t, \vec {t'}, s}\bigr|}.
                \]

\textbf{Шаг 4. } Посчитаем мощность множества $\XXell_{\vec t, \vec {t'}, s}$.

Обозначим $\ell' = \ell - \sum \nolimits_{j = 1}^h \bigl( [t_j \neq \rho D] + [t'_j \neq \rho D] \bigr)$,\:
$k' = k - |\vec t| - |\vec t'|$, $L' = \ell' + k'$. Тогда
$\bigl|\XXell_{\vec t, \vec {t'}, s}\bigr| = \Binom{m}{s} \Binom{L' - m}{k' - s}$.
Воспользовавшись определением функции гипергеометрического распределения получим:
\[
    Q_\varepsilon(A) = \sum_{\lambda \in Y_h^D}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,            \\\|\vec {t'}\| \leq \rho D}}
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                               {T\bigl(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor\bigr)}
                         \frac{\Binom{L'}{\ell'}}{\CLl}
                         H_{L'}^{\ell', m}(s_0).
\]

\end{vkProof}

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \IncludeHalfPicture{rarefield_monotonic.eps}
    \hfill
    \caption{Зависимость $Q_\eps(\ddot{A})$ от~разреженности $\rho$ монотонной сетки
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1,2,3,4$.}
    \label{fig:MonotRarefieldNet}
    \medskip
    \hfill
    \IncludeHalfPicture{rarefield_unimodal_vs_monotonic.eps}
    \hfill
    \caption{Зависимости вероятности переобучения $Q_\eps(\ddot{A})$ для разреженной монотонной
    и~унимодальной сеток от~$\rho$
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1(2), 2(4)$.}
    \label{fig:UnimodRarefieldNet}
    \end {multicols}
\end{figure}

Приведем результаты численных расчетов,
иллюстрирующих поведение вероятности переобучения
монотонной и~унимодальной разреженных сеток.
Расчеты выполнены с~помощью доказанных выше формул
\eqref{eq:monotonicNetFormula}, \eqref{eq:unimodalNetFormula}.

На~рис.\,\ref{fig:MonotRarefieldNet} изображена зависимость
вероятности переобучения $h$"~мерной монотонной сетки при $h\!=\!1, 2, 3, 4$ от~разреженности~$\rho$.
При~увеличении размерности вероятности переобучения также возрастает.
При~увеличении разреженности $\rho$ вероятность переобучения падает, и~вскоре выходит на~константу,
соответствующую вероятности переобучения лучшего алгоритма семейства $a_0$.
Это связано с~тем, что с~уменьшением плотности семейства
возрастает роль \emph{явления расслоения} \cite{voron09dan, voron09mmro}.

На~рис.\,\ref{fig:UnimodRarefieldNet} приведены результаты сравнения разреженных
$h$"~мерных унимодальных сеток
с~разреженными $2h$-мерными монотонными сетками при~$h=1$ и~$h=2$.
Тонкая серая кривая соответствует вероятности переобучения для~унимодальной сетки.
Полученные результаты подтверждают гипотезу \cite{botov09mmro} о~связи вероятности переобучения для~унимодальных
сеток с~вероятностью переобучения монотонных сеток удвоенной размерности.

\subsection{Один слой хэммингова шара}

Напомним, что расстояние между алгоритмами $\rho(a, a')$
определялось как расстояние Хэмминга между их~векторами ошибок:
\[\rho(a, a') = \sum \limits_{x \in \XX} |I(a, x) - I(a', x)|.\]

\begin{df}
Шаром алгоритмов $B_{r}(a_0)$ радиуса $r$ назовем множество, заданное условием
$B_{r}(a_0) = \{a \in \AA \colon \rho(a, a_0) \leq r\}$.
Алгоритм $a_0$ будем называть \emph{центром шара}.
\end{df}

В~отличие от~рассмотренных в~предыдущих разделах модельных семейств алгоритмов, хеммингов шар алгоритмов не~имеет явного аналога среди реальных семейств алгоритмов.
Однако, изучение этого модельного семейства, а также различных его подмножеств, представляет значительный теоретический интерес, поскольку хэммингов шар является максимально связным множеством булевых векторов.
Одновременно с~этим он состоит из~огромного числа алгоритмов, значительно расслоеных по~числу ошибок.
Это делает хэммингов шар привлекательным примером для~изучения влияния эффектов сходства и расслоения на~вероятность переобучения.

Шары алгоритмов были подробно изучены в~\cite{tolstikhin10diplom}.
В~частности, были получены точные формулы вероятности переобучения
для~рандомизированного метода минимизации эмпирического риска.
При этом, поскольку эти модельные семейства обладают определенными симметриями, использовалась рассмотренная выше техника,
основанная на~действии группы симметрии $\Sym(A)$ на~множестве алгоритмов
и~разбиений выборки.

%\REVIEWNOTE{В предыдущем абзаце нехватает объяснения: почему же мы изучаем шары и их слои.}

Ниже мы рассматриваем следующее множество алгоритмов:
\[
    B_r^m(a_0) = \{a \in \AA \colon n(a, \XX) = m, \text { и~} \rho(a, a_0) \leq r\}.
\]
Данное множество получается сечением шара $B_r(a_0)$ слоем алгоритмов с~$m$ ошибками.
Следовательно, можно рассматривать его как пример наиболее <<плотного>> расположения алгоритмов внутри слоя
с фиксированным числом ошибок.

Ограничимся изучением случая $n(a_0, \XX) = m$.
Тогда количество алгоритмов в~$B_r^m(a_0)$
дается выражением $\sum \nolimits_{i = 0}^{\lfloor r / 2 \rfloor} \Binom{m}{i} \Binom{L-m}{i}$.
Данная величина чрезвычайно быстро растет с~увеличением радиуса $r$.
Мы покажем, что несмотря на~стремительное увеличение числа алгоритмов
рост вероятности переобучения оказывается весьма незначительным.

Положим без ограничения общности, что алгоритм~$a_0$ ошибается на~первых $m$~объектах генеральной выборки~$\mathbb{X}$.
В~дальнейшем множество, состоящее из~первых~$m$~объектов $\mathbb{X}$, мы будем обозначать~$X^m$.
Множество, состоящее из~последних~$L-m$ объектов мы будем обозначать~$X^{L-m}$.
Для~удобства $m$"~й слой алгоритмов "--- множество, состоящее из~всех алгоритмов, допускающих ровно~$m$ ошибок на генеральной выборке, "--- будем обозначать~$A_m$.

\begin{lemm}
\label{lem:ballSyms}
Группа $S_m \times S_{L-m}$, где $S_m$ и~$S_{L_m}$ "--- симметрические группы перестановок,
действующие на~множествах $X^m$ и~$X^{L-m}$ соответственно, является подгруппой группы
симметрий множества алгоритмов $B_{r}^m(a_0)$.
\end{lemm}

\begin{vkProof}
    Очевидно, что для~$\pi\in S_m\times S_{L-m}$ справедливы следующие равенства:
    ${n(a,X^m)=n\bigl(\pi(a),X^m\bigr)}$ и ${n(a,X^{L-m})=n\bigl(\pi(a),X^{L-m}\bigr)}$.
    Отсюда получаем:
    \begin{multline*}
    \rho(a,a_0)=m-n(a,X^m)+n(a,X^{L-m})={}\\
    {}=m - n\bigl(\pi(a),X^m\bigr) + n\bigl(\pi(a),X^{L-m}\bigr) = \rho\bigl(\pi(a),a_0\bigr).
    \end{multline*}
    Таким образом, элементы группы $S_m\times S_{L-m}$ не~меняют расстояния до~центра шара $B_{r_0}(a_0)$.

    Также заметим, что действие элементов симметрической группы~$S_L$ не~меняет числа ошибок алгоритмов.
    Поскольку симметрическая группа действует на~алгоритмы инъективно (у~каждой перестановки~$\pi$ есть обратная ей), приходим к~выводу, что $B_{r}^m(a_0)\brop=\pi\bigl(B_{r}^m(a_0)\bigr)$.
\end{vkProof}

\begin{lemm}
\label{lem:ballOrbitSize}
Орбиты $\tau \in \Omega(\XXell)$ действия группы $S_m \times S_{L-m}$ на~множестве $\XXell$ индексированы параметром $i = |\Xl \cap X^m| \brop= n(a_0, X^\ell)$ "--- числом ошибок алгоритма $a_0$ на обучении. Мощность орбиты $\tau_i$ записывается в~виде $|\tau_i| = \CLl h_L^{\ell, m}(i)$.
\end{lemm}

\begin{vkProof}
Первое утверждение леммы непосредственно следует из~строения подгруппы симметрий,
отмеченного в~\ref{lem:ballSyms}.

Мощность орбиты $\tau_i$ определяется числом способов независимо выбрать $i$ объектов из~$X^m$ и
$\ell - i$ объектов из~$X^{L-m}$. Таким образом $|\tau_i| = \CLl h_L^{\ell, m}(i)$.
\end{vkProof}

\begin{theorem}
Вероятность переобучения множества алгоритмов, получаемого сечением
шара алгоритмов центральным $m$-слоем, дается в~виде
\[
    Q_\mu(\epsilon, A)
    =
    H_L^{\ell, m}\bigl(s_d(\epsilon) + \big\lfloor r/2 \big\rfloor\bigr),
\]
где $s_d(\epsilon) = \frac \ell L (m - \epsilon k)$,
$H_{L}^{\ell, m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{theorem}

%\REVIEWNOTE{Нужно ли тут условие $r \leq min(m, L - m)$? alfrey: нет, не нужно!}

\begin{vkProof}
Заметим, что утверждение лемм \ref{lem:ballSyms} и~\ref{lem:ballOrbitSize} верно и~для
сечения шара центральной плоскостью.
Поскольку все алгоритмы имеют равное число ошибок на полной выборке, применим следствие \ref{crl:easyFormula}
из теоремы о разложении вероятности переобучения по орбитам разбиений выборки:
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{m}
    h_L^{\ell, m}(i) \left[\min_{a \in A} n(a, X_i) \leq \frac \ell L(m - \epsilon k)\right].
\]

Напомним, что по~определению $i = |\Xl \cap X^m|$. Пусть $r' = \big\lfloor \frac r 2 \big\rfloor$.
Тогда выполнено следующее утверждение:
\[
    \min_{a \in A} n(a, X_i) =
    \begin{cases}
        & 0, \text{ при } i \leq r', \\
        & i - r', \text{ при } i > r'.
    \end{cases}
\]
Следовательно
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{\lfloor s_d(\epsilon)\rfloor + r'}
    h_L^{\ell, m}(i) = H_L^{\ell, m}\bigl(s_d(\epsilon) + \big\lfloor r/2 \big\rfloor\bigr),
\]
где $s_d(\epsilon) = \frac \ell L (m - \epsilon k)$.
\end{vkProof}

\begin{figure}[bt]
        \centering
        \includegraphics[width=100mm,height=70mm]{pic4.eps} %BallVsLayer1
        \caption{Зависимость вероятности переобучения $Q_{\varepsilon}$ и $\log_{10}|A|$ для~слоя шара от~радиуса шара~$r$, при $\ell=k=100$,\; $m=10,$\; $\varepsilon=0.05$.}
        \label{TBallVsLayer}
	%\REVIEWNOTE{Из подписи к рисунку \ref{fig3} не понятно, о каком семействе идет речь --- о слое шара или о всём шаре.}
\end{figure}

На рис.\,\ref{TBallVsLayer} представлена зависимость точной оценки вероятности переобучения слоя шара, а~также числа алгоритмов в~семействе, от~радиуса шара~$r$.
Видно, что за~счёт значительной <<плотности>> данного семейства вероятность переобучения может оставаться на~приемлемо низком уровне при мощности семейства порядка тысяч и~относительно небольшой длине выборки $\ell=k=100$.
Заметим, что VC"~оценки в~этой ситуации вырождены и~существенно превышают единицу.
\subsection{Хэммингов шар}
В~прошлом разделе был рассмотрен пример наиболее <<плотного>> множества, наделенного свойством связности, но не~обладающего свойством расслоения.
Теперь мы изучим совместное влияние сходства и расслоения на вероятность переобучения.
Хэммингов шар "--- наиболее подходящее для~этого модельное семейство.
Это пример наиболее <<плотного>> из~одновременно связных и расслоенных семейств алгоритмов.

%\REVIEWNOTE{В применении к семейству алгоритмов мы используем термины "компактное", "плотное", "связное", "расслоенное", "разреженное".
%По-моему "компактное" и "плотное" это синонимы (поправь меня если это не так). Давай использовать один из них.
%Согласен ли ты что "Связное" и "Разреженное" --- это антонимы? Для меня целью изучения \emph{разреженных} сеток было понять, как
%уменьшение связности влияет на вероятность переобучения. Разреженность можно определить численно: напимер в качестве игрушечного определения
%можно взять минимальное расстояние между парой алгоритмов: $sparse(A) = \min_{a, a' \in A} \rho(a, a')$.
%Есть ли у тебя какое-нибудь определение (пусть даже и очень игрушечное) для плотности?}

Пусть семейство алгоритмов~$A$ представляет собой хэммингов шар $B_{r_0}(a_0)$, где~$n(a_0,\mathbb{X})=m$.
Пусть, без ограничения общности, алгоритм~$a_0$ допускает ошибки на~первых~$m$ объектах генеральной выборки $\mathbb{X}$.

Получим точную оценку вероятности переобучения для~хэммингова шара алгоритмов.

Множество,~состоящее из первых $m$ объектов генеральной выборки $\mathbb{X}$, будем обозначать $X^m$, а
множество, состоящее из последних $L-m$ объектов, будем обозначать $X^{L-m}$.

%\REVIEWNOTE{Предлагаю тебе взять на себя ответственность ("take an ownership") за предыдущий параграф --- слой хэммингова шара.
%Очевидно что нужно переместить туда определения $X_m$, $X_{L-m}$, доказательства леммы \ref{leTolstBallSubGroup}, и т.д.
%Нужно как-то аккуратно объяснить что слой шара и шар --- это разные семейства, но группы симметрии и строение орбит у них похожи.
%Мне нравится что вначале мы рассматриваем слой шара (потому что он проще).}

\begin{lemm}\label{leTolstBallSubGroup}
    Группа $S_m\times S_{L-m}$, где $S_m$ и $S_{L-m}$ "--- симметрические группы перестановок, действующих на~множествах $X^m$ и~$X^{L-m}$ соответственно, является подгруппой группы симметрии семейства алгоритмов~$A$.
\end{lemm}
\begin{vkProof}
    Очевидно, что для~$\pi\in S_m\times S_{L-m}$ справедливы следующие равенства:
    ${n(a,X^m)=n\bigl(\pi(a),X^m\bigr)}$ и ${n(a,X^{L-m})=n\bigl(\pi(a),X^{L-m}\bigr)}$.
    Отсюда получаем:
    \begin{multline*}
    \rho(a,a_0)=m-n(a,X^m)+n(a,X^{L-m})={}\\
    {}=m - n\bigl(\pi(a),X^m\bigr) + n\bigl(\pi(a),X^{L-m}\bigr) = \rho\bigl(\pi(a),a_0\bigr).
    \end{multline*}
    Таким образом, элементы группы $S_m\times S_{L-m}$ не~меняют расстояния до~центра шара $B_{r_0}(a_0)$.

    Тогда, если $a\in B_{r_0}(a_0)$, то и $\pi(a)\in B_{r_0}(a_0)$.
    Поскольку действие элементов симметрической группы на~алгоритмы инъективно, приходим к~выводу, что $B_{r_0}(a_0)\brop=\pi\bigl(B_{r_0}(a_0)\bigr)$.
\end{vkProof}

%\REVIEWNOTE{Поправь нижние индексы у алгоритма: если всмотреться, $a_0$ (а-ноль) и $a_0$ (а-буква-"о") выглядят по-разному.}

%\REVIEWNOTE{Термин "инъективность" --- это когда для разные алгоритмов $a_1 \neq a_2$ образы тоже различаются: $\pi(a_1) \neq \pi_(a_2)$. Правильно?
%Если так --- предлагаю написать почему действие группы инъективно. А именно, потому что у каждого $\pi$ есть обратный.}
%\REVIEWNOTE{Мне кажется,~это понятно само собой...}


\begin{lemm}\label{leTolstBallOrbA}
    Орбитами действия группы $S_m\times S_{L-m}$ на~множестве алгоритмов~$A$ являются пересечения слоев $m-r_0,\ldots,m+r_0$ со~сферами радиусов $0,1,\ldots,r_0$ и~центрами в~алгоритме~$a_0$.
\end{lemm}
\begin{vkProof}
    Пусть алгоритм $a\in A_p$: $d(a,a_0)=r_1$.
    Мы установили, что в этом случае $d\bigl(\pi(a),a_0\bigr)=d(a,a_0)=r_1$.
    Также мы знаем, что действие перестановки на~алгоритм не меняет числа его ошибок на $\mathbb{X}$.
    Таким образом, $\pi(a)$ также принадлежит пересечению $p$"~го слоя со сферой радиуса $r_1$.

    Осталось показать, что для любых $a_1,a_2 \in A_p$ таких, что $d(a_1,a_0)=d(a_2,a_0)\brop=r_1$, найдётся перестановка $\pi\in S_m\times S_{L-m}$, при которой $\pi(a_1)=a_2$.
    Но это следует из~того, что $n\left(a_1,X^m\right)\brop=n\left(a_2,X^m\right)$ и $n\left(a_1,X^{L-m}\right)=n\left(a_2,X^{L-m}\right)$.
    Последний факт легко установить, выразив число ошибок, допускаемых алгоритмами~$a_1$ и $a_2$ на~множестве~$X^m$, через $m$ и $r_1$.
\end{vkProof}

\begin{figure}[h]
    \centering
    \includegraphics[width=88mm]{BScheme}
    \caption{Алгоритмы из орбит семейства $A$.}
    \label{fig4}
\end{figure}

На~рис.\,\ref{fig4} для~наглядности представлено по одному алгоритму из каждой орбиты семейства~$A$.
Пронумеруем орбиты двумя целочисленными индексами следующим образом: алгоритмы из~орбиты~$\mathop{\rm Orb}(r,n)$,\; $r=0,\ldots, r_0$,\; $n=0,\ldots, r$, принадлежат пересече\-нию сферы радиуса~$r$ с~центром в~$a_0$ со~слоем~$r+m-2n$.
Обратим внимание на~то, что алгоритм $a_{(r,n)}$ из~орбиты~$\mathop{\rm Orb}(r,n)$
имеет ${m-n}$ единиц и $n$ нулей на~множестве~$X^m$ и
${r-n}$ единиц, ${L-m-r+n}$ нулей на~множестве~$X^{L-m}$.
Всего в~орбите $\mathop{\rm Orb}(r,n)$ содержится $C^n_m C^{r-n}_{L-m}$ алгоритмов.

В~дальнейшем особую роль будет играть орбита~$\mathrm{Orb}(r_0,r_0)$ "--- в~неё входят алгоритмы шара~$A$, допускающие наименьшее число ошибок на~генеральной выборке~$\mathbb{X}$.
Вопреки закономерной аналогии с~шаром в~$\mathbb{R}^3$, для~которого пересечение с~касательной состоит из одной точки, нижний слой хэммингова шара состоит из~$C^{r_0}_m$~алгоритмов.
%\REVIEWNOTE{Особую роль в дальнейшем будет играть $\mathop{\rm Orb}(r_0, r_0)$. Предлагаю дать ей некоторое описание --- например,
%"это множество, состоящее из алгоритмов шара с наименьшим количеством ошибок на полной выборке". Еще было бы интересно узнать, сколько
%таких алгоритмов "--- возможно, некоторые читатели рисуют себе шар как шар в $R^3$, у которого любой "крайний случай" (например, самая низкая горизонтальная
%касательная плоскость) имеет одну точку пересечения. Нужно разрушать эти иллюзии :)}

\begin{lemm}
\label{leTolstBallCr1}
    Для~любой обучающей выборки~$X$ и любого алгоритма~$a \in A \setminus \mathop{\rm Orb}(r_0,r_0)$ выполнено:
    \[
        a \in A(X) \Leftrightarrow n(a,X)=0.
    \]
\end{lemm}

%\REVIEWNOTE{Не хватает текстовой расшифровки утверждения леммы. Например: алгоритм не из $Orb(r_0, r_0)$ может лежать в $A(X)$ только если он корректен (не допускает ошибок) на обучении.}

\begin{vkProof}
    Достаточность очевидна.
    Докажем необходимость.

    Введём обозначения:
    $X^{\ell_1} = |X^m\cap X|$,\;
    $X^{\ell_2} = |X^{L-m} \cap X|$,\;
    $\ell_1=|X^{\ell_1}|$,\;
    $\ell_2=|X^{\ell_2}|$,\;
    $\ell=|X|$,\;
    $\ell=\ell_1+\ell_2$,\;
    $X= X^{\ell_1}\cup X^{\ell_2}$.

    Пусть $a\in A(X)$ и $a$ принадлежит орбите $\mathop{\rm Orb}(r,n)$, отличной от $\mathop{\rm Orb}(r_0,r_0)$.
    Докажем, что алгоритм $a$ не допускает ошибок на~обучающей выборке $X$.

    Начнем с рассмотрения случая $\ell_1\leqslant r_0$.
    Алгоритмы орбиты $\mathop{\rm Orb}(\ell_1,\ell_1)$ имеют ровно $\ell_1$~нулей на~множестве~$X^m$ и не допускают ни одной ошибки на~множестве~$X^{L-m}$.
    Очевидно, существует алгоритм ${a^{\ell_1}\in \mathop{\rm Orb}(\ell_1,\ell_1)}$, такой что $n\left(a^{\ell_1},X\right)=0$.

    В~случае $\ell_1>r_0$ в~орбите $\mathop{\rm Orb}(r_0,r_0)$~существует алгоритм $a^{r_0}$ для которого $n\left(a^{r_0},X^{L-m}\right)=0$ и $n\left(a^{r_0},X^m\right)=\ell_1-r_0$.
    Поскольку $n\left(a,X^m\right)\geqslant \ell_1-n\geqslant \ell_1-r_0=n\left(a^{r_0},X^m\right)$, то мы приходим к~противоречию с тем, что $a\in A(X)$.
\end{vkProof}
Таким образом, алгоритм не~из~$\mathrm{Orb}(r_0, r_0)$ может лежать в~$A(X)$ только если он не~допускает ошибок на~обучающей выборке.

\begin{cor}\label{CorTolstBall1}
    В~ходе доказательства леммы~\ref{leTolstBallCr1} также \mbox{установлено}, что при ${|X\cap X^{m}|>r_0}$ алгоритмы из~орбит, отличных от~$\mathop{\rm Orb}(r_0,r_0)$, не могут попасть во~множество~$A(X)$.
\end{cor}
\begin{lemm}\label{leTolstBallCr2}
    Для~любого алгоритма~$a \in \mathop{\rm Orb}(r_0,r_0)$ выполнено:

    если выборка $X \in \XXell$ такова, что $|X\cap X^m| \leqslant r_0$, то:
    \[
        a \in A(X) \Leftrightarrow n(a,X)=0;
    \]

    если $X$ такова, что $|X\cap X^m| > r_0$, то:
    \[
        a \in A(X) \Leftrightarrow
        n\left(a,X\cap X^{m}\right)=|X\cap X^m| - r_0.
    \]
\end{lemm}
\begin{vkProof}
    Случай $|X\cap X^m|\leqslant r_0$ повторяет первую часть доказательства леммы \ref{leTolstBallCr1}.
    Рассмотрим случай $|X\cap X^m|>r_0$.

    Поскольку в~этом случае ни один алгоритм из~множества~$A$ не~допускает меньше $|X\cap X^m|-r_0$~ошибок на обучающей выборке $X$,
    а алгоритмы из $\mathop{\rm Orb}(r_0,r_0)$ не ошибаются на~объектах множества $X^{L-m}$, то достаточность очевидна.
    Необходимость вытекает из того факта, что существует алгоритм~$a\in \mathop{\rm Orb}(r_0,r_0)$, такой что $n(a,X)=n(a,X^m)=|X\cap X^m|-r_0$.
\end{vkProof}
\begin{lemm}\label{leTolstOrbX}
    Орбитами действия группы $S_m\times S_{L-m}$ на~множестве разбиений являются множества  $\left\{ X: |X\cap X^{m}|=i_0\right\}$, где $i_0=\max(0,m-k),\ldots,\min(l,m)$ .
\end{lemm}
\begin{vkProof}
    Утверждение леммы очевидным образом следует из~определения действия группы~$S_m\times S_{L-m}$ на~множество разбиений.
\end{vkProof}
%Теорема - точная оценка для шара
\begin{theorem}[Точная оценка для хэмингова шара]\label{Ball}
    Пусть $n(a_0,\mathbb{X})=m$.
    Рассмотрим шар алгоритмов $B_{r_0}(a_0)$.
    Тогда при~обучении рандомизированным методом минимизации эмпирического риска и $r\leqslant\min\left(m,L-m\right)$ вероятность переобучения может быть записана в~следующем виде:
    \[
        Q_{\varepsilon}(A)
        =\!\!
        \sum_{\substack{i=m-k;\\i\geqslant 0}}^{r_0} \!\!\! h^{\ell,m}_L(i) \:
        \frac{
                \mathop{\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r}}
                \limits_{
                            \mbox{\tiny{$m+r-2n\geqslant\varepsilon k$}}
                }
                S(n,r,i)
            }
            {\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r} S(n,r,i)}
        +
        \sum_{i=r_0+1}^{\lfloor s(\varepsilon)\rfloor}h^{\ell,m}_L(i),
    \]
    где
    $S\left(n,r,i\right)=C^{n-i}_{m-i} C^{r-n}_{k-m+i}$,\:
    $s(\varepsilon)=\frac{\ell}{L}(m\brop-\varepsilon k) + \frac{r_0k}{L}$,\:
    $h^{\ell,m}_L(i)\brop=\frac{C^i_m C^{\ell-i}_{L-m}}{C^{\ell}_L}$.
\end{theorem}

%\REVIEWNOTE{Запись ограничение $m + r - 2n \geq \eps k$ под двумя суммами выглядит несколько странно. Я догадался, но предлагаю текстом объяснить что это значит.
%В качестве альтернативы предложу воспользоваться воронцовскими квадратными кавычками (нотацией Айверсона)? Но, возможно, это менее эстетично --- в твоей записи формулы очень хорошо выровнены.}

%Доказательство теоремы о точной оценке для шара
\begin{vkProof}
    Доказательство основано на~применении теоремы~\ref{th:QAXorbit}, которая позволяет представить вероятность переобучения в~виде:
    \[
        Q_\eps(A)
        =
        \sum_{\omega\in\Omega(A)}
        \frac{|\omega|}{\CLl}
        \sum_{\tau\in\Omega\XXell}
        |\{X\in \tau\colon a_{\omega}\in A(X)\}|
        \frac{\bigl[\delta(a_{\omega},X_\tau) \geq \eps \bigr]}{|A(X_\tau)|},
    \]
    где $\Omega(A)$ "--- орбиты алгоритмов, а $\Omega\XXell$ "--- орбиты разбиений.

    C~учетом лемм \ref{leTolstBallSubGroup}, \ref{leTolstBallOrbA} и \ref{leTolstOrbX}:
    \[
    Q_{\varepsilon}(A)=
        \sum_{r=0}^{r_0}\sum_{n=0}^r
        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
        \sum_{\substack{i=m-k;\\i\geqslant0}}^{\min(l,m)}
        \frac{ S\left(i,r,n\right) \left[\delta\left(a_{(r,n)},X_i\right)\geqslant \varepsilon\right]}{|A(X_i)|},
    \]
    где
    $S\left(i,r,n\right) = \bigl|\{X\colon|X \cap X^m|=i,\;a_{(r,n)}\in A(X)\}\bigr|$,\:
    $a{(r,n)}$ "--- алгоритмы, представленные на~рисунке~\ref{fig4},
    а $X_i$ "--- произвольное разбиение, в~обучающую выборку~которого входят ровно $i$~объектов из~$X^m$.

%    \REVIEWNOTE{Предлагаю начать доказательства с копирования общей формулы из теоремы \ref{th:QAXorbit}.}

    Разобьем суммирование по~орбитам разбиений (по $i$) на~два слагаемых: $i \leq r_0$ и $i > r_0$.
    \begin{align*}
    Q_{\varepsilon}&(A)={}\\
    {}=
        &\sum_{r=0}^{r_0}\sum_{n=0}^r
        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
        S\left(i,r,n\right)
        \frac{\left[\delta\left(a_{(r,n)},X_i\right)\geqslant \varepsilon\right]}{|A(X_i)|}+{}\\
        {}+
        &\sum_{r=0}^{r_0}\sum_{n=0}^r
        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
        \sum_{i=r_0+1}^{\min(l,m)}
        \!\!\!S\left(i,r,n\right)
        \frac{\left[\delta\left(a_{(r,n)},X_i\right)\geqslant \varepsilon\right]}{|A(X_i)|}.
    \end{align*}

%  REVIEWNOTE{Оформить предыдущую формулу так, что бы знаки двойного суммирования из разных строчек находились на одной вертикали. Этого можно добиться окружение aligned и знаками амперсанда $\&$.}

    Из лемм \ref{leTolstBallCr1} и \ref{leTolstBallCr2}, а также из следствия \ref{CorTolstBall1} следует, что первое слагаемое соответствует случаю выбора алгоритма, не допускающего ошибок на~обучающей выборке~$X$.
    Следствие \ref{CorTolstBall1} позволяет опустить суммирование по~орбитам во~втором слагаемом, поскольку при $i>r_0$ во~множество~$A(X_i)$ попадают только алгоритмы из $\mathop{\rm Orb}(r_0,r_0)$, допускающие в соответствии с леммой \ref{leTolstBallCr2} ров\-но~$i-r_0$~ошибок на~$X$.
    С учетом этого:
    \begin{multline}
    \label{BallTemp1}
    Q_{\varepsilon}(A)={}\\
        {}=
        \sum_{r=0}^{r_0}\sum_{n=0}^r
        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
        S\left(i,r,n\right)
        \frac{\left[r+m-2n \geqslant \varepsilon k\right]}{|A(X_i)|}+{}
    \\
        {}+
        \frac{C^{r_0}_m C^{0}_{L-m}}{C^l_L}
        \sum_{i=r_0+1}^{\min(l,m)}
        \!\!\!S\left(i,r,n\right)
        \frac{\left[i\leqslant s(\varepsilon)\right]}{|A(X_i)|}.
    \end{multline}

    Вычислим значение $S\left(i,r,n\right)$.
    В случае $i\leqslant r_0$ это число способов выбрать $i$ из~$n$~объектов множества~$X^m$ и $l-i$ из~${L-m-r+n}$~объектов множества~$X^{L-m}$, на~которых не ошибается алгоритм~$a_{(r,n)}$.
    В случае $i > r_0$ "--- число способов выбрать~$i-r_0$~объектов из множества $X^m$, на~которых алгоритм~$a_{(r,n)}$~ошибается, и $l-i$ произвольных объектов из $X^{L-m}$.
    Итого:
    \[
    S\left(i,r,n\right)=
        \begin{cases}
            C^i_n C^{l-i}_{L-m-r+n}, & i\leqslant r_0;
            \\
            C^{i-r_0}_{m-r_0} C^{l-i}_{L-m}, & i>r_0.
        \end{cases}
    \]

    Найдём значения $|A(X_i)|$.
    В~случае $i>r_0$ в~$A(X_i)$~попадают те алгоритмы $\mathop{\rm Orb}(r_0,r_0)$, которые ошибаются на~множестве~$X^m$ $i-r_0$~раз.
    При~$i\leqslant r_0$ из~каждой орбиты во~множество~$A(X_i)$ попадают алгоритмы, не~ошибающиеся ни на~одном объекте множеств~$X^m$ и $X^{L-m}$.
    Получаем:
    \[
    |A(X_i)|=
        \begin{cases}
            \sum\limits_{r=0}^{r_0}\sum\limits_{n=0}^{r}C^{n-i}_{m-i}C^{r-n}_{k-m+i}, & i\leqslant r_0;
            \\
            C^{r_0}_i, & i>r_0.
        \end{cases}
    \]

    Подстановка полученных результатов в~(\ref{BallTemp1}) дает:
    \begin{multline}
    \label{BallTemp2}
    Q_{\varepsilon}(A)={}\\
    {}=
        \mathop{\sum_{r=0}^{r_0}\sum_{n=0}^r}\limits_{r+m-2n \geqslant \varepsilon k}
        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
        \frac{C^i_n C^{l-i}_{L-m-r+n}}{\sum\limits_{r_1=0}^{r_0}\sum\limits_{n_1=0}^{r_1}C^{n_1-i}_{m-i}C^{r_1-n_1}_{k-m+i}}+{}
    \\
        {}+
        \frac{C^{r_0}_m}{C^l_L}
        \sum_{i=r_0+1}^{\min(l,m)}
        C^{i-r_0}_{m-r_0} C^{l-i}_{L-m}
        \frac{\left[i\leqslant s(\varepsilon)\right]}{C^{r_0}_i}.
    \end{multline}

    Во~втором слагаемом:
    \[
        \frac{C^{i-r_0}_{m-r_0} }{C^{r_0}_i}=\frac{C^{m-i}_{m-r_0} }{C^{r_0}_i}=\frac{C^{r_0-(i-m+r_0)}_{i-(i-m+r_0)} }{C^{r_0}_i}=
        \frac{C^{i-m+r_0}_{r_0}}{C^{i-m+r_0}_i}=\frac{C^{m-i}_{r_0}}{C^{m-r_0}_i}.
    \]

    Далее, $C^{r_0}_m C^{m-i}_{r_0}=C^i_m C^{r_0-m+i}_i=C^i_m C^{m-r_0}_i$.
    Подстановка полученных формул во~второе слагаемое~(\ref{BallTemp2}) дает:
    \begin{multline}
    \label{BallTemp3}
        \frac{C^{r_0}_m}{C^l_L}
        \sum_{i=r_0+1}^{\min(l,m)}
        C^{i-r_0}_{m-r_0} C^{l-i}_{L-m}
        \frac{\left[i\leqslant s(\varepsilon)\right]}{C^{r_0}_i}={}
    \\
        {}=\sum_{i=r_0+1}^{\min(l,m)}
        h^{l,m}_L(i)
        \left[i\leqslant s(\varepsilon)\right]
        =\sum_{i=r_0+1}^{\lfloor s(\varepsilon)\rfloor}h^{l,m}_L(i).
    \end{multline}

    В~первом слагаемом $C^n_m C^i_n=C^i_m C^{n-i}_{m-i}$, а
    \[
        C^{r-n}_{L-m}C^{l-i}_{L-m-r+n} = C^{L-m-r+n}_{L-m}C^{l-i}_{L-m-r+n} \brop= C^{l-i}_{L-m}C^{r-n}_{k-m+i}.
    \]
    Заменив порядок суммирования, получим:
    \begin{multline}
    \label{BallTemp4}
        \mathop{\sum_{r=0}^{r_0}\sum_{n=0}^r}\limits_{r+m-2n \geqslant \varepsilon k}
        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
        \frac{C^i_n C^{l-i}_{L-m-r+n}}{\sum\limits_{r_1=0}^{r_0}\sum\limits_{n_1=0}^{r_1}C^{n_1-i}_{m-i}C^{r_1-n_1}_{k-m+i}}={}
    \\
        {}=
        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
        h^{l,m}_L(i)
        \frac
        {\mathop{\sum_{r=\,0}^{r_0}\sum_{n=\,0}^r}\limits_{\mbox{\tiny{$m+r-2n\geqslant\varepsilon k$}}}C^{n-i}_{m-i}C^{r-n}_{k-m+i}}
        {\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r}C^{n-i}_{m-i}C^{r-n}_{k-m+i}}.
    \end{multline}

    Подстановка (\ref{BallTemp3}) и (\ref{BallTemp4}) в (\ref{BallTemp2}) завершает доказательство.
\end{vkProof}

\begin{rem}
Впервые оценка, представленная в~прошлом разделе (оценка для~одного слоя хэммингова шара), была получена с~помощью теоремы~\ref{th:QAXorbit}.
Первый вариант ее доказательства занимал существенно больше места и был перегружен техническими деталями.
В~этом разделе приведено схожее доказательство оценки для шара, поскольку более изящного доказательства авторы не знают.
Тем не менее интуиция подсказывает, что ключ к~нему лежит в~непосредственной работе с~гипергеометрическим распределением в обход суммированию биномиальных коэффициентов.
\end{rem}

%\REVIEWNOTE{Нехватает точки после слова Замечание. Используй окружение rem (определено в файле ccaspreprint.cls)}

%\paragraph{Замечание.}
%Интересно заметить, что полученные в~двух последних разделах оценки не зависят от~центра шара~$a_0$, и~зависят лишь от~номера слоя~$m$, в~котором лежит центр шара.

\begin{figure}[t]
    \centering
    \includegraphics[width=95mm,height=55mm]{pic1.eps}%LayersProb1
    \caption{Вклады $d$"=слоёв шара в вероятность переобучения при~$\ell=k=100$,\; $m=20$,\; $r_0=10$,\; $\varepsilon=0.05$.}
    \label{fig5}
\end{figure}
На рис.\,\ref{fig5} представлены точные значения вкладов слоев шара в~его вероятность переобучения.
Видно, что несколько нижних слоев шара дают б\'ольшую часть вероятности переобучения.
Возникает вопрос: нельзя ли приближать оценку шара оценкой для~$t$ его нижних слоев.

\begin{rem}
Поясним, почему график на~рис.\,\ref{fig5} имеет вид <<гармошки>>.
Понимание эффектов сходства и расслоения позволяют выдвинуть следующую гипотезу:
{\it вероятность переобучения расслоенного и связного множества алгоритмов может быть аппроксимирована вероятностью переобучения его подмножества,
состоящего из~существенно различных алгоритмов нижних слоев.}
Из рисунка~\ref{fig4} видно, что на~внешней сфере шара представлены алгоритмы не~из~всех слоев: слои чередуются через один.
Поскольку <<существенно различные>> алгоритмы лежат именно на~внешней сфере, большие вклады в~вероятность переобучения дают те слои шара, которые дают непустое пересечение с~его внешней сферой, а именно "--- $m-r_0, m-r_0+2,\dots$.
\end{rem}
\begin{figure}[t]
    \centering
    \includegraphics[width=95mm,height=55mm]{pic2.eps}%%%FirstLayers1
    \caption{Зависимость $Q_{\varepsilon}$ от числа~$t$ нижних слоев шара, при $\ell=k=100$,\; $m=10$,\; $r_0=6$,\; $\varepsilon=0.05$.}
    \label{fig6}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=95mm,height=55mm]{pic3.eps}%%Approx1
    \caption{Зависимость $Q_{\varepsilon}$ от радиуса шара~$r$ для полного шара (верхняя кривая) и~$d$ его нижних слоёв, при $\ell=k=100$,\; $m=10$,\; $r_0=6$,\; $\varepsilon=0.05$.}
    \label{fig7}
\end{figure}
\subsection{Нижние слои хэммингова шара}
\begin{theorem}[Точная оценка для нижних слоев хэммингова шара]
\label{BallVsLayers}
    Пусть $n(a_0,\mathbb{X})=m$.
    Рассмотрим $t$ нижних слоев шара алгоритмов $B_{r_0}(a_0)$.
    Тогда при обучении рандомизированным методом минимизации эмпирического риска и $r\leqslant\min\left(m,L-m\right)$ вероятность переобучения может быть записана в виде:
    \[
        Q_{\varepsilon}(A)
        =
        \sum_{\substack{i=m-k;\\i\geqslant 0}}^{r_0}
            h^{\ell,m}_L(i) \:
        \frac
            {
                \mathop{\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r}}
                \limits_{
                \mbox{\tiny{$m+r-2n\geqslant\varepsilon k$}}
                }
                S'\left(n,r,i\right)
            }
            {\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r}S'\left(n,r,i\right)}
        +
        \sum_{i=r_0+1}^{\lfloor s(\varepsilon)\rfloor}
            h^{\ell,m}_L(i),
    \]
    где
    $h^{\ell,m}_L(i)=\frac{C^i_m C^{\ell-i}_{L-m}}{C^{\ell}_L}$,
    $S'\left(n,r,i\right)=C^{n-i}_{m-i} C^{r-n}_{k-m+i}[r+r_0+1\leqslant 2n+t]$,
    $s(\varepsilon)\brop=\frac{\ell}{L}(m-\varepsilon k) + \frac{r_0k}{L}$.
\end{theorem}

Доказательство повторяет доказательство теоремы~\ref{Ball}.
Леммы~\ref{leTolstBallSubGroup}, \ref{leTolstBallCr1} и \ref{leTolstBallCr2} остаются справедливы для~этого семейства алгоритмов.
В лемме \ref{leTolstBallOrbA} множество слоев, в~пересечении с~которыми сферы дают орбиты алгоритмов, меняется на $m-r_0,\ldots, m-r_0+t-1$.

Основное отличие в~ходе доказательства "--- при~использовании теоремы~\ref{th:QAXorbit} в~начале доказательства множество суммируемых орбит алгоритмов сокращается добавлением проверочного множителя $[r+r_0+1\leqslant 2n+t]$ после знаков суммирования по~индексам~$r$~и~$n$.

На рис.\,\ref{fig6} представлена зависимость точной оценки вероятности переобучения для~$t$ нижних слоев шара от~параметра~$t$.
Видно, что существенные скачки происходят лишь на~первых нескольких слоях.

На рис.\,\ref{fig7} представлены результаты приближения оценки шара $t$ его нижними слоями.
Черным цветом изображена оценка шара.
Красным, зеленым и синим "--- оценки 1, 2 и 3 его нижних слоев соответственно.
Падение к~нулю оценок первых слоев шара объясняется уменьшением нижнего слоя шара с~ростом его радиуса.
В определенный момент количество ошибок~$m$, допускаемое алгоритмами нижнего слоя шара, становится меньше $\varepsilon k$.
В~этом случае переобучение невозможно.

\medskip
Итак, в~последних параграфах были исследованы хэммингов шар алгоритмов, его центральный слой и сечение несколькими его нижними слоями.
На~примере центрального слоя хэммингова шара мы еще раз продемонстрировали влияние эффекта связности на~вероятность переобучения семейства.
В~то время, как вероятность переобучения множества, состоящего из~случайных алгоритмов фиксированного слоя, в~среднем экспоненциально растёт к~единице с~ростом его мощности (факт, который будет установлен в~следующих разделах), вероятность переобучения центрального слоя хэммингова шара остается на~достаточно низком уровне даже при~десятках тысяч алгоритмов в~нем.

Также на~примере всего шара и его нижних слоев удалось показать возможность приближения вероятности переобучения расслоенных семейств алгоритмов несколькими их нижними слоями.
%\REVIEWNOTE{На закуску параграфа нужен вывод: "мы изучили слой, шар, и нижние слои шара. в итоге, оказалось, что..."}

\section{Рандомизированные семейства алгоритмов}
В~предыдущем параграфе высказывалась гипотеза о~приближении вероятности переобучения множества алгоритмов
с~помощью подмножества, состоящего из небольшого количества существенно различных алгоритмов, выбранных из~нижних слоев.
Ниже рассматриваются различные способы выбора таких подмножеств, и~развивается теория позволяющая выводить соответствующие оценки вероятности переобучения.

%\REVIEWNOTE{Как насчет текста здесь? Например про гипотезу, что в.п. семейства возможно приблизить в.п. его подмножества существенно различных алгоритмов из нижних слоев "--- это я упоминал в %комментариях под картинкой"=гармошкой. А то, что дальше рассматривается "--- помогает проверять такие штуки...}

\subsection{Разреженные подмножества слоя}
%\REVIEWNOTE{Даю ответы на твои комментарии, которые ты приводил к началу следующего раздела про~мой случайный выбор без возвращений.
%Если ты не помнишь их "--- они прямо под~этими комментариями в~исходнике экранированы.
%1) По~поводу справедливости терминов <<с возвращением и без>>.
%Факт: в результате применения описанного тобой декартового произведения перестановок два алгоритма начального семейства могут принять ОДИНАКОВЫЕ векторы ошибок. Фактически "--- твое семейство уже будет состоять из~меньшего числа уникальных алгоритмов (кстати "--- как рандомизированный метод обучения реагирует на~дубли алгоритмов в~семействе? я примерно представляю, но не~стоит ли что-то об~этом сказать, раз уж ты считаешь в.п. для~мультисемейств? Опять же, картинка твоя с~совпадением экспериментов с~теорией становится не~совсем <<честной>> "--- с~учетом прошлой части препринта хочется думать, что по~оси X "--- число разных алгоритмов в~семействе...).
%Твой пример в~этом разделе демонстрирует способ <<посчитать в.п. для~разреженных семейств слоя>> (смотри мой следующий комментарий в~конце этого раздела). В~этом контексте "--- в~моем следующем параграфе целесообразно использованы слова <<с~возвращением и без>>.
%Потому что я там демонстрирую решение ТОЙ~ЖЕ~САМОЙ задачи, что в~твоем разделе (ведь так?), но с~другим подходом.
%Что дает еще один подход? "--- а он дает возможность делать трюки, во~втором моем разделе "--- изучать приближение в.п. подмножеств к в.п. самих множеств. Чего твой подход не~позволяет "--- ты согласен? Вот в~этом ключе, возможно, можно привести пример с~тремя алгоритмами.
%2) По~поводу замены места упоминания слов <<с~заменой и без>>.
%Я бы не~хотел это делать.
%Если ты с~учетом пункта 1) комментариев добавишь некие слова про~возможность совпадения векторов ошибок в~твоем подходе, то~читатели сразу поймут в~чем <<соль>> :)
%
%\REVIEWNOTE{На первый взгляд кажется странными называть мой подход "случайным выбором с возвращаением", а твой --- случайным выбором без возвращания.
%Эти слова мне стали понятны только после прочтения теоремы \ref{thTolstR1}. Они справедливы только если множество, из которого ты производишь выбор, есть $A_m$ "--- полный слой шара.
%
%У меня в принципе нету такого понятия как $d$.
%Давай для примера рассмотрим максимально-связное множество из $D=3$ алгоритмов с равным числом ошибок,
%и выбор случайного подмножества из $d=2$ алгоритмов. Ты будешь усреднять по трём ($С_3^2$) подмножествам, каждое из которых будет связным (пара близких алгоритмов).
%Я буду изучать сумму из дикого количества слагаемых --- $(S_L)^3$ штук; каждое слагаемое будет соответствовать семейству из трех алгоритмов,
%подавляющее большинство слагаемых будет соответствовать случаю несвязной тройки алгоритмов (большое попарное расстояние). По-моему очевидно что наши подходы должны давать разный результат.
%Предлагаю
%а: включить разобранный выше пример (семейство из трех алгоритмов и выбор двух из них) в текст препринта.
%б: поместить слова про выбор с / без возвращения между формулировкой и началом доказательства теоремы \ref{thTolstR1}}.

В~одном из~предыдущих параграфов была получена
формула для вероятности переобучения сечения шара центральным слоем.
Такие семейства являются наиболее <<плотными>> множествами алгоритмов, допускающими равное число ошибок.
Ниже рассматривается другое подмножество слоя, в~котором алгоритмы лишены сходства.
Показывается, что вероятность переобучения в~данном случае экспоненциально стремиться к~единице
с ростом числа алгоритмов.

Отметим, что задача построения множества алгоритмов фиксированной мощности с~ограничением
на минимальное попарное расстояние между алгоритмами хорошо изучена в~теории кодов
исправляющих ошибки. Однако методы построения кодов исправляющих ошибки накладывают существенные
ограничения на~параметры задачи: количество объектов в~полной выборке и~минимальные расстояния между алгоритмами.
Предлагаемый ниже подход позволит обойти задачу порождения несвязного множества алгоритмов заданной мощности.

%\REVIEWNOTE{Убрать слово "расстопыренный" из предыдущего предложения. Например, заменить всё предложение на "Предлагаемый ниже подход позволит обойти задачу порождения несвязного множества %алгоритмов заданной мощности."}

Рассмотрим \textbf{упорядоченное} множество (т.\,е. вектор) алгоритмов $A = (a_1, \ldots, a_d ) \subset \AA$,\: $d = |A|$,
и~группу $G = (S_L)^{d}$, состоящую из~$(L!)^{d}$ элементов.
Группа $G$ действует на~множество~$A$,
всевозможными способами переставляя ошибки разных алгоритмов независимо друг от~друга.

Для формального определения действия $G$ на~$A$ рассмотрим элемент группы
$g = (\pi_1, \ldots, \pi_d) \in G$,
где все $\pi_j \in S_L$.
Он действует на~множество $A$ по~правилу
$g A = ( \pi_1 a_1, \ldots, \pi_d a_d )$,
где действие элементов группы $S_L$ на~$a \in \AA$
определено перестановками объектов выборки так же, как и~раньше.

Определим усредненный функционал вероятности переобучения:
\[
    \big\langle Q_\eps(A) \big\rangle_G
        = \frac 1{|G|} \sum_{g \in G} Q_\eps(g A).
\]
На~самом деле, $\big\langle Q_\mu(\epsilon, A) \big\rangle_G$ зависит уже не~от
самого множества алгоритмов $A$, а~только от~его профиля расслоения.
Рассмотрим в~качестве примера подмножество $m$-слоя, состоящее из~$d$ алгоритмов.

\begin{theorem}
Пусть $A$ "--- произвольное множество из~$D$ попарно"=различных алгоритмов,
каждый из~которых допускает~$m$~ошибок на~полной выборке.
Тогда
\begin{equation}
    \label{eq:scateredSlice}
    \big\langle Q_\varepsilon(A) \big\rangle_G = 1 - \left( 1 - \Hyper{L}{m}{\ell}{s(\eps)} \right)^D\!\!\!,
\end{equation}
где
$s(\eps) = \tfrac\ell L(m-\eps k)$.
\end{theorem}
\begin{vkProof}
Переставим знаки суммирования по~$X \in \XXell$ и~по $g \in G$ в~функционале $\big\langle Q_\eps(A) \big\rangle_G$:
\[
    \big\langle Q_\varepsilon(A) \big\rangle_G
        = \frac 1{|G|} \frac1{\CLl}
                \sum_{\Xl \in \XXell}
                \sum_{g \in G}\;
                 \sum \limits_{a\in (gA)(\Xl)}
                    \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}
                            {|(gA)(\Xl)|}.
\]

Заметим, все слагаемые под знаком усреднения $\frac 1{\CLl} \sum \nolimits_{\Xl \in \XXell}$ равны друг другу.
Поэтому, выбрав произвольного представителя $\Xl \in \XXell$, запишем
\begin{equation}
\label{rndSlice1}
    \big\langle Q_\varepsilon(A) \big\rangle_G
        = \frac 1{|G|}
                \sum_{g \in G}\;
                 \sum \limits_{a\in (gA)(\Xl)}
                    \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}
                            {|(gA)(\Xl)|}.
\end{equation}

Введем обозначение $s(\eps) = \frac \ell L(m - \epsilon k)$ и~докажем, что для любого множества
$A$, такого что все $a \in A$ имеют $m$ ошибок на~полной выборке, выполнено:
\begin{equation}
\label{oneslice1eq}
    \sum \limits_{a\in A(\Xl)}
        \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}
            {|A(\Xl)|} =
    \left[\min_{a \in A} n(a, \Xl) \leq s(\eps) \right].
\end{equation}
Действительно, пусть $m_\ell = \min\limits_{a \in A} n(a, \Xl)$. Тогда все алгоритмы из~
$A(X)$ имеют по~$m_\ell$ ошибок на~обучении и по~$m_k = m - m_\ell$ ошибок на~контроле.
Следовательно, все $a \in A(X)$ имеют одинаковую переобученность
$\delta(a, X) = \frac{m_k}k - \frac{m_\ell}{\ell}$.
Отсюда немедленно получим $[\delta(a, X) \geq \eps] = [m_\ell \leq s(\eps)]$.

Подставляя \eqref{oneslice1eq} в~\eqref{rndSlice1}, получим:
\[
\begin{aligned}
    \big\langle Q_\eps(A) \big\rangle_G
    &= \frac 1{|G|} \sum_{g \in G} \left[\min_{a \in gA} n(a, \Xl) \leq s(\eps) \right] = \\
    &= \frac 1{|G|} \sum_{g \in G} \Big[\exists a \in g A, n(a, \Xl) \leq s(\eps) \Big] = \\
    &= 1 - \frac 1{|G|} \sum_{g \in G} \Big[\forall a \in g A, n(a, \Xl) > s(\eps) \Big] = \\
   &= 1 - \frac 1{|G|} \sum_{g \in G} \prod_{i = 1}^{D} \Big[n(\pi_i a_i, X) > s(\eps) \Big].
\end{aligned}
\]
Пользуясь тем, что группа~$G$ "--- декартово произведение групп, получим
\[
     \big\langle Q_\eps(A) \big\rangle_G
   = 1 - \prod_{i = 1}^{D} \frac 1{|S_L|} \sum_{\pi_i \in S_L} \Big[n(\pi_i\, a_i, X) > s(\eps) \Big].
\]

Отметим, что все множители произведения $\prod \nolimits_{i = 1}^{D}$ равны друг другу.
Зафиксируем произвольный $a \in A$, и~пользуясь условием $n(\pi a, X) = n(a, \pi^{-1}) X$,
перепишем предыдущее выражение в~виде
\[
     \big\langle Q_\eps(A) \big\rangle_G
   = 1 - \Big(1 -
   \underbrace{\frac 1{|S_L|} \sum_{\pi' \in S_L}
   \big[n(a, \pi' X) \leq s(\eps) \big]}_{Q_\eps(a)} \Big)^D\!\!\!,
\]
где $a$ "--- произвольный алгоритм с~$m$ ошибками на~полной выборке.

Нетрудно заметить, что выделенное в~предыдущей формуле выражение
$Q_\eps(a)$ равно $\BHyper{L}{m}{\ell}{s(\eps)}$ "--- вероятности переобучения
вырожденного метода обучения, всегда возвращающего алгоритм~$a$.
\end{vkProof}

\begin{rem}
С увеличением числа алгоритмов в~слое
величина $\left(1 - \BHyper{L}{m}{\ell}{s(\eps)} \right)^D$ стремиться к~нулю,
а, следовательно, вероятность переобучения стремится к~единице.
\end{rem}

\begin{figure}[h]
    \label{fig:scateredSlice}
    \begin{centering}
    \includegraphics[height=80mm]{randomSlices.eps}
    \caption{Рассеянный слой алгоритмов.}
    \end{centering}
\end{figure}

На~рисунке приведены результаты численного эксперимента,
подтверждающего полученную выше формулу \eqref{eq:scateredSlice}.
По оси $OX$ откладывается число алгоритмов,
случайным образом выбранных из~слоя с~$m = 2$ ошибками.
По оси $OY$ откладывается разность между единицей и~вероятностью переобучения
для~$\epsilon = 0{.}04$,\: $L=100$,\: $\ell = 60$.
Гладкая кривая получена вычислением по~формуле \eqref{eq:scateredSlice},
тонкая кривая получена для одного случайно сгенерированного множества алгоритмов
методом Монте-Карло по~50~тыс. разбиений.

%\REVIEWNOTE{И что\dots? Этот раздел имел какую-то цель "--- Ведь так? Надо что-то сказать, что нам этот факт дал "--- ведь не просто же это была демонстрация <<хитрого>> метода? Я~бы заключил так: мы 1)в~который раз убедились в~необходимости связности для~обучаемости классификаторов "--- бессвязные множества не~просто переобучаются, а ЭКСПОНЕНЦИАЛЬНО быстро сносят крышу; 2) мы предложили некий новый механизм дополнительной рандомизации "--- в~очередной раз спасающей нам жизнь... Так ведь?}
%\REVIEWNOTE{Начало следующего параграфа вполне отвечает на этот вопрос.}

%\begin{task}
%Обобщить, теоретически или по~крайней мере экспериментально, формулу \eqref{eq:scateredSlice}
%на~случай произвольного профиля расслоения.
%\end{task}
\subsection{Разреженные подмножества слоя: \\случайный выбор без~возвращения}
%\REVIEWNOTE{С~учетом двух прошлых комментариев, предлагаю отразить в~названии этого параграфа идеологию <<без~возвращения>>.}
%\REVIEWNOTE{Придумать содержательное название этому параграфу}

В предыдущем параграфе было показано, что в~отсутствии связности и~расслоения вероятность переобучения экспоненциально быстро стремиться к~единице.
Отметим, что для доказательства этого факта нам пришлось отказаться от~предположения попарной различности алгоритмов семейства.
Этот шаг соответствует случаю выбора алгоритмов случайно {\it с возвращением}.
Возможен и другой подход к~задаче изучения вероятностей переобучения разреженных подмножеств слоя.
%Если в~прошлом разделе алгоритмы выбирались случайно {\it с возвращением}, то
Далее будет описана модель случайного выбора подмножества слоя {\it без~возвращений}.

Мы будем рассматривать $m$"=й слой алгоритмов $A_m$ и случайно выбирать $d$ различных алгоритмов из~него.
Требуется оценить среднее значение вероятности переобучения таких случайных подмножеств $m$"=го~слоя:
\[
\bar{Q}_{\varepsilon}(A_m,d) = \frac{1}{C^d_{D}}\sum_{\substack{A'\subset A_m:\\ |A'| = d}}Q_{\varepsilon}(A'),
\]
где $D = |A_m|= C^m_L$ "--- число алгоритмов в~$m$"=м~слое.
\begin{theorem}
\label{thTolstR1}
Среднее значение вероятности переобучения случайного подмножества $A_m$, состоящего из~$d$~различных алгоритмов, выражается следующим образом:
\[
\bar{Q}_{\varepsilon}(A_m,d) = 1 - \frac{C^d_{C^m_L\bar{H}^{\ell,m}_L(s(\varepsilon))}}{C^d_{C^m_L}},
\]
где
    $s(\varepsilon)=\frac{\ell}{L}(m - \varepsilon k)$,\:
    $\bar{H}^{\ell,m}_L(z) = \sum_{s=\lceil z\rceil}^{\ell}\frac{C^s_m C^{\ell - s}_{L - m}}{C^{\ell}_L}$ "--- правый хвост гипергеометрического распределения.
\end{theorem}
\begin{vkProof}
\[
\bar{Q}_{\varepsilon}(A_m,d) =
    \frac{1}{C^d_{D}}\sum_{\substack{A'\subset A_m:\\ |A'| = d}}
    \frac{1}{C^{\ell}_L}
    \sum_{X\in [\mathbb{X}]^{\ell}}
    \frac
        {\sum_{a\in A'(X)}[\delta(a,X)\geqslant \varepsilon]}
        {|A'(X)|},
\]
где $A'(X) = \arg\min\limits_{a\in A'}n(a,X)$.
Несложно заметить, что в~слое
\[
    \sum_{a\in A'(X)}[\delta(a,X)\geqslant \varepsilon]=\bigl|A'(X)\bigr|[\delta(a_0,X)\geqslant \varepsilon]
\]
для~произвольного $a_0 \in A'(X)$.

%\REVIEWNOTE{Давай, не будем?) по-моему,~все лаконично получается итак...}
%\REVIEWNOTE{Ровно ту же самую идею я эксплуатировал в параграфе про центральный слой шара! Из этой штуки выводиться теорема crl:easyFormula,
%которой нехвтает в этом препринте и на которую я ссылаюсь в параграфе про центральный слой шара. Предлагаю тебе оформить твоё утверждение "несложно заметить, что в слое..."
%в виде леммы, и воспользоваться ею сразу в двух местах --- тут и в центральном слое шара.}

Таким образом,
\[
\bar{Q}_{\varepsilon}(A_m,d) =
    \frac{1}{C^d_{D}}\sum_{\substack{A'\subset A_m:\\ |A'| = d}}
    \frac{1}{C^{\ell}_L}
    \sum_{X\in [\mathbb{X}]^{\ell}}
    \bigl[\exists a\in A'\colon n(a,X)\leqslant s(\varepsilon)\bigr].
\]
Переставим местами знаки суммирования:
\begin{multline*}
\bar{Q}_{\varepsilon}(A_m,d) ={}\\
    {}=
    \frac{1}{C^d_{D}}
    \frac{1}{C^{\ell}_L}
    \sum_{X\in [\mathbb{X}]^{\ell}}
    \sum_{\substack{A'\subset A_m:\\ |A'| = d}}
    \bigl[\exists a\in A'\colon n(a,X)\leqslant s(\varepsilon)\bigr]
    ={}\\
    {}=
    \frac{1}{C^d_{D}}
    \frac{1}{C^{\ell}_L}
    \sum_{X\in [\mathbb{X}]^{\ell}}
    F(X,A_m,d,\varepsilon).
\end{multline*}
Очевидно, что $F(X_1,A_m,d,\varepsilon) = F(X_2,A_m,d,\varepsilon)$ для~любых разбиений~$X_1$ и $X_2$.
Зафиксируем произвольное разбиение~$X_0$ и опустим первый аргумент у функции $F$:

%\REVIEWNOTE{Предлагаю писать так: очевидно что $F(X_1,A_m,d,eps) = F(X_2, A_m, d, eps)$, т.е. не зависит от X. Зафиксируем произвольное разбиение %$X_0$ и опустим первый аргумент у функции $F$.}

\begin{align}
\bar{Q}_{\varepsilon}&(A_m,d) = {}\notag\\
    &{}=
    \frac{1}{C^d_{D}}
    \sum_{\substack{A'\subset A_m:\\ |A'| = d}}
    \bigl[\exists a\in A'\colon n(a,X_0)\leqslant s(\varepsilon)\bigr]
    ={}\notag\\
    &{}=
    \frac{1}{C^d_{D}}
    \sum_{\substack{A'\subset A_m:\\ |A'| = d}}
    \bigl(
        1 - \bigl[\forall a\in A'\colon n(a,X_0) > s(\varepsilon)\bigr]
    \bigr)
    ={}\notag\\
    \label{thTolstRand}
    &{}= 1 -
    \frac{1}{C^d_{D}}
    \sum_{\substack{A'\subset A_m:\\ |A'| = d}}
    \prod_{a\in A'}\bigl[n(a,X_0)>s(\varepsilon)\bigr].
\end{align}
Рассмотрим $D$~различных бинарных переменных вида
${z_i=[n(a_i,X_0)>s(\varepsilon)]}$,\: $a_i\in A_m$,\: $i = 1,\dots,D$.
Нас интересует сумма всевозможных произведений $d$~различных из~них.
Очевидно, что это число способов выбрать $d$~ненулевых из~них.
С~учетом этого равенство~\eqref{thTolstRand} примет следующий вид:
\[
\bar{Q}_{\varepsilon}(A_m,d) =
    1 \;-\!\!\!
    \sum_{1\leqslant i_1<\dots<i_d\leqslant D}\frac{z_{i_1}\dots z_{i_d}}{C^d_{D}}
    =
    1 -
    \frac
        {C^d_{|\{a\in A_m\colon n(a,X)>s(\varepsilon)\}|}}
        {C^d_{D}},
\]
что после несложных вычислений завершает доказательство.
\end{vkProof}

Из~теоремы следует достаточно очевидный, но интересный факт:
\begin{cor}
Для~любого семейства $A$, лежащего в~$m$"=м~слое булева куба~$A_m$, если $|A|\geqslant C^m_L\bar{H}^{\ell,m}_L\bigl(s(\varepsilon)\bigr)$, то $Q_{\mu}(\varepsilon,A)=1$.
\end{cor}
\begin{vkProof}
При~$d\geqslant C^m_L\bar{H}^{\ell,m}_L\bigl(s(\varepsilon)\bigr)$ сумма в~последней строчке неравенства~\eqref{thTolstRand} равна~0, поскольку в~каждое из~слагаемых в~этом случае будет входить нулевой множитель.
\end{vkProof}
%\REVIEWNOTE{Ну, интерпретация тут довольно очевидная: фактически, достаточно посчитать число всех алгоритмов в~слое, на которых может быть переобучение =)}
%\REVIEWNOTE{Клёво!! Я не задумывался об этом! Действительно очень интересный факт! Особенно если вспомнить что правый хвост ---
%это вероятность переобучения одного алгоритма. Можно это как-нибудь проинтерпретировать?.. }

%\REVIEWNOTE{Добавь доказательство этого следствия. Мне не очевидно как оно следует из предыдущей теоремы. }

\begin{rem}
Мы снова получили экспоненциальный рост вероятности переобучения к~единице с~ростом числа алгоритмов~$d$.
Это легко увидеть, исследуя асимптотику логарифма дроби с~использованием формулы Стирлинга.
\end{rem}

Помимо исследования разреженных семейств слоя и роста их~переобученности с~числом алгоритмов в~них, описанный в~этом разделе подход позволяет также изучать вопрос о~приближении вероятности переобучения произвольного семейства алгоритмов с~помощью его разреженных подмножеств.
Это становится возможным, благодаря учету~структуры самого семейства алгоритмов~$A$, а не~только его профиля расслоения, как в прошлом разделе.

В~следующем разделе подобная возможность исследуется для~семейств, лежащих в~одном слое "--- в~этом случае результат получается особо просто.

\subsection{Разреженные подмножества семейств, лежащих в~слое}
Продолжая предложенный подход, сформулируем следующую теорему, которая, в~некотором смысле, обобщает теорему~\ref{thTolstR1}:
\begin{theorem}
\label{thTolstR2}
Пусть $A\subseteq A_m$, $|A|=D$, где~$A_m$ "--- $m$"=й слой алгоритмов.
Среднее значение вероятности переобучения случайного подмножества $A$, состоящего из~$d$~различных алгоритмов, выражается следующим образом:
\[
\bar{Q}_{\varepsilon}(A,d) = 1 -
    \frac
        {\sum_{X\in[\mathbb{X}]^{\ell}}C^d_{N(X,\varepsilon)}}
        {C^d_{D}C^{\ell}_L},
\]
где
    $N(X,\varepsilon) = \sum_{a\in A}[n(a,X)>s(\varepsilon)]$,\:
    $s(\varepsilon)=\frac{\ell}{L}(m - \varepsilon k)$.
\end{theorem}
Доказательство теоремы полностью повторяет доказательство теоремы~\ref{thTolstR1}.

С~помощью формулы Стирлинга снова легко показать, что каждое из~$C^{\ell}_L$ слагаемых вида ${C^d_{N(X,\varepsilon)}}/{C^d_D}$ с~ростом~$d$ экспоненциально быстро стремится к~0 либо~1 (для~тех разбиений $X$, для~которых $N(X,\varepsilon)=D)$.
При~$d=|A|$ мы в точности получаем вероятность переобучения всего семейства~$A$:
${\bar{Q}_{\varepsilon}(A,|A|) = Q_{\varepsilon}(A)}$.

Итак, мы получили важный результат "--- {\it в пределах одного слоя булева куба вероятность переобучения подмножеств произвольных семейств алгоритмов в среднем чрезвычайно быстро сходится к вероятности переобучения самого семейства с~ростом размера подмножеств}.

Следующая лемма показывает, что вероятность переобучения множества алгоритмов,~лежащего в~одном слое, может только увеличиваться при~добавлении к~нему новых алгоритмов из~того же слоя.
\begin{lemm}
\label{le:LayerGr}
Пусть $A\subset A_m$ и $a\in A_m\setminus A$. Тогда $Q_{\varepsilon}(A)\leqslant Q_{\varepsilon}(A\cup{a})$.
\end{lemm}

Лемма~\ref{le:LayerGr} вместе с~теоремой~\ref{thTolstR2} дают нам простой способ оценки вероятности переобучения семейства, лежащего в~слое.
Сначала нужно определить размер~$d$ подмножества данного семейства, достаточно большой для~требуемой точности оценки.
Затем необходимо~равномерно и случайно выбрать $d$~алгоритмов из семейства и вычислить вероятность переобучения полученного подмножества.
Тем не менее, на~практике открытыми остается ряд вопросов.
Например, нужно иметь возможность равномерно <<вытягивать>> алгоритмы из~семейства, полную матрицу ошибок которого мы, очевидно, не~знаем.


\section{Профили расслоения и~связности множества алгоритмов}

Поведение функционала $Q_\eps(A)$ существенным образом зависит от~структуры множества алгоритмов $A$.
В~то же время очевидно, что далеко не~все возможные множества алгоритмов $A \subset \AA$ возникают
при решении практических задач обучения по~прецедентам.

Во"~первых, большинство реальных семейств обладают свойством связности:
для каждого алгоритма ${a\in A}$ найдутся другие алгоритмы ${a'\in A}$
такие, что векторы ошибок $\vec a$~и~$\vec a'$
отличаются только на~одном объекте~\cite{sill98phd}.
Связные семейства порождаются, в~частности, методами классификации
с~непрерывной по~параметрам разделяющей поверхностью.
К~ним относятся линейные классификаторы,
машины опорных векторов с~непрерывными ядрами,
нейронные сети с~непрерывными функциями активации,
решающие деревья с~пороговыми условиями ветвления, и~многие другие.
Эксперименты \ref{voron10pria} показали, что с~уменьшением связности семейства
алгоритмов вероятность переобучения существенно возрастает.

Другим свойством, характерным для реальных семейств алгоритмов, является \emph{расслоение}.
Под $m$-слоем алгоритмов $A_m \subset A$ будем понимать подмножество алгоритмов,
допускающих ровно~$m$~ошибок на~полной выборке.
Тогда профиль расслоения $\Delta(A, m)$ множества алгоритмов $A$ определим как
зависимость количества алгоритмов в~$m$-слое $|A_m|$ от~номера слоя $m$:
\[
    \Delta(A,m) = |A_m| = |\{a \in A \colon n(a, \XX) = m\}|.
\]
Эксперименты~\cite{langford00computable,langford02quantitatively} показывают,
что для большинства применяемых на~практике семейств алгоритмов $A$ профиль расслоения $\Delta(A,m)$
имеет форму узкого пика, сконцентрированного в~средних слоях~$m\approx L/2$.
В~то же время, вероятность переобучения в~значительной мере определяется нижними слоями "---
алгоритмами с~наименьшим числом ошибок на~полной выборке.
Алгоритмы высоких слоев имеют ничтожно малую вероятность реализоваться в~результате обучения,
и~потому оказывают незначительный вклад в~вероятность переобучения.

Изучение профиля расслоения и~эффекта связности представляется важным шагом
на~пути к~получению универсальных формул вероятности переобучения.

\subsection{Профиль $r$-связности множества алгоритмов}

В~данном параграфе мы определим профиль $r$-связности множества алгоритмов и~изучим его свойства.
В~частности, для задач классификации на~два класса будет доказана инвариантность профиля $r$-связности
по~отношению к~произвольной смене меток целевых классов объектов.

Напомним, что \emph{расстояние между алгоритмами} $\rho(a, a')$ определялось как
расстояние Хэмминга между их~векторами ошибок на~полной выборке:
\[
    \rho(a, a') = \sum\limits_{x \in \XX} |I(a, x) - I(a', x)|.
\]
Шаром $B_r(a, A)$ радиуса $r$ с центром в алгоритме $a_0$ называлось следующее множество множества алгоритмов $A$:
\[
\{B_r(a, A) = a' \in A \colon \rho(a, a') \leq r\}.
\]

%\REVIEWNOTE{Включи в~определение шара слова о~семействе~$A$.}

\begin{df}
Профилем $r$-связности множества алгоритмов $A$ назовем функцию от~параметра $q$, заданную выражением:
\[
    \Theta_r(q, A) = \sum_{a \in A} [|B_r(a, A)| = q].
\]
Значение $\Theta_r(q, A)$ соответствует числу алгоритмов $a \in A$,
имеющих ровно $q$ соседей в~шаре $B_r(a, A)$.
\end{df}

Рассмотрим задачу классификации выборки $X_L$ на~два класса $Y = \{+1, -1\}$.
Обозначим целевой класс объекта $x_i \in X_L$ через $y_i \in Y$.

Рассмотрим действие группы $S_2 = \{e, h\}$ на~множестве $Y$,
при~котором неединичный элемент $h\in S_2$ действует
сменой метки целевого класса на~противоположную: $+1 \leftrightarrow -1$.

Рассмотрим группу $G = (S_2)^L$ и~ее элемент $g \in G \brop= \{h_1, h_2, \ldots, h_L\}$.
Элемент $g$ действует на~генеральную выборку $\XX$ с~помощью смены целевых классов у~тех
объектов $x_i$, для~которых $h_i \neq e$.
Действие группы $G$ на~$\XX$ естественным образом продолжается до
действия на~множестве всех алгоритмов $\AA$.
Действительно, на~каждый алгоритм $a \in \AA$ элемент $g \in G$ действует по~правилу
\begin{equation}
\label{eq:inverselabels}
    I(g a, x_i) =
    \begin{cases}
        +I(a, x_i), &\text { при } h_i = e \\
        -I(a, x_i), &\text { при } h_i \neq e. \\
    \end{cases}
\end{equation}
Это соответствует инверсии вектора ошибок на~тех объектах,
у~которых $g$ меняет целевой класс на~противоположный.

\begin{lemm}
\label{lem_isometr}
Действие определенной выше группы $G$ является изометрией
относительно хэммингова расстояния $\rho$ между векторами ошибок: для~всех $a, a'\in \AA$ и~любого~$g \in G$
\[
\rho(a, a') = \rho(g a, g a').
\]
\end{lemm}

\begin{vkProof}

Используя определение \eqref{eq:inverselabels}, убеждаемся, что
\[|I(g a, x_i) - I(g a', x_i)| = |I(a, x_i) - I(a', x_i)|\]
при обоих возможных значениях~$g_i$. Тогда
\begin{align*}
    \rho(ga, ga') &= \sum_{x_i \in \XX} |I(g a, x_i) - I(g a', x_i)| ={}\\
    {}&=\sum_{x_i \in \XX} |I(a, x_i) - I(a', x_i)| = \rho(a, a').
\end{align*}

\end{vkProof}

Из доказанной выше леммы немедленно следует, что для задач бинарной классификации
профиль $r$-связности инвариантен к~смене меток целевых классов:
\[
    \text{для любого } g \in (S_2)^L \text{ выполнено } \Theta_r(q, A) = \Theta_r(q, g A).
\]

Таким образом, связность является топологическим свойством семейства алгоритмов,
зависящим лишь от~взаимного расположения классифицируемых объектов в~пространстве признаков,
но не~зависящим от~меток их~классов.
Проиллюстрируем предыдущее утверждение на~примере семейства линейных классификаторов.

Рассмотрим задачу классификации точек трехмерного пространства разделяющими плоскостями,
проходящими через центр координат: $y = \sign(\langle w, x\rangle)$.
Тогда, с~топологической точки зрения, множеством алгоритмов будет сфера $S^2$.
Точки сферы соответствуют единичному направляющему вектору разделяющей плоскости.
Противоположные точки сферы соответствуют одновременной смене классификации всех объектов на~противоположную.

\begin{figure}
    \begin{centering}
    \includegraphics[height=100mm]{sphereMap.eps}
    \caption {
      Точки сферы "--- направляющие векторы линейной разделяющей гиперплоскости,
      грани графа на~сфере "--- классы эквивалентных алгоритмов (с равными векторами ошибок);
      пары граней имеющих общую границу соответствуют алгоритмам, различающимся на~одном объекте.
    }
    \end{centering}
\end{figure}

Зафиксируем выборку объектов $X_L$, каждому из~которых приписан целевой класс.
Это порождает раскраску всей сферы на~области, соответствующие алгоритмам с~равными векторами ошибок.
Заметим, что алгоритмы различаются на~одном объекте тогда и~только тогда, когда у~них есть общая граница
на полученной карте. Следовательно, профиль $1$-связности полностью определяется отношением соседства алгоритмов.
Границы карты, в~свою очередь, задаются условием $\langle w, x\rangle = 0$,
инвариантным по~отношению к~меткам целевых классов.
Следовательно, и~профиль $1$"~связности не~зависит от~меток целевых классов.

Изучение топологической природы множества алгоритмов позволяет
вывести ряд полезных свойств профиля $r$-связности, и, в~частности, среднего числа связей алгоритмов.
Для рассмотренного примера данную величину легко получить используя соотношение
Эйлера между числом вершин, ребер и~граней графа, реализованного на~сфере.

Действительно, пусть точки выборки $X_L$ находятся в~общем положении.
Тогда для~каждой точки выборки $x \in X_L$ рассмотрим семейство разделяющих плоскостей $A_x \subset S^2$,
проходящих через начало координат и~заданную точку.
На~рассмотренной выше сфере алгоритмов данное множество является центральной окружностью,
двойственной к~рассматриваемой точке.
Следовательно, граф границ алгоритмов получается с~помощью сечения сферы центральными окружностями.

Каждая вершина графа получается как пересечение двух центральных окружностей. Следовательно, количество
вершин ${V = L(L-1)}$. Из каждой вершины выходит ровно четыре ребра. Следовательно, количество ребер графа
$E = 2 L (L-1)$. Это позволяет определить количество граней графа:
\[  V - E + S = \varphi(S^2),\]
где $\varphi(S^2) = 2$ "--- Эйлерова характеристика сферы.
Следовательно, количество разделяющих гиперплоскостей в~семействе равно ${S = L(L-1) + 2}$.

Среднее количество связей между
$\frac {1}{|A|}\sum\nolimits_{q = 0}^{|A-1|} q \cdot \Theta_1(q, A)$
алгоритмами определяется отношением $2\, E/ S$,
т.\,е. асимптотически стремиться к~$4$ в~рассматриваемом двумерном случае.
В~работе \ref{kochedikov00candiser} доказывается более общее утверждение о~том,
что для семейства линейных классификаторов данная величина равна
удвоенной размерности пространства признаков. Вопрос об изучении профиля $r$-связности
$\Theta_r(q, A)$ при $r \geq 2$, по~всей видимости, еще не~изучался.

\subsection{Профиль расслоения"=связности множества алгоритмов}

Напомним, что профиль расслоения $\Delta(m, A)$ определялся как число алгоритмов $a \in A$,
допускающих ровно $m$ ошибок на~объектах полной выборки:
\[
    \Delta(m, A) = \sum_{a \in A} [ n(a, \XX) = m].
\]

Назовем профилем расслоения"=связности $\Lambda_r(m, q, A)$ семейства $A$ число алгоритмов
$a \in A$ с~$m$ ошибками и~$q$ соседями в~шаре радиуса $r$:
\[
    \Lambda_r(m, q, A) = \big|\{a \in A \colon |B_r(a, A)| = q\text{ и~} n(a, \XX) = m \}\big|.
\]
Известны общие верхние оценки вероятности переобучения \cite{voron10pria},
использующие профиль расслоения"=связности при $r=1$.
Вместе с~тем было известно, что профиль расслоения"=связности обладает рядом интересных свойств.
В~частности, для семейства линейных классификаторов было экспериментально показано,
что профиль расслоения"-связности приближенно раскладывается в~произведение двух функций,
одна из~которых зависит от~числа ошибок алгоритма на~полной выборке, а~вторая "--- только от~числа связей алгоритма.

Ниже мы приводим точную формулировку и~доказательство данной гипотезы.
Оказывается, точное равенство выполняется после усреднения профиля расслоения"-связности
по действию группы $G=(S_2)^L$ всевозможных смен целевых классов объектов.

\begin{lemm}[О наиболее вероятном профиле расслоения]
\label{th_averageS}
\[
    \frac {1}{|G|} \sum_{g \in G} \Delta(m, g A) = \frac{\Binom{L}{m}}{2^L} |A|.
\]
\end{lemm}
Данное утверждение означает, что биномиальное распределение $P(m) = \frac{\Binom{L}{m}}{2^L}$ задает статистически
наиболее вероятное распределение числа алгоритмов по~количеству ошибок.
Необходимо учитывать, что для~реальных задачах данное распределение будет иным.
Смещение фактического распределения алгоритмов по~числу ошибок
относительно биномиального можно использовать в~качестве меры
информативности (<<удачности>>) использования данного семейства $A$
для описания свойств конкретной выборки данных $\XX$.

\begin{vkProof}

Переставим знаки суммирования по~$g \in G$ и~по $a' \in A$:
\[
    \sum_{g \in G} \Delta(m, g A)
    = \sum_{g \in G} \sum_{a \in g A} [ n(a, \XX) = m]
    = \sum_{a' \in A} \sum_{g \in G} [n (g a', \XX) = m].
\]
Теперь осталось  воспользоваться тем, что усредненный профиль
 расслоения, записанный для фиксированного алгоритма $a'$,
задается биномиальным распределением:
\[
    \text{для всех } a' \in A_m \text{ выполнено: } \frac 1{|G|}\sum \limits_{g \in G} [n (g\, a', \XX) = m] =
    \frac {\Binom{L}{m}}{2^L}.
\]

\end{vkProof}

\begin{theorem}[О~наиболее вероятном профиле расслоения"=связности]
\label{th:MostLikeProfile}
\[
    \frac {1}{|G|} \sum_{g \in G} \Lambda_r(m, q, g A) =
    \frac{\Binom{L}{m}}{2^L} \cdot \Theta_r(q, A).
\]
\end{theorem}

Доказательство этой теоремы непосредственно следует из~инвариантности профиля
$r$"~связности относительно действия группы $G$ и
леммы \ref{th_averageS} о~наиболее вероятном профиле расслоения.

\begin{vkProof}

Обозначим оператор усреднения по~действию группы $G$ через $\textbf{E}_G =
\frac {1}{|G|} \sum_{g \in G}$.
\[
\begin{aligned}
    \textbf{E}_G \Lambda_r&(m, q, g A) ={}\\
        & {}= \textbf{E}_G \sum_{a' \in A} [|B_r(g a', g A)| = q ][n(g a', \XX) = m ] ={}\\
        & {}= \sum_{a' \in A} \textbf{E}_G [|B_r(a', A)| = q ][n(g a', \XX) = m ] ={}\\
        & {}= \sum_{a' \in A} \Big( [|B_r(a', A)| = q ] \cdot \textbf{E}_G [n(g a', \XX) = m ] \Big)={}\\
        & {}= \frac{\Binom{L}{m}}{2^L} \cdot \Theta_r(q, A). \\
\end{aligned}
\]

\end{vkProof}

В~работах \cite{kochedikov} экспериментально установлено,
что без усреднения по~действию группы $G$
аналогичное равенство выполнено приближенно:
\[
    \Lambda_r(m, q, A) \approx \frac 1{|A|} \Delta(m, A) \cdot \Theta_r(q, A).
\]

\subsection{Экспериментальные результаты о~профиле расслоения}

Приведем экспериментальные результаты по~сравнению
наиболее вероятного профиля расслоения $\frac{\Binom{L}{m}}{2^L}$
с~реальными профилями, возникающими при классификации объектов плоскости линейными классификаторами.

На~приведенных ниже рисунках рассматривалось четыре случая,
отличающихся степенью линейной разделимости объектов выборки.
Все сгенерированные выборки содержали 24 объекта (по~12 в~каждом классе).
Строились все возможные варианты линейной классификации моделями вида $y = \sign(\langle w, x\rangle + w_0)$
и~экспериментально вычислялось значения
профиля расслоение $\Delta(m, A)$.
На~графиках с~профилями расслоения по~оси X откладывается количество ошибок,
по~оси Y "--- доля алгоритмов с~соответствующим уровнем ошибки.
Более толстая кривая соответствует фактическому профилю расслоения, более тонкая "--- усредненному биномиальному.

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=54mm,height=36mm]{data_L24_V2.0.eps}
    \hfill
    \includegraphics[width=54mm,height=36mm]{data_L24_V1.0.eps}
    \hfill
    \includegraphics[width=54mm,height=36mm]{data_L24_V0.5.eps}
    \hfill
    \includegraphics[width=54mm,height=36mm]{data_L24_V0.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=54mm,height=36mm]{profile_L24_V2.0.eps}
    \hfill
    \includegraphics[width=54mm,height=36mm]{profile_L24_V1.0.eps}
    \hfill
    \includegraphics[width=54mm,height=36mm]{profile_L24_V0.5.eps}
    \hfill
    \includegraphics[width=54mm,height=36mm]{profile_L24_v0.eps}
    \hfill
    \end {multicols}
    \caption{Зависимость профиля расслоения от степени резделимости выборки.}
    %\caption{В левом столбце показаны четыре различных двумерных выборки из~24~объектов.
    %Объекты разных классов показаны точками и звездочками.
    %В~правом столбце показаны соответствующие профили расслоения для семейства линейных классификаторов.
    %По~оси~OX отложено количество ошибок,
    %по~OY "--- доля алгоритмов семейства с~соответствующим количеством ошибок.
    %Тонкая кривая одинакова на~всех четырех рисунках и соответствует наиболее вероятному распределению числа ошибок.}
\end{figure}

Отметим, что смещение профиля расслоения относительно
биномиального распределения можно использовать в качестве
меры информативности множества алгоритмов.
К сожалению, данный профиль является ненаблюдаемой величиной.
Обычно поиск лучшего алгоритма обычно является итерационной процедурой,
в~ходе которой генерируется лишь некоторое подмножество алгоритмов
из~рассматриваемого семейства.
Профиль расслоения наблюдаемого подмножества будет, очевидно, смещен
будет смещен в~сторону алгоритмов с~малым количеством ошибок.
Возможно, данное смещение удастся оценить, если провести калибровку:
подать на~вход алгоритму оптимизации шумовую выборку,
полученную по~исходном данным с~помощью случайной смены меток целевых классов.
Перспективным применением развитой выше теории представляется
дальнейшее изучение критериев информативности основанных на наблюдаемом профиле распределения ошибок.

%\clearpage
%\def\url#1.{}

\bibliography{MachLearn}
\bibliographystyle{gost71sv}

%\begin{thebibliography}{1}
%\bibitem{botov09mmro}
%    \BibAuthor{Ботов\;П.\,В.}
%    \BibTitle{Точные оценки вероятности переобучения для монотонных и~унимодальных семейств алгоритмов}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,7--10}.
%\bibitem{voron09dan}
%    \BibAuthor{Воронцов\;К.\,В.}
%    \BibTitle{Точные оценки вероятности переобучения}~//
%    Доклады РАН, 2009. "--- Т.\,429, \No\,1.  "--- С.\,15--18.
%\bibitem{voron09mmro}
%    \BibAuthor{Воронцов\;К.\,В.}
%    \BibTitle{Комбинаторный подход к~проблеме переобучения}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009. "---  \mbox{С.\,18--21}.
%\bibitem{frey09mmro}
%    \BibAuthor{Фрей\;А.\,И.}
%    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,66--69}.
%\bibitem{frey10ioi}
%    \BibAuthor{Фрей\;А.\,И.}
%    \BibTitle{Вероятность переобучения плотных и~разреженных многомерных~сеток алгоритмов}~//
%    Интеллектуализация обработки информации "--- М.:~МАКС Пресс, 2010.  "---  \mbox{С.\,87--90}.
%\bibitem{frey10pria}
%    \BibAuthor{Frei\;A.\,I.}
%    \BibTitle{Accurate estimates of the generalization ability for symmetric set of predictors and randomized learning %algorithms}~//
%    Pattern Recognition and Image Analysis. "--- 2010. "--- Vol. 20, no. 3. "--- \mbox{Pp.\,241--250}.
%\bibitem{voron10pria}
%    \BibAuthor{Vorontsov\;K.\,V.}
%    \BibTitle{Exact combinatorial bounds on the probability of overfitting for empirical risk minimization}~//
%    Pattern Recognition and Image Analysis. "--- 2010. "--- Vol. 20, no. 3. "--- \mbox{Pp.\,269--285}.

%\end{thebibliography}

%\begin{thebibliography}{10}
%\bibitem{bishop2006pattern} \BibAuthor{Bishop~C.}
%Pattern Recognition And Machine Learning. Springer.~2006.
%\end{thebibliography}

\end{document}
