\documentclass[12pt]{article}
\usepackage{Diplo}
\usepackage{multicol}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}
\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\epsilon}{\varepsilon}


\renewcommand{\vec}[1]{\boldsymbol{#1}}

\begin{document}

{
\renewcommand{\baselinestretch}{1}
\thispagestyle{empty}
\begin{center}
    \sc
        Министерство образования и науки Российской Федерации\\
        Московский физико-технический институт
        {\rm(государственный университет)}\\
        Факультет управления и прикладной математики\\
        Вычислительный центр им. А. А. Дородницына РАН\\
        Кафедра <<Интеллектуальные системы>>\\[35mm]
    \rm\large
        Фрей Александр Ильич\\[10mm]
    \bf\Large

        Точные оценки обобщающей способности\\
    для~симметричных множеств алгоритмов\\
    и~рандомизированных методов обучения\\[10mm]
    \rm\normalsize
        010656 --- Математические и информационные технологии\\[10mm]
    \sc
        Магистерская диссертация\\[30mm]
\end{center}
\hfill\parbox{80mm}{
    \begin{flushleft}
    \bf
        Научный руководитель:\\
    \rm
        с.н.с. ВЦ РАН, д.ф.-м.н.\\
        Воронцов Константин Вячеславович\\[4cm]
    \end{flushleft}
}

\begin{center}
    Москва\\
    2010
\end{center}
}

\newpage

\tableofcontents

\newpage

\abstract {
    В~комбинаторном подходе к~проблеме переобучения основной задачей является
    получение вычислительно эффективных формул для вероятности переобучения.
    %и~вероятности получить каждый из~имеющихся алгоритмов в~результате обучения.
    В работе предлагается теоретико-групповой подход,
    который позволяет проще выводить такие формулы в~тех случаях,
    когда множество алгоритмов наделено некоторой группой симметрий.
    Приводятся примеры таких множеств.
    Для рандомизированного метода обучения
    доказывается общая оценка вероятности переобучения.
    Показывается её применение для модельных множеств алгоритмов:
    слоя булева куба, булева куба, шара алгоритмов, связки монотонных цепочек,
    монотонных и унимодальных сеток произвольной размерности.
}

\newpage

\section{Введение}

При решении задач
распознавания образов, восстановления регрессии, прогнозирования
всегда возникает проблема выбора по~неполной информации.
Имея лишь конечную обучающую выборку объектов,
требуется из~заданного множества алгоритмов выбрать алгоритм, который
ошибался~бы как можно реже
не~только на~объектах наблюдаемой обучающей выборки,
но~и~на~объектах скрытой контрольной выборки,
которая в~момент выбора алгоритма ещё неизвестна.
Если частота ошибок на~контрольной выборке
оказывается значительно выше, чем на~обучающей,
то~говорят, что произошло
<<переобучение>> (overtraining) или
<<переподгонка>> (overfitting) алгоритма "---
он~слишком хорошо описывает конкретные данные,
но~не~обладает способностью к~обобщению этих данных,
не~восстанавливает порождающую их~зависимость
и~не~пригоден для построения прогнозов.

Частоту ошибок на~обучающей выборке называют также \emph{эмпирическим риском}.
\emph{Минимизация эмпирического риска} "--- это
метод обучения, который выбирает из~заданного множества алгоритм,
допускающий наименьшее число ошибок на~обучающей выборке~%
\cite{vapnik74rus,vapnik98stat}.
В~следующей таблице показан пример, когда минимизация эмпирического риска приводит к~переобучению.
Столбцы таблицы соответствуют алгоритмам,
строки "--- объектам обучающей выборки $\{x_1,\dots,x_\ell\}$ и~контрольной выборки $\{x_{\ell+1},\dots,x_L\}$.
Единица в~$[i,d]$-й ячейке таблицы означает, что
алгоритм~$a_d$ допускает ошибку на~объекте~$x_i$.
\[
    \bordermatrix{
          & a_1 & a_2 & \dots & a_d & \dots & a_D \cr
    x_1 & 0 & 1 & \dots & {0} & \dots & 1 \cr
    \dots & 1 & 1 & \dots & {0} & \dots & 0 \cr
    x_\ell & 0 & 0 & \dots & {0} & \dots & 0 \vspace{-2ex}\cr\cline{2-7}
    x_{\ell + 1} & 1 & 1 & \dots & {1} & \dots & 1 \cr
    \dots & 1 & 0 & \dots & {1} & \dots & 0 \cr
    x_L & 0 & 0 & \dots & {1} & \dots & 0
    }
\]

В данном примере переобучение могло быть следствием <<неудачного>> разбиения
генеральной выборки на~обучение и~контроль.
Поэтому вводится функционал \emph{вероятности переобучения},
равный доле разбиений выборки, при которых возникает переобучение~\cite{voron09dan, voron09mmro}.
Этот функционал инвариантен относительно выбора разбиения
и~характеризует качество данного метода обучения на~данной генеральной выборке.

Для некоторых семейств простой структуры
(монотонных и~унимодальных цепочек и~$h$"~мерных сеток)
в~\cite{voron09dan,botov09mmro} найдены точные выражения вероятности переобучения.
При этом использовалась техника производящих и~запрещающих объектов~\cite{voron09dan}.
Применение этой техники для получения формул в~ряде других случаев (полный куб алгоритмов,
шар алгоритмов) не представлялось возможным.

В~данной работе развивается новый, теоретико"=групповой подход~\cite{frey09mmro}, позволяющий выводить
эффективные оценки вероятности переобучения для множеств алгоритмов,
обладающих свойствами симметрии. Учет этих свойств позволил на порядки сократить
число слагаемых в~функционале вероятности переобучения, и~тем самым упростить вывод формул
в~конкретных случаях.

Вместо метода пессимистической минимизации эмпирического риска
в~работе предлагается исследовать рандомизированный метод.
Это позволяет окончательно отказаться от использования контрольной выборки
на этапе обучения алгоритмов и~дает возможность сравнить оценку худшего и~среднего случаев.

В~первом разделе работы вводятся формальные определения, включая
определения рандомизированного метода минимизации эмпирического риска
и~определение функционала вероятности переобучения.

Во втором разделе определяется группа симметрии множества алгоритмов,
и~доказывается точная формула для вероятности переобучения,
учитывающая свойства симметрии.

В~третьем разделе полученные результаты применяются для вывода явных формул вероятности
переобучения для ряда конкретных семейств алгоритмов: полного слоя, полного куба, шара алгоритмов,
связки из~монотонных цепочек, монотонной и унимодальной сеток произвольной размерности.
Приводятся результаты численных экспериментов.

\subsection{Определения}

Пусть задана генеральная выборка $\XX=\bigl( x_1, \dots, x_L)$, состоящая из~$L$ объектов.
Произвольный алгоритм классификации, примененный к~данной выборке, порождает бинарный вектор
ошибок $a \equiv \bigl( a(x_i) \bigr){}_{i=1}^L$,
где $a(x_i) = 1$ означает, что
алгоритм~$a$ допускает ошибку на объекте~$x_i$.
Генеральная выборка $\XX$ предполагается фиксированной, поэтому алгоритмы
отождествляются со~своими векторами ошибок.

Обозначим
через $\AA = \{0, 1\}^L$ множество всех возможных векторов ошибок длины $L$,
тогда $2^{\AA}$ "--- это множество всех подмножеств $\AA$.
Заметим, что $|\AA|=2^L$,\; $|2^{\AA}|=2^{2^L}$.

Через $\XXell$ обозначим множество всех разбиений генеральной выборки $\XX$
на обучающую выборку $\Xl$ длины~$\ell$ и~контрольную выборку $\Xk$ длины $k=L-\ell$.

\emph{Число ошибок} алгоритма $a$ на~выборке $U \subseteq \XX$
обозначим через $n(a, U) = \sum \limits_{x \in U}a(x)$.

\emph{Детерминированным методом обучения} назовем произвольное отображение вида
$\mu \colon 2^\AA \times \XXell \rightarrow \AA$.
Метод обучения~$\mu$ по~обучающей выборке~$\Xl$
выбирает некоторый алгоритм $a=\mu(A,\Xl)$ из~подмножества $A \subseteq \AA$.
Метод обучения называется \emph{минимизацией эмпирического риска}, если
возвращаемый им алгоритм допускает наименьшее число ошибок на~обучении:
для всех $\Xl \in \XXell$ и~$A \subseteq \AA$ выполнено
$\mu (A, \Xl) \in A(\Xl)$,
где
\[
    A(\Xl) = \Argmin_{a \in A} n(a, \Xl).
\]

При минимизации эмпирического риска может возникать неоднозначность "---
несколько алгоритмов из~$A(\Xl)$ могут иметь одинаковое число ошибок на~обучающей выборке.
В~\cite{voron09mmro} для устранения неоднозначности и~получения точных верхних
оценок вероятности переобучения использовалась \emph{пессимистичная}
минимизация эмпирического риска "--- предполагалось, что в~случае неоднозначности
выбирается алгоритм с~наибольшим числом ошибок на~генеральной выборке~$\XX$.
Это не~устраняет неоднозначность окончательно.
Возможны ситуации, когда несколько алгоритмов имеют
наименьшее число ошибок на~обучающей выборке~$\Xl$
и~одинаковое число ошибок на~генеральной выборке~$\XX$.
\mbox{В~таких} случаях на~множестве алгоритмов вводился линейный порядок,
и~среди неразличимых алгоритмов выбирался алгоритм с~б\'ольшим порядковым номером.
Введение приоритетности алгоритмов является искусственным приемом,
не~имеющим адекватных аналогов среди известных методов обучения.

\subsection{Рандомизированный метод обучения}

\emph{Рандомизированный метод обучения} произвольному множеству алгоритмов $A\subseteq \AA$
и~произвольной обучающей выборке~$\Xl \in \XXell$ ставит в~соответствие функцию
распределения весов на~множестве~алгоритмов:
\begin{equation}
    \label{eq:randomizedSearchMethodDraft}
    \mu : 2^\AA \times \XXell \rightarrow \{f : \AA \rightarrow [0, 1]\}.
\end{equation}
Естественно полагать, что эта функция нормирована и~может быть интерпретирована как вероятность
получить каждый алгоритм в~результате обучения.

Детерминированный метод обучения является частным случаем рандомизированного,
когда функция распределения весов~$f(a)$
принимает единичное значение ровно на~одном алгоритме и~нулевое на~всех остальных.

Заметим, что вместо определения \eqref{eq:randomizedSearchMethodDraft}
можно пользоваться эквивалентным способом задать то~же самое отображение:
\[
    \mu : 2^\AA \times \XXell \times \AA \rightarrow [0, 1].
\]

Рассмотрим группу $S_L$ "--- симметрическую группу из~L элементов, действующую на~множестве
объектов генеральной выборки перестановками $S_L = \{ \pi \colon \XX \rightarrow \XX\}$.

Для каждого $\pi \in S_L$ определим действие $\pi$ на~произвольную выборку $\Xl \in \XXell$
поэлементным действием отображения $\pi \colon \XX \rightarrow \XX$
на каждый объект выборки $\Xl$: $ \pi \Xl \brop= \{ \pi x \colon x \in \Xl\}$.
Это отображение не~меняет числа объектов: $|\Xl| = |\pi \Xl|$, поэтому можно
говорить о~действии $\pi$ на~множестве разбиений генеральной выборки на~обучение и~контроль
фиксированной длины $\pi : \XXell \rightarrow \XXell$.

Определим действие $S_L$ на~множестве всех алгоритмов $\AA$ перестановкой координат векторов
ошибок алгоритмов: $(\pi a)(x_i)= a(\pi^{-1}x_i)$.
Здесь на~объекты действует обратная перестановка $\pi^{-1}$, поскольку именно
в этом случае корректно говорить, что группа~$S_L$ \emph{действует} на~множестве~$\AA$.

\begin{Lemma}
\label{lem:errorCountInvariance}
Число ошибок алгоритма $a$ на~подвыборке $U \subseteq \XX$ не~меняется от~одновременного
применения перестановки $\pi \in S_L$ к~алгоритму и~к подвыборке:
\begin{equation}
    n(a, U) = n(\pi a, \pi U).
\end{equation}
\end{Lemma}

$\square$ \textbf{Доказательство.}
Запишем определение числа ошибок алгоритма и
воспользуемся определенным выше действием перестановки $\pi$ на~алгоритм $a$:
\[
\begin{aligned}
  n(\pi a, \pi U) =
& \sum_{x_i \in \pi U} (\pi a)(x_i) =
  \sum_{x'_i \in U} (\pi a)(\pi x'_i) = \\
& \sum_{x'_i \in U} a(\pi^{-1}(\pi x'_i )) =
  \sum_{x'_i \in U} a(x'_i) =
  n(a, U). \;\;\;\blacksquare
\end{aligned}
\]

\emph{Расстоянием между алгоритмами} $\rho(a, a')$ будем называть
расстояние Хэмминга между их векторами ошибок:
\[
    \rho(a, a') = \sum_{x \in \XX} |a(x) - a'(x)|.
\]
\begin{Lemma}
    \label{lem:algDistanceInvariance}
    Произвольная $\pi \in S_L$ является изометрией на множестве алгоритмов:
    \[
        \rho(a, a') = \rho(\pi a, \pi a').
    \]
\end{Lemma}
$\square$ \textbf{Доказательство.}
Рассмотрим алгоритм $b \equiv |a - a'|$. Тогда $\rho(a, a') = n(b, \XX)$.
Непосредственной проверкой убеждаемся, что $\pi b = | \pi a - \pi a' |$.
Следовательно, $\rho(\pi a, \pi a') = n(\pi b, \XX)$.
Из леммы $\ref{lem:errorCountInvariance}$ следует, что $n(b, \XX) = n(\pi b, \XX)$,
т.е. $\rho(a, a') = \rho(\pi a, \pi a')$.
$\blacksquare$

Действие группы $S_L$ на~множестве всевозможных алгоритмов $\AA$ естественным  образом продолжается
до действия на~системе всех подмножеств "--- $S_L \colon 2^\AA \rightarrow 2^\AA$ по~правилу
$\pi A = \{\pi a \colon a\in A\}$.
В дальнейшем будет использоваться единое обозначение $\pi$ для описанных выше действий.

Теперь можно дать более строгое определение рандомизированного метода обучения.

\begin{Def}\label{searchMethod}
    \label{def:randomizedSearchMethod}
    \emph{Рандомизированным методом обучения} назовем отображение вида
    \begin{equation}
        \label{eq:randomizedSearchMethod}
        \mu : 2^\AA \times \XXell \times \AA \rightarrow [0, 1],
    \end{equation}
    удовлетворяющее при любых
    $A \in 2^\AA$,\; ${\Xl \in \XXell}$,\; $a,\, b \in A$ и~$\pi \in S_L$
    условиям:
    \begin{enumerate}
        \item[1)] нормировка:
              \begin{equation}
                \sum\limits_{a \in A} \mu(A, \Xl, a) = 1;
              \end{equation}
        \item[2)] неразличимость алгоритмов с~одинаковой частотой ошибок на~обучении:
              \begin{equation}
              n(a, \Xl) = n(b, \Xl) \;\to\; \mu(A, \Xl, a) = \mu(A, \Xl, b);
              \end{equation}
        \item[3)] инвариантность результата обучения относительно замены множества алгоритмов $A$ на $\pi(A)$:
              \begin{equation}
              \label{eq:randomizedSearchMethod_3}
                    \mu(A, \Xl, a) = \mu \bigl( \pi A, \pi \Xl, \pi a \bigr).
              \end{equation}
    \end{enumerate}
\end{Def}

Первое условие означает <<вероятностную>> нормировку весов алгоритмов
и~обеспечивает нулевую <<вероятность>> алгоритмам, не~принадлежащих множеству~$A$.
Второе условие означает, что при любом разбиении $\XX = \Xl \sqcup \Xk$, $\Xl \in \XXell$
вероятность получить алгоритм в~результате обучения
зависит только от~количества ошибок алгоритма на~обучении.
Третье условие означает, что результат обучения не~изменится,
если подействовать перестановкой~$\pi$ одновременно
и~на~множество объектов~$\XXell$, и~на~множество алгоритмов~$\AA$.

Конструктивным примером рандомизированного метода обучения
является следующее отображение, которые мы назовем
\emph{рандомизированным методом минимизации эмпирического риска}:
\begin{equation}
    \label{eq:randomizedRiskMinimization}
    \mu(A, \Xl, a)
    =
    \frac{\bigl[ a \in A(\Xl) \bigr]}{|A(\Xl)|}.
\end{equation}
Тут и далее квадратные скобки "--- нотация Айверсона \cite{knuth98concrete},
переводящая логическое выражение в число $0$ или $1$ по правилам
$[\text{истина}] = 1$, $[\text{ложь}] = 1$.

\begin{Lemma} %(О инвариантности множества $A(\Xl)$ относительно $\pi \in S_L$)
\label{lem:AXinvariance}

Для всех $\pi \in S_L$ алгоритм
$a_0 \in A(\Xl)$
тогда и только тогда, когда
$\pi a_0 \in (\pi A)(\pi \Xl)$.
\end{Lemma}

$\square$ \textbf{Доказательство.}

Перепишем утверждение леммы в виде
\[
    a_0 \in \Argmin_{a \in A} n(a, \Xl)
    \;\;\Leftrightarrow\;\;
    \pi a_0 \in \Argmin_{a \in \pi A} n(a, \pi \Xl).\]

Используя лемму \ref{lem:errorCountInvariance},
проведем следующую цепочку равносильных утверждений:
\[
    \begin{aligned}
    a_0 \in & \Argmin_{a \in A} n(a, \Xl) \;\;\Leftrightarrow \\
    & \;\;\Leftrightarrow \;\;
    \forall a \in A \rightarrow n(a_0, \Xl) \leq n(a, \Xl) \;\; \Leftrightarrow\\
    & \;\; \Leftrightarrow \;\;
    \forall a \in A \rightarrow n\big(\pi a_0, \pi \Xl\big)
            \leq n\big(\pi a, \pi \Xl \big) \;\; \Leftrightarrow\\
    & \;\; \Leftrightarrow \;\;
    \forall a' \in \pi A \rightarrow n\big(\pi a_0, \pi \Xl \big)
            \leq n\big(a', \pi \Xl \big) \;\; \Leftrightarrow\\
    & \;\; \Leftrightarrow \;\;
    \pi a_0 \in \Argmin_{a \in \pi A} n(a, \pi \Xl). \;\;\; \blacksquare
    \end{aligned}
\]

\begin{Theorem}
Отображение \eqref{eq:randomizedRiskMinimization}
является рандомизированным методом обучения.
\end{Theorem}

$\square$ \textbf{Доказательство.}
Первое условие проверяется явно:
\[
    \sum \limits_{a\in A}\mu(A,\Xl,a) =
    \sum \limits_{a \in A(\Xl)}\frac1{|A(\Xl)|} = 1.
    \]

Для доказательства второго утверждения достаточно заметить,
что два алгоритма $a_1$ и~$a_2$ с~равным числом ошибок на~обучении
могут лежат в~множестве $A(\Xl)$ только одновременно.
Следовательно, вероятность получить каждый из~алгоритмов в~результате обучения
равна либо нулю, либо $\frac 1{|A(\Xl)|}$.

Третье условие непосредственно следует из доказанной выше леммы \ref{lem:AXinvariance}.

Теорема доказана.
$\blacksquare$

\subsection{Вероятность переобучения}

Величину $\nu(a, U) = n(a, U) / |U|$ будем называть \emph{частотой ошибок}
алгоритма~$a$~на~выборке $U$.
\emph{Уклонение частот} на~разбиении $\XX = \Xl \sqcup \Xk$ определим как разность
частот ошибок на~контроле и~на обучении:
$\delta(a, \Xl) = \nu(a, \Xk) - \nu(a, \Xl)$.

Зафиксируем параметр $\epsilon \in (0, 1]$.
Будем говорить, что алгоритм $a$ \emph{переобучен} при разбиении $\Xl \sqcup \Xk$,
если $\delta(a, \Xl) \geq \epsilon$.

Сделаем основное (и~единственное) вероятностное предположение, что
все разбиения генеральной выборки на~наблюдаемую и~скрытую подвыборки равновероятны~%
\cite{voron09dan,voron09mmro}.

Если $\varphi\colon\XXell\to\{\text{истина},\text{ложь}\}$ "--- некоторый предикат,
то~\emph{вероятностью события}~$\varphi(\Xl)$ будем называть
долю разбиений выборки, при которых предикат $\varphi(\Xl)$ истинен:
\[
    \textbf{P}\bigl[\varphi(\Xl)\bigr]
    =
    \frac{1}{C_L^\ell}
    \sum\limits_{\Xl \in \XXell} \bigl[\varphi(\Xl)\bigr].
\]

Соответственно, математическое ожидание произвольной функции $\xi\colon\XXell\to\RR$ есть
\[
    \textbf{E} \xi(\Xl)
    =
    \frac{1}{C_L^\ell}
    \sum\limits_{\Xl \in \XXell} \xi(X).
\]

Вероятностью получить алгоритм~$a\in A$ в~результате обучения назовем величину
\begin{equation}
    \label{eq:algProbability}
    P_\mu(a,A)
    =
    %\frac1{C_L^\ell} \sum_{\Xl \in \XXell}
    \textbf{E}
    \mu(A, \Xl, a).
\end{equation}

Для произвольного  $\epsilon \in (0, 1]$ определим
\emph{вклад} алгоритма $a\in A$ в~вероятность переобучения:
\begin{equation}
    \label{eq:algContribution}
    Q_\mu(\epsilon, a, A)
    =
    %\frac1{C_L^\ell} \sum_{\Xl \in \XXell}
    \textbf{E}
    \mu(A, \Xl, a)
    \bigl[ \delta(a, \Xl) \geq \epsilon \bigr].
\end{equation}

\emph{Вероятность переобучения} определим как сумму вкладов по~всем алгоритмам:
\begin{equation}
    \label{eq:overfitProbability}
    Q_\mu(\epsilon, A)
    =
    \sum_{a\in A} Q_\mu(\epsilon, a,A)
    =
    %\frac1{C_L^\ell} \sum_{\Xl \in \XXell}
    \textbf{E}
    \sum_{a\in A}
        \mu(A, \Xl, a)
        \bigl[ \delta(a, \Xl) \geq \epsilon \bigr].
\end{equation}

Для детерминированного метода обучения $\mu \colon 2^\AA \times \XXell \rightarrow \AA$
это определение можно упростить:
\[
\begin{aligned}
    Q_\mu(\epsilon, A)
    &=
    \textbf{E}
        \sum_{a\in A}
            \bigl[ \mu(A, \Xl) = a \bigr]
            \bigl[ \delta(a, \Xl) \geq \epsilon \bigr] = \\
    &=
    \textbf{E}
            \bigl[ \delta(\mu(A, \Xl), \Xl) \geq \epsilon \bigr].
\end{aligned}
\]

Полученное выражение буквально означает <<долю разбиений выборки на~обучение и~контроль,
при которых выбранный алгоритм $a=\mu(A,X)$ оказался переобученным>>.

\begin{Def}
Методы минимизации эмпирического риска
\begin{align*}
    \mu_o \Xl &= \arg\!\!\min\limits_{a\in A(\Xl)} n(a,\Xk); \\
    \mu_p \Xl &= \arg\!\!\max\limits_{a\in A(\Xl)} n(a,\Xk);
\end{align*}
называются, соответственно, \emph{оптимистичным} и~\emph{пессимистичным}.
\end{Def}

\begin{Theorem}
Путь $\mu$ "--- рандомизированный метод минимизации эмпирического риска.
\mbox{Тогда} для произвольного
множества алгоритмов $A \subseteq \AA$ и каждого $\epsilon \in (0, 1]$ справедлива цепочка неравенств:
\begin{equation}
    \label{eq:sortedMethods}
    Q_{\mu_o}(\epsilon, A) \leq Q_{\mu}(\epsilon, A) \leq Q_{\mu_p}(\epsilon, A).
\end{equation}
\end{Theorem}
Эта теорема позволяет называть методы $\mu$, $\mu_p$ и~$\mu_o$ соответственно
выбором случайного, худшего и~лучшего алгоритма из~лучших на~обучении.

$\square$ \textbf{Доказательство.}
Для краткости обозначений будем опускать аргумент $A$ у отображений $\mu_o$ и~$\mu_p$.
Покажем, что утверждение верно для каждого разбиения выборки:
\[
        \bigl[ \delta(\mu_o(\Xl), \Xl) \geq \epsilon \bigr]
    \leq
        \sum_{a\in A(\Xl)} \frac 1{|A(\Xl)|}
        \bigl[ \delta(a, \Xl) \geq \epsilon \bigr]
    \leq
        \bigl[ \delta(\mu_p(\Xl), \Xl) \geq \epsilon \bigr].
\]

Введем обозначения:
\begin{align*}
    F_o &\equiv \bigl[ \delta(\mu_o(\Xl), \Xl) \geq \epsilon \bigr];\\
    F_p &\equiv \bigl[ \delta(\mu_p(\Xl), \Xl) \geq \epsilon \bigr];\\
    F\; &\equiv \frac 1{|A(\Xl)|} \sum \limits_{a\in A(\Xl)}
                \bigl[ \delta(a, \Xl) \geq \epsilon \bigr].
\end{align*}

Рассмотрим неравенство $F_o \leq F$. Заметим, что $F_o$ может принимать
только два значения "--- $0$ и~$1$, а~значение выражения $F$
ограничено отрезком $[0, 1]$. Следовательно, если $F_o = 0$ неравенство выполнено
автоматически.

Докажем, что из~$F_o = 1$ следует $F = 1$.
Обозначим $a_o \equiv \mu_o(\Xl)$.
По определению $\mu_o$ это значит, что
$a_o \in A(\Xl)$ и~$\forall a \in A(\Xl)$ выполнено
$n(a_o, \Xk) \leq n(a, \Xk)$.
Следовательно, $\forall a \in A(\Xl)$ выполнено
$\delta(a, \Xl) \geq \delta(a_o, \Xl) \geq \epsilon$.
Значит $F = \sum \limits_{a\in A(\Xl)} \frac 1{|A(\Xl)|} = 1$.

Для доказательства утверждения $F \leq F_p$ достаточно рассмотреть два случая: $F_p = 0$ и~$F_p = 1$
и провести аналогичные рассуждения.
$\blacksquare$

\section{Симметрия множества алгоритмов}
Введённые выше понятия позволяют определить группу симметрии множества алгоритмов
и~с~её помощью получать вычислительно эффективные формулы вероятности переобучения.

\subsection{Инвариантность вероятности переобучения к~действию группы $S_L$}

Определения рассмотренных выше функционалов $P_\mu(a, A)$, $Q_\mu(\epsilon, a, A)$ и~$Q_\mu(\epsilon, A)$
опирались на~упорядоченность объектов в~генеральной выборке $\XX$.
Докажем инвариантность указанных функционалов к~изменению
нумерации объектов~в~$\XX$.

Для краткости обозначений будем опускать аргумент $\epsilon$ у функции $Q_\mu(\epsilon, a, A)$.

\begin{Lemma}
    \label{Lem:PQ_invariance}
    Вероятность $P_\mu(a, A)$ получить алгоритм~$\,a$ в~результате обучения,
    а~также вклад $Q_\mu(a, A)$ алгоритма $a$  вероятность переобучения
    сохраняются при одновременном применении произвольной перестановки
    $\pi \in S_L$ к~множеству~$A$ и~алгоритму~$a$:
    \begin{align}
        P_\mu(a, A) &= P_\mu(\pi a, \pi A), \\
        Q_\mu(a, A) &= Q_\mu(\pi a, \pi A).
    \end{align}
\end{Lemma}
$\square$
    \textbf{Доказательство.}
    Заметим, что для произвольной функции $f(\Xl)$ от~разбиения выборки
    $\Xl \sqcup \Xk$ на~обучение и~контроль выполнено
    $\textbf{E} f(\Xl) = \textbf{E} f(\pi \Xl)$.
    Воспользуемся также свойством $\delta(\pi a, \pi \Xl) = \delta(a, \Xl)$,
    которое следует из~леммы \ref{lem:errorCountInvariance} и~определения
    уклонения частот ошибок алгоритма.
    Тогда
    \[
        \begin{aligned}
        Q_\mu(\pi a, \pi A)
        &= \textbf{E}%\frac 1{C_L^\ell} \sum_{\Xl \in \XXell}
            \mu(\pi A, \Xl, \pi a) \left[ \delta (\pi a, \Xl) \geq \epsilon \right] = \\
        &= \textbf{E}%\frac 1{C_L^\ell} \sum_{\Xl \in \XXell}
            \mu(\pi A, \pi \Xl, \pi a) \left[ \delta (\pi a, \pi \Xl)
            \geq \epsilon \right] = \\
        &= \textbf{E}%\frac 1{C_L^\ell} \sum_{\Xl \in \XXell}
            \mu(A, \Xl, a) \left[ \delta (a, \Xl) \geq \epsilon \right]
        = Q_\mu(a, A).
        \end{aligned}
    \]
    Равенство $P_\mu(\pi a, \pi A) = P_\mu(a, A)$  получается из
    выражения $Q_\mu(a, A) = Q_\mu(\pi a, \pi A)$ подстановкой $\epsilon = -1$.
$\blacksquare$

\begin{Corollary}
Вероятность переобучения сохраняется при применении произвольной перестановки $\pi \in S_L$ к множеству алгоритмов:
\begin{equation}
    Q_\mu(A) = Q_\mu(\pi A).
\end{equation}
\end{Corollary}
$\square$
    \textbf{Доказательство.}
\[
    Q_\mu(\pi A) =
    \sum_{a \in \pi A}Q_\mu(a, \pi A) =
    \sum_{a \in A}Q_\mu(\pi a, \pi A) =
    \sum_{a \in A}Q_\mu(a, A) =
    Q_\mu(A).
    \;\;\blacksquare
\]

Последнее утверждение выглядит очень естественно, поскольку
в большинстве задач обучения по~прецедентам порядок объектов в~выборке
не имеет значения.

\subsection{Группа симметрии множества алгоритмов}

Напомним, что выше было определено действие группы $S_L$ на~множестве
всех возможных наборов алгоритмов $2^\AA$.

\begin{Def}
    \label{def:symmetryGroup}
    \emph{Группой симметрий} $\Sym(A)$ множества алгоритмов $A\in 2^\AA$
    будем называть его стационарную подгруппу:
    \[
        \Sym(A) = \{\pi \in S_L \colon \pi A = A\}.
    \]
\end{Def}

\begin{Example}
Рассмотрим множество алгоритмов,
заданное следующей матрицей ошибок:
\[
    \bordermatrix{& a_1 & a_2 & a_3 & a_4 & a_5 \cr
        x_1 & 1 & 1 & 1 & 0 & 0 \cr
        x_2 & 0 & 1 & 1 & 1 & 0 \cr
        x_3 & 0 & 0 & 1 & 1 & 1 \cr
        x_4 & 1 & 0 & 0 & 1 & 1 \cr
        x_5 & 1 & 1 & 0 & 0 & 1 \cr
    }
\]
Строки матрицы соответствуют объектам генеральной выборки $\XX$,
столбцы --- алгоритмам $a \in A$.
Группа симметрии данного множества алгоритмов является диэдральной группой:
$\Sym(A) \cong S_2 \ltimes \mathbb{Z} \slash 5\, \mathbb{Z}$.
Образующими элементами группы являются циклическая перестановка
$\pi_{{}_\circlearrowright} = (x_1, x_2, x_3, x_4, x_5) \in S_5$ и~пара транспозиций
$\pi_{\!{}_\leftrightarrow} = (x_2, x_5)(x_3, x_4)$.
\end{Example}

Важно отметить, что группа симметрии $\Sym(A)$ \emph{действует} на~множестве алгоритмов~$A$.
Действительно, каждый элемент группы симметрий $\pi \in \Sym(A)$
переставляет алгоритмы $a$ только \emph{внутри }множества $A$.
Значит, для любого $a\in A$ и~любого $\pi \in \Sym(A)$ выполнено $\pi a \in A$.
Поэтому для группы $\Sym(A)$, в~отличии от~всей группы $S_L$,
естественным образом определено действие на~множестве $A$.

\emph{Орбитой} элемента $m$ множества $M$, на~котором действует группа $G$,
называется подмножество $Gm = \{g m \colon g \in G\} \subseteq M$.
Орбиты двух элементов $m_1$ и~$m_2$ либо не~пересекаются, либо совпадают.
Это позволяет говорить о разбиении множества $M$ на~непересекающиеся орбиты:
$M = G m_1 \sqcup \ldots \sqcup G m_k$.

В дальнейшем будут рассматриваться орбиты действия группы симметрии $\Sym(A)$
на множестве алгоритмов.
Совокупность всех орбит множества алгоритмов $A$ обозначим через $\Omega(A)$.
Представителя орбиты $\omega~\in~\Omega(A)$ обозначим через $a_\omega \in A$.

В теории групп точки одной орбиты принято называть эквивалентными.
Однако~в~\cite{vapnik74rus} \emph{эквивалентными алгоритмами} называют
алгоритмы с равными векторами ошибок на~генеральной выборке~$\XX$.
Поэтому различных представителей одной и~той же орбиты будем называть
\emph{идентичными алгоритмами}.

\begin{Lemma}
    \label{lemEqualErrorsCount}
    \emph Идентичные алгоритмы имеют равное число ошибок на~полной выборке.
\end{Lemma}
$\square$ Доказательство утверждения автоматически следует из~леммы \ref{lem:errorCountInvariance}:
\[n(a, \XX) = n(\pi a, \pi \XX) = n(\pi a, \XX).\;\;\blacksquare\]

Согласно данному выше определению \emph{алгоритм}
$a \equiv \bigl( a(x_i) \bigr){}_{i=1}^L$ является вектором,
следовательно, зависит от~нумерации объектов выборки.
Однако ни~группа симметрий~$\Sym(A)$, ни~разбиение на~классы идентичных алгоритмов~$\Omega(A)$,
уже не~зависят от~этой нумерации.

\begin{Lemma}
    \label{lem:SymGroupIsimorphism}
    Для любого множества алгоритмов ${A\in 2^\AA}$
    и~любой перестановки $\pi \in S_L$ группы $\Sym(A)$ и~$\Sym(\pi A)$ сопряжены:
    $\Sym(\pi A) = \pi \circ \Sym(A) \circ \pi^{-1}$.
\end{Lemma}

Эта лемма эквивалентна известному утверждению из~теории групп:
стационарные подгруппы точек, лежащих на
одной орбите действия, получаются друг из~друга сопряжением \cite{vinberg2001}.

\begin{Lemma}
    Пусть алгоритмы $a_1$ и~$a_2$ идентичны в~множестве алгоритмов $A$.
    Тогда $\forall \pi \in S_L$ алгоритмы $\pi a_1$ и~$\pi a_2$ идентичны
    в~множестве алгоритмов $\pi A$.
\end{Lemma}

$\square$ Пусть $\gamma \in \Sym(A)$ "--- перестановка, такая что $a_2 = \gamma a_1$.
Тогда $\pi a_2 = \pi \gamma a_1 = (\pi \gamma \pi^{-1}) \pi a_1$ = $\tilde \gamma \pi a_1$.
Из леммы \ref{lem:SymGroupIsimorphism} получаем, что
$\tilde\gamma = \pi \gamma \pi^{-1}$ "--- элемент $\Sym(\pi A)$. $\blacksquare$

\subsection{Теоремы о равном вкладе идентичных алгоритмов в~вероятность переобучения}

Теоремы, приведенные в~данном параграфе, позволяют в~ряде случаев существенно упростить
получение явных формул для вероятности переобучения.

\begin{Theorem}
    \label{th:equalContribution}
    Идентичные алгоритмы имеют равную вероятность реализоваться в~результате обучения,
    а~также дают равный вклад в~вероятность переобучения:
    \begin{align}
        P_\mu(a, A) &= P_\mu(\pi a, A), \\
        Q_\mu(a, A) &= Q_\mu(\pi a, A),
    \end{align}
    где $\pi \in \Sym(A)$.
\end{Theorem}
$\square$
    Доказательство автоматически следует из~леммы \ref{Lem:PQ_invariance} и~определения
    группы симметрии: $P_\mu(\pi a, A) = P_\mu(\pi a, \pi A) = P_\mu(a, A)$,
    и~аналогично для $Q_\mu(a, A)$. $\blacksquare$

\begin{Corollary}
    Пусть группа симметрии действует на~множестве алгоритмов транзитивно:
    $A = \{\pi a_0, \, \pi \in \Sym(A)\}$, где $a_0 \in A$ "--- произвольный алгоритм множества $A$.
    Тогда все алгоритмы множества имеют равную вероятность реализоваться в~результате обучения.
\end{Corollary}

Теорема \ref{th:equalContribution} позволяет перейти от~суммирования по~всем
алгоритмам множества к~суммированию по~орбитам действия группы $\Sym(A)$.

\begin{Theorem}
\label{th:equalContributionERM}
Вероятность переобучения $Q_\mu(A)$
для рандомизированного метода минимизации эмпирического риска
можно записать в~следующем виде:
\begin{equation}
\label{eq:MainInstrument}
    Q_\mu(A) =
        %\frac 1{C_L^\ell}
        \sum_{\omega \in \Omega(A)}\!\! |\omega| \,
        %\sum_{\substack{
        %        \Xl \in \XXell\colon \\
        %        a_\omega \in  A(\Xl)
        %}}
        \mathbf{E}
        \frac {\bigl[a_\omega \in  A(\Xl)\bigr]}{|A(\Xl)|}
        %\frac 1{|A(\Xl)|}
        \left[ \delta(a_\omega, \Xl) \geq \epsilon \right].
\end{equation}
\end{Theorem}

$\square$
Воспользуемся теоремой о равном вкладе идентичных алгоритмов в~вероятность переобучения,
затем определениями~\eqref{eq:algContribution} и~\eqref{eq:randomizedRiskMinimization}:
\[
    \begin{aligned}
    Q_\mu(A) & =
        \sum_{a \in A} Q_\mu(a, A) =
        \sum_{\omega \in \Omega(A)}\!\! |\omega| \,
        Q_\mu(a_\omega, A)
        =
    \\
        & =
        %\frac 1{C_L^\ell}
        \sum_{\omega \in \Omega(A)}\!\! |\omega| \,
        %\sum_{\substack{
        %        \Xl \in \XXell \colon \\
        %        a_\omega \in  A(\Xl)
        %}}
        %\frac 1{|A(\Xl)|}
        \mathbf{E}
        \frac {\bigl[a_\omega \in  A(\Xl)\bigr]}{|A(\Xl)|}
        \left[ \delta(a_\omega, \Xl) \geq \epsilon \right].
        \quad\blacksquare
    \end{aligned}
\]

Формула~\eqref{eq:MainInstrument} является основным инструментом вывода точных оценок
вероятности переобучения для рандомизированного метода
минимизации эмпирического \mbox{риска}.

\subsection{Группа автоморфизмов графа смежности множества алгоритмов}

\emph{Графом смежности} множества алгоритмов $A$ назовем направленный ациклический граф $\,T(A) = (A, E)$,
вершины которого соответствуют алгоритмам из $A$, а ребро $(a_1, a_2) \in E$ соединяет пары алгоритмов,
чьи вектора ошибок отличаются только на одном объекте: $\rho(a_1, a_2) = 1$, причем число ошибок
алгоритма $a_2$ на единицу больше, чем у $a_1$.

\begin{Example}
Рассмотрим множество алгоритмов, заданное следующей матрицей ошибок:
\[
    \bordermatrix{& a_1 & a_2 & a_3 & a_4 & a_5 \cr
        x_1 & 0 & 1 & 1 & 0 & 0 \cr
        x_2 & 0 & 0 & 1 & 0 & 0 \cr
        x_3 & 0 & 0 & 0 & 1 & 1 \cr
        x_4 & 0 & 0 & 0 & 0 & 1 \cr
    }
\]
Данное множество алгоритмов мы будем называть унимодальной цепочкой.
Нетрудно убедиться, что граф смежности данного множества будет состоять из ребер
$E = \{(a_1, a_2), (a_2, a_3), (a_1, a_4), (a_4, a_5)\}$:
\[
\;\;\; \includegraphics[height=16mm]{unimod2.eps} \;\;\;
\]
\end{Example}

Заметим, что разным множествам алгоритмов могут соответствовать изоморфные графы смежности.

\begin{Example}
Рассмотрим множество алгоритмов, заданное следующей матрицей ошибок:
\[
    \bordermatrix{& a_1 & a_2 & a_3 & a_4 & a_5 \cr
        x_1 & 0 & 1 & 1 & 0 & 0 \cr
        x_2 & 0 & 0 & 1 & 0 & 1 \cr
        x_3 & 0 & 0 & 0 & 1 & 1 \cr
        x_4 & 0 & 0 & 0 & 0 & 0 \cr
    }
\]
Его граф смежности изоморфен графу унимодальной цепочки, рассмотренной в предыдущем примере.
\end{Example}

Таким образом, невозможно восстановить множество алгоритмов по его графу смежности.
Граф смежности сохраняет информацию только о близком соседстве алгоритмов.
Тем не менее граф смежности остается самым естественным способом визуализировать множество алгоритмов.

\begin{Def}
\label{def:connectionGraphAutomorphism}
Группой автоморфизмов графа смежности $T(A) = (A, E)$ множества алгоритмов A называют
максимальную подгруппу $Aut(T(A))$ группы перестановок вершин графа,
такую что каждый ее элемент $\pi \in Aut(T(A))$ удовлетворяет двум условиям:
\begin{itemize}
      \item Сохранение ребер графа и их ориентации:
      \begin{equation}
        (a_1, a_2) \in E \rightarrow (\pi a_1, \pi a_2) \in E;
      \end{equation}
      \item Сохранение числа ошибок алгоритмов:
      \begin{equation}
        n(a, \XX) = n (\pi a, \XX).
      \end{equation}
\end{itemize}
\end{Def}

Данное определение использует, помимо структуры графа, дополнительную информацию о числе ошибок алгоритмов.
Ниже мы покажем, что для широкого класса \emph{связных графов} группа автоморфизмов зависит только от
структуры самого графа.

\begin{Def} Граф смежности $T(A) = (A, E)$ назовем связным, если в соответствующем
неориентированном графе существует путь между каждой парой вершин:
для всех $a, a' \in A$ существует конечная последовательность вершин $\{a_i\}_{i = 1}^n$, такая что
$a = a_1$, $a' = a_n$, и для всех $i = 2, \dots, n$ одно из ребер
$(a_{i - 1}, a_i)$ или $(a_i, a_{i-1})$ лежит в $E$.
\end{Def}

\begin{Theorem}
Пусть граф смежности $T(A) = (A, E)$ связен. Тогда в определении
\ref{def:connectionGraphAutomorphism} условие $n(a, \XX) = n (\pi a, \XX)$ выполняется автоматически.
\end{Theorem}

$\square$ \textbf{Доказательство. }

Для краткости обозначений будем опускать аргумент $\XX$ у функционала числа ошибок:
$n(a, \XX) \equiv n(a)$.
Рассмотрим произвольную перестановку вершин графа $\pi$, такую что для всех ребер $(a, a') \in E$
выполнено $(\pi a, \pi a') \in E$. Покажем, что для всех $a \in A$ выполнено $n(a) = n (\pi a)$.


\textbf{Шаг 1.} Покажем, что перестановка $\pi$ сохраняет разности между числом ошибок любой пары алгоритмов:
$n(a') - n(a) = n(\pi a') - n(\pi a)$. Для этого рассмотрим путь $\{a_i\}_{i=1}^n$, который соединяет вершины $a = a_1$ и $a' = a_n$.

Пусть $\sigma \in \{0, 1\}$. Введем обозначение
\[
    (a_{i-1}, a_i)^\sigma =
    \begin{cases}
        & (a_{i-1}, a_i), \text{ при } \sigma = 0, \\
        & (a_i, a_{i-1}), \text{ при } \sigma = 1.
    \end{cases}
\]

Тогда, согласно определению связного графа, существует конечная последовательность $\{\sigma_i\}_{i=2}^n$,
такая что для всех $i = 2, \dots, n$ ребро $(a_{i-1}, a_i)^{\sigma_i} \in E$.
Непосредственной проверкой убеждаемся, что разность $n(a') - n(a)$ записывается в виде
\[
    n(a') - n(a) = \sum_{i=2}^n [\sigma_i = 0] - \sum_{i=2}^n [\sigma_i = 1].
\]

Заметим, что конечная последовательность $\{\pi a_i\}_{i=1}^n$ задает путь между вершинами
$\pi a = \pi a_1$ и $\pi a' = \pi a_n$, причем ребро $(\pi a_{i-1}, \pi a_i)^{\sigma _i} \in E$.
Значит, $n(\pi a') - n(\pi a)$ дается тем же выражением:
\[
    n(\pi a') - n(\pi a) = \sum_{i=2}^n [\sigma_i = 0] - \sum_{i=2}^n [\sigma_i = 1].
\]

\textbf{Шаг 2.}
Покажем, что существует алгоритм $a \in A$, такой что $n(\pi a) = n(a)$.
Рассмотрим алгоритм с минимальным и максимальным числом ошибок:
\[
    \begin{aligned}
        & a_{min} \in \Argmin_{a \in A} n(a), \\
        & a_{max} \in \Argmax_{a \in A} n(a). \\
    \end{aligned}
\]
Для них выполнено $n(a_{max}) - n(a_{min}) = n(\pi a_{max}) - n(\pi a_{min})$.
Выразим отсюда $n(\pi a_{max})$:
\[n(\pi a_{max}) = n(a_{max}) + n(\pi a_{min}) - n(a_{min}).\]
Разность $n(\pi a_{min}) - n(a_{min})$ неотрицательна, следовательно $n(\pi a_{max}) \geq n(a_{max})$.
Но с другой стороны $a_{max} \in \Argmax_{a \in A} n(a)$. Поэтому $n(\pi a_{max}) = n(a_{max})$.

Соединяя вместе результаты, полученные на первом и втором шаге, приходим к утверждению теоремы.
$\blacksquare$

Следующая теорема устанавливает связь между группой симметрии множества алгоритмов и
группой автоморфизмов соответствующего графа смежности.
В ряде случаев это утверждение помогает угадать орбиты действия группы симметрии $\Sym(A)$.

\begin{Theorem}
Орбиты действия группы симметрии $\Sym(A)$ на множестве алгоритмов $A$ вложены в орбиты действия
группы автоморфизмов $Aut(T(A))$ на $A$.
\end{Theorem}

$\square$ \textbf{Доказательство.}

Рассмотрим пару алгоритмов $a, a'$ из одной орбиты действия группы $\Sym(A)$.
Это значит, что существует $\pi \in \Sym(A)$, такая что $\pi a = a'$.
Каждый элемент группы $\Sym(A)$ действует на $A$, следовательно
можно рассматривать $\pi$ и как элемент группы перестановок вершин $A$.
Нам необходимо показать, что $\pi \in Aut(T(A))$.

Рассмотрим произвольное ребро $(a, a') \in E$. Рассматривая перестановку $\pi$
как элемент симметрической группы $S_L$,
применим леммы \ref{lem:errorCountInvariance} и \ref{lem:algDistanceInvariance} к алгоритмам $a, a'$. Получим, что $n(a, \XX) = n(\pi a, \XX)$, $n(a', \XX) = n(\pi a', \XX)$,
$\rho(\pi a, \pi a') = \rho (a, a') = 1$. Следовательно, $(\pi a, \pi a') \in E$,
а значит $\pi \in Aut(T(A))$.
$\blacksquare$

\subsection{Орбиты разбиений выборки}

Напомним, что вероятность переобучения для рандомизированного метода минимизации эмпирического
риска может быть записана следующим образом:
\[
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell} \sum_{\Xl \in \XXell}
    %\textbf{E}
    \sum_{a\in A(\Xl)}
        \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}{|A(\Xl)|},
\]
где $A(\Xl) = \Argmin_{a \in A} n(a, \Xl)$.

Заметим, что коэффициент $\frac {1}{|A(\Xl)|}$ не зависит алгоритма $a$,
и потому может быть вынесен за знак суммирования по $a \in A$:
\[
    Q_\mu(\epsilon, A)
    =
    \frac 1{C_L^\ell} \sum_{\Xl \in \XXell}
    %\textbf{E}
    \frac 1{|A(\Xl)|}
    \sum_{a\in A(\Xl)}
        \bigl[ \delta(a, \Xl) \geq \epsilon \bigr]
\]
Обозначим через $\Delta_A^{\,\epsilon}(\Xl) \subset A$ множество алгоритмов из $A$,
переобученных на разбиении $\Xl$:
$\Delta_A^{\,\epsilon}(\Xl) = \{a \in A \colon \delta(a, \Xl) \geq \epsilon \}$.

Тогда
\begin{equation}
    \label{def:overfitWithoutAlgs}
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell} \sum_{\Xl \in \XXell}
    %\textbf{E}
    \frac { | \Delta_{A(\Xl)}^{\,\epsilon} (\Xl) | }{|A(\Xl)|}.
\end{equation}

В этой интерпретации вероятность переобучения "--- это усредненная по всем разбиениям доля
переобученных алгоритмов в $A(\Xl)$.

\begin{Lemma}
\label{lem:overfittingDependence}
В множестве $A(\Xl)$ переобученными на разбиении $\Xl$ являются
те и только те алгоритмы, у которых число ошибок на полной выборке
не меньше порога $m_\delta(A, \Xl) = \epsilon k + \frac L \ell \min \limits_{a \in A} n(a, \Xl)$.
\end{Lemma}
$\square$ \textbf{Доказательство.}

По определению $A(\Xl)$ все алгоритмы $a \in A(\Xl)$ имеют равное число ошибок на обучении.
Обозначим это число через $n_0 = \min \limits_{a \in A} n(a, \Xl)$.
Тогда $\delta(a, \Xl) = \frac{n(a, \XX) - n_0}{k} - \frac{n_0}\ell$,
и неравенство $\delta(a, \Xl) \geq \epsilon$ можно записать как
$n(a, \XX) \geq \epsilon k + \frac L \ell \min \limits_{a \in A} n(a, \Xl) \equiv m_\delta(A, \Xl)$.
Это число означает минимальное количество ошибок на полной выборке, начиная с которого
алгоритмы из $A(\Xl)$ будут переобученными.
$\blacksquare$

Напомним, что определенная выше группа симметрий $\Sym(A)$ являлась подгруппой в $S_L$,
и потому действовала на $\XXell$.
Обозначим орбиты этого действия через $\Omega(\XXell)$.
Произвольного представителя орбиты $\tau \in \Omega(\XXell)$ обозначим через $\Xl_\tau$.

Важно отметить, что идея рассматривать орбиты действия группы симметрий на множестве разбиений
выборки впервые упоминается И. Толстихиным в \cite{tolstikhin10diplom}.
В частности, там была доказана следующая теорема:
\begin{Theorem}
\[
    Q_\mu(\epsilon, A) =
        \frac 1{C_L^\ell}\sum_{\omega \in \Omega(A)} |\omega|
        \sum_{\tau \in \Omega(\XXell)} |\{X \in \tau \colon a_\omega \in A(\Xl)\}|
        \frac{[\delta(a_\omega, X_\tau) \geq \epsilon]} {|A(\Xl)|}.
\]
\end{Theorem}
Ниже доказывается аналогичная теорем, в котором удалось избавиться
от суммирования по орбитам алгоритмов $\omega \in \Omega(A)$.

\begin{Theorem}
\label{th:equalContribution_Sample}
Вероятность переобучения для рандомизированного метода минимизации
эмпирического риска можно записать в виде
\begin{equation}
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell} \sum_{\tau \in \Omega(\XXell)}
    %\textbf{E}
    |\tau|
    \frac { | \Delta_{A(\Xl)}^{\,\epsilon} (\Xl) | }{|A(\Xl)|}.
\end{equation}
\end{Theorem}

$\square$ \textbf{Доказательство.}

Достаточно показать, что для каждой $\pi \in \Sym(A)$ и разбиения $\Xl \in \XXell$ выполнено
$|A(\Xl)| = |A(\pi \Xl)|$ и $|\Delta_{A(\Xl)}^{\,\epsilon} (\Xl)| = |\Delta_{A(\pi \Xl)}^{\,\epsilon} (\pi \Xl)|$.

Для доказательства обоих тождеств нам понадобится лемма \ref{lem:AXinvariance} о инвариантности
множества $A(\Xl)$ к действию $\pi \in S_L$. Данная лемма утверждает, что
$ a_0 \in A(\Xl) \Leftrightarrow \pi a_0 \in (\pi A)(\pi \Xl)$.
Для $\pi \in \Sym(A)$ выполнено $\pi A = A$, значит
$a_0 \in A(\Xl) \Leftrightarrow \pi a_0 \in A(\pi \Xl)$.
Это устанавливает взаимно-однозначное соответствие между множествами $A(\Xl)$ и $A(\pi \Xl)$.
Следовательно $|A(\Xl)| = |A(\pi \Xl)|$.

Пусть $a_0 \in \Delta_{A(\Xl)}^{\,\epsilon} (\Xl)$. Тогда $\pi a_0 \in A(\pi \Xl)$.
Согласно лемме \ref{lem:errorCountInvariance} имеем $n(a_0, \XX) = n(\pi a_0, \XX)$.
Воспользовавшись леммой \ref{lem:overfittingDependence} получаем, что
$\pi a_0 \in \Delta_{A(\pi \Xl)}^{\,\epsilon} (\pi \Xl)$.
Тем самым установлено соответствие между множествами $\Delta_{A(\Xl)}^{\,\epsilon} (\Xl)$ и
$\Delta_{A(\pi \Xl)}^{\,\epsilon} (\pi \Xl)$.
Данное соответствие является взаимно-однозначным, поскольку в группе $\Sym(A)$ существует обратный
элемент $\pi^{-1}$.
Следовательно, мощности рассматриваемых множеств совпадают.
$\blacksquare$

Завершая параграф, интересно рассмотреть специфический случай, когда все алгоритмы из $A$
имеют равное число ошибок на полной выборке.

\begin{Corollary}
\label{crl:easyFormula}
Пусть все $a \in A$ имеют равное число ошибок на полной выборке: $n(a, \XX) = m$.
Тогда вероятность переобучения рандомизированного метода минимизации эмпирического риска записывается в виде
\begin{equation}
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell} \sum_{\tau \in \Omega(\XXell)}
    %\textbf{E}
    |\tau| \left[\min_{a \in A} n(a, X_\tau) \leq \frac \ell L(m - \epsilon k)\right].
\end{equation}
\end{Corollary}

\section{Точные оценки вероятности переобучения}

В данном параграфе будут получены явные комбинаторные формулы
для функционала $Q_\mu(\epsilon, A)$ для некоторых множеств алгоритмов $A$,
обладающих свойством симметрии.

\subsection{Полный слой алгоритмов}

\emph{Полным $m$-слоем} алгоритмов будем называть множество,
состоящее из~всех алгоритмов $a\in\AA$ с~фиксированным числом ошибок:
$n(a, \XX) = m$.

\begin{Theorem}
\label{th:fullSliceFormula}
При обучении рандомизированным методом минимизации эмпирического риска вероятность
переобучения для полного $m$-слоя алгоритмов есть
\begin{equation}
    \label{formula:fullSliceFormula}
    Q_\mu(\epsilon, A)
    =
    \bigl[
        \epsilon k \leq m \leq L - \epsilon \ell
    \bigr].
\end{equation}
\end{Theorem}

$\square$ \textbf{Доказательство.}

В рассматриваемом случае группой симметрии $\Sym(A)$ будет вся симметрическая группа $S_L$.
Следовательно, действие группы симметрии на~множестве алгоритмов транзитивно, и
в рассматриваемом множестве есть только один класс из~$C_L^m$ идентичных алгоритмов.
Согласно теореме \ref{th:equalContributionERM} запишем:
\[
    Q_\mu(\epsilon, A) =
%        \frac {C_L^m}{C_L^\ell}
%        \sum_{\substack{
%                \Xl \in \XXell \,\colon \\
%                a_0 \in  A(\Xl)
%        }}
        C_L^m
        \mathbf{E}
        \frac {\bigl[ a_0 \in  A(\Xl) \bigr]}{|A(\Xl)|} \left[ \delta(a_0, \Xl) \geq \epsilon \right].
\]
где $a_0$ "--- произвольный алгоритм рассматриваемого семейства.

Алгоритм $a_0$ будет выбран только если он~имеет минимальное число ошибок на~обучении.
Рассмотрим два случая.

Случай 1, $m \leq k$. Все ошибки $a_0$ помещаются в~контроль,
и переобучение наступает при условии $m \geq \epsilon k$.
Этим фиксируются $m$ объектов контроля, следовательно число слагаемых в~сумме по~разбиениям $\Xl$
определяется числом способов выбрать $k-m$ объектов, на~которых алгоритм $a_0$ не~ошибается.
Это число равно $C_{L-m}^{k-m}$.

Мощность множества лучших на~обучении алгоритмов $A(\Xl)$
не~зависит от~$\Xl$ и~равна $C_k^m$ "---
числу способов расставить $m$ ошибок алгоритма
на $k$ позициях контрольной выборки. Таким образом,
\[
    Q_\mu(\epsilon, A) =
        \frac{C_L^m}{C_L^\ell}
        \frac{C_{L-m}^{k-m}}{C_k^m}
        \bigl[ m \geq \epsilon k\bigr],
        \text { при } m \leq k.
\]

Случай 2, $m > k$. Контрольная выборка должна содержать только объекты, на~которых $a_0$ ошибается.
Тогда в~обучении останется $m-k$ ошибок, а~условие переобучения примет вид
$1 - \frac{m-k}{\ell} \geq \epsilon$, откуда $m \leq L - \epsilon \ell$.

Число разбиений выборки, при которых $a_0 \in A(\Xl)$, равно $C_m^k$ "---
числу способов выбрать $k$ ошибок алгоритма $a_0$ в~контрольную выборку.
Мощность множества $A(\Xl)$ вновь не~зависит от~$\Xl$, и~равна $C_\ell^{m-k}$ "---
числу способов отобрать $m-k$ ошибок в~обучающую выборку.
\[
    Q_\mu(\epsilon, A) =
        \frac{C_L^m}{C_L^\ell}
        \frac{C_{\ell}^{m-k}}{C_m^k}
        \bigl[m \leq L - \epsilon \ell\bigr],
        \text { при } m > k.
\]

Записав для каждого комбинаторного коэффициента тождество
$C_L^k = \frac {L!}{k!(L-k)!}$,
убеждаемся, что в~обеих формулах комбинаторные множители равны единице.
Соединяя вместе условия
$\epsilon k \leq m \leq k$ и~$k < m \leq L =\epsilon \ell$,
получаем утверждение теоремы.
$\blacksquare$

Заметим, что данное доказательство можно было существенно упростить,
сославшись на следствие \ref{crl:easyFormula}.

\subsection{Куб алгоритмов}
Кубом алгоритмов $\AA$ называется множество, содержащее все возможные $a \in \{0, 1\}^L$.

\begin{Theorem}
Вероятность переобучения для куба алгоритмов дается формулой:
\[
    Q_\mu(\epsilon, \AA) =
    \frac 1{2^k}
    \sum_{m = \lceil \epsilon k \rceil}^k
    C_k^m.
\]
\end{Theorem}

$\square$ \textbf{Доказательство.}

Очевидно, что в~данном случае группа симметрии "--- это вся $S_L$.
Тогда орбитами ее действия будут слои алгоритмов с~одинаковым числом ошибок.
Поэтому, согласно теореме~\ref{th:equalContributionERM},
\[
    Q_\mu(\epsilon, \AA) =
        %\frac 1{C_L^\ell}
        \sum_{m=0}^L C_L^m
%        \sum_{\substack{
%                \Xl \in \XXell \,\colon \\
%                a_m \in A(\Xl)
%        }}
        \mathbf{E}
        \frac {\bigl[ a_m \in A(\Xl) \bigr]}{|A(\Xl)|} \left[ \delta(a, \Xl) \geq \epsilon \right].
\]

Алгоритм может быть выбран в~результате обучения только в~том случае, когда
он не~допускает ошибок на~обучении. Поэтому все его ошибки должны помещаться в~контрольную
выборку, значит можно ограничить индекс суммирования $m \leq k$.

Раз все ошибки выбранного алгоритма расположены в~контрольной выборке, то,~вне зависимости от
разбиения, уклонение частот равно $\delta(a, \Xl) = \frac mk$.
Следовательно, переобучение наступает при $m \geq \lceil \epsilon k \rceil$.

В множестве $A(\Xl)$ всегда $2^k$ алгоритмов. Это алгоритмы с~нулевым числом ошибок на~обучении
и~всеми возможными векторами ошибок на~контрольной выборке.

Собирая вместе установленные выше факты, получаем формулу
\[
    Q_\mu(\epsilon, \AA) =
        %\frac 1{C_L^\ell}
        \sum_{m = \lceil \epsilon k \rceil}^k C_L^m
%        \sum_{\substack{
%                \Xl \in \XXell \,\colon \\
%                a_m \in A(\Xl)
%        }}
        \frac {\mathbf{E}\bigl[ a_m \in A(\Xl) \bigr]}{2^k}.
\]

Осталось вычислить число разбиений, на~которых алгоритм $a_m$ будет выбран методом обучения.
Этих разбиений столько, сколько способов выбрать $\ell$ объектов обучающей выборки из
$L-m$ правильных ответов алгоритма $a_m$. Итого получаем
\[
    Q_\mu(\epsilon, \AA) =
        \sum_{m = \lceil \epsilon k \rceil}^k C_L^m
        \frac {C_{L-m}^\ell}{C_L^\ell 2^k}
    =
    \frac 1{2^k}
    \sum_{m = \lceil \epsilon k \rceil}^k
    C_k^m.
    \;\;\; \blacksquare
\]

\subsection{Шар алгоритмов}

Напомним, что под расстоянием между алгоритмами $\rho(a, a')$
подразумевается расстояние Хэмминга между их векторами ошибок:
$\rho(a, a') = \sum \limits_{x \in \XX} |a(x) - a'(x)|$.

\begin{Def}
Шаром алгоритмов $B_{r}(a_0)$ радиуса $r$ назовем множество, заданное условием
$B_{r}(a_0) = \{a \in \AA \colon \rho(a, a_0) \leq r\}$.
Алгоритм $a_0$ будем называть \emph{центром шара}.
\end{Def}

Число ошибок центра шара будем обозначать через $m$. В дальнейшем рассматриваются
только такие шары, для которых $r \leq m$.
Введем обозначения $X^m = \{x \in \XX \colon a_0(x) = 1\}$ для множество объектов, на которых ошибается алгоритм $a_0$,
и $X^{L-m} = \{x \in \XX \colon a_0(x) = 0\}$ для объектов, на которых $a_0$ не ошибается.

Шары алгоритмов были впервые изучены И. Толстихиным в \cite{tolstikhin10diplom}.
В частности, были получены точные формулы вероятности переобучения
для рандомизированного метода минимизации эмпирического риска.
При этом использовалась техника рассмотренная выше техника
орбит алгоритмов под действием группы симметрии $\Sym(A)$.
Ниже мы получим аналогичную формулу более простым способом, с использованием
теоремы \ref{th:equalContribution_Sample}.
При этом будет использован ряд утверждений, доказанных в \cite{tolstikhin10diplom}.

\begin{Lemma}
\label{lem:ballSyms}
Группа $S_m \times S_{L-m}$, где $S_m$ и $S_{L_m}$ "--- симметрические группы перестановок,
действующие на множествах $X^m$ и $X^{L-m}$ соответственно, является подгруппой группы
симметрий множества алгоритмов $B_{r}(a_0)$.
\end{Lemma}

\begin{Lemma}
\label{lem:ballUpperAlgs}
Пусть $\Xl \in \XXell$, $a \in A$, $n(a, \Xl) > m - r$.
Тогда \[a \in A(\Xl) \Leftrightarrow n(a, \Xl) = 0.\]
\end{Lemma}

\begin{Lemma}
\label{lem:ballLowestAlgs}
Пусть $\Xl \in \XXell$, $a \in A$, $n(a, \Xl) = m - r$.
Обозначим $i = |\Xl \cap X^m|$. Тогда
\[
    a \in A(\Xl) \Leftrightarrow
    \begin{cases}
        & n(a, \Xl) = 0, \text{ при } i \leq r; \\
        & n(a, \Xl) = i - r, \text { при } i \geq r.
    \end{cases}
\]
\end{Lemma}

Теперь, воспользовавшись \ref{th:equalContribution_Sample},
выведем явную формулу для вероятности переобучения шара алгоритмов.

\begin{Lemma}
\label{lem:ballOrbitSize}
Орбиты $\tau \in \Omega(\XXell)$ индексированы параметром $i = |\Xl \cap X^m|$.
Мощность орбиты $\tau_i$ записывается в виде $|\tau_i| = C_L^\ell h_L^{\ell, m}(i)$.
\end{Lemma}

$\square$ \textbf{Доказательство.}

Первая утверждение леммы непосредственно следует из строения подгруппы симметрий,
отмеченного в \ref{lem:ballSyms}.

Мощность орбиты $\tau_i$ определяется числом способов независимо выбрать $i$ объектов из $X^m$ и
$\ell - i$ объектов из $X^{L-m}$. Таким образом $|\tau_i| = C_L^\ell h_L^{\ell, m}(i)$.
$\blacksquare$

\begin{Theorem}
Пусть $A = B_r(a_0)$ "--- шар алгоритмов, $m = n(a_0, \XX)$, $r \leq \min(m, L-m)$.
Тогда вероятность переобучения рандомизированного метода минимизации
эмпирического риска записывается в виде
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{r}
        h_L^{\ell, m}(i)
        \frac {
            \sum \limits_{p=0}^{r - i}
            \sum \limits_{q = q_0}^{r-i-p}
            C_{m-i}^p C_{k-(m-i)}^q
        } {
            \sum \limits_{p=0}^{r-i}C_k^p
        } +
    \sum_{i=r + 1}^{\lfloor s'_d(\epsilon) \rfloor}
        h_L^{\ell, m}(i),
\]
где
$q_0 = \max(\lceil \epsilon k + i + p - m \rceil, 0)$,
$s'_d(\epsilon) = \frac {\ell}{L} (m - \epsilon k ) + \frac{r k}{L}$,
$h_L^{\ell, m}(i) = \frac{C_m^i C_{L-m}^{\ell - i}}{C_L^\ell}$.
\end{Theorem}

$\square$ \textbf{Доказательство.}

Объединяя формулу теоремы \ref{th:equalContribution_Sample} и утверждение
о строении орбит $\Omega(\XXell)$ из леммы \ref{lem:ballOrbitSize} получим
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^m
    h_L^{\ell, m}(i) \frac { | \Delta_{A(\Xl_i)}^{\,\epsilon} (\Xl_i) | }{|A(\Xl_i)|}.
\]
Рассмотрим два случая: $i \leq r$ и $i > r$.

\textbf{Случай} $i \leq r$.
Тогда из лемм \ref{lem:ballLowestAlgs} и \ref{lem:ballUpperAlgs} следует,
что в $A(\Xl_i)$ будут те и только те алгоритмы, у которых нет ошибок на обучении.
Следовательно, число алгоритм в $a \in A(\Xl_i)$ определяется количеством способов выбрать
не более $r-i$ объектов из контрольной выборки $\Xk_i$, на которых алгоритм $a$
будет отличаться от центра шара $a_0$:
\[
    |A(\Xl_i)| = \sum \limits_{p=0}^{r-i}C_k^p.
\]
Подсчитаем количество переобученных алгоритмов из $A(\Xl)$.
Для этого заметим, что $\Delta_{A(\Xl_i)}^{\,\epsilon}(\Xl_i) = \{a \in A(\Xl) \colon n(a, \XX) \geq \epsilon k\}$.
Пусть $a \in \Delta_{A(\Xl_i)}^{\,\epsilon}(\Xl_i)$ отличается от центра шара $a_0$ на
$p$ объектах из множества $X^m \cap \Xk$, и на $q$ объектах из множества $X^{L-m} \cap \Xk$. Тогда
условие $a \in A$ запишется как $p + q + i \leq r$, а условие переобученности $a$ "--- как $m - i - p + q \geq \epsilon k$.
Следовательно
\[
\begin{aligned}
    |\Delta_{A(\Xl_i)}^{\,\epsilon}(\Xl_i)|
    & = \sum_{p=0}^{r - i} \sum_{q = 0}^{r-i-p}
        [m-i-p+q \geq \epsilon k]
        C_{m-i}^p C_{L-m-(\ell - i)}^q = \\
    & = \sum_{p=0}^{r - i} \sum_{q = q_0}^{r-i-p}
        C_{m-i}^p C_{k-(m-i)}^q,
\end{aligned}
\]
где $q_0 = \max(\lceil \epsilon k + i + p - m \rceil, 0)$.

\textbf{Случай} $i > r$. Из лемм \ref{lem:ballLowestAlgs} и \ref{lem:ballUpperAlgs} следует,
что в $A(\Xl)$ попадут только алгоритмы с числом ошибок на полной выборке $m - r$
и числом ошибок на обучении $i - r$.
Значит
$\frac{|\Delta_{A(\Xl_i)}^{\,\epsilon}(\Xl_i)|}{|A(\Xl)|} =
 [\frac{m-i}{k} - \frac{i-r}{\ell} \geq \epsilon] =
 [i \leq s'_d(\epsilon)]$,
где $s'_d(\epsilon) \equiv \frac {\ell}{L} (m - \epsilon k ) + \frac{r k}{L}$.

Собирая вместе полученные результаты, получаем
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{r}
        h_L^{\ell, m}(i)
        \frac {
            \sum \limits_{p=0}^{r - i}
            \sum \limits_{q = q_0}^{r-i-p}
            C_{m-i}^p C_{k-(m-i)}^q
        } {
            \sum \limits_{p=0}^{r-i}C_k^p
        } +
    \sum_{i=r + 1}^{\lfloor s'_d(\epsilon) \rfloor}
        h_L^{\ell, m}(i).\;\;\;\blacksquare
\]

В заключении параграфа воспользуемся следствием \ref{crl:easyFormula}
для вывода вероятности переобучения множества алгоритмов, получаемого
сечением шара алгоритмов центральным слоем: $A = \{a \in  B_r(a_0) \colon n(a, \XX) = m\}$.

\begin{Theorem}
Вероятность переобучения множества алгоритмов, получаемого сечением
шара алгоритмов центральным $m$-слоем, дается в виде
\[
    Q_\mu(\epsilon, A)
    =
    H_L^{\ell, m}(s_d(\epsilon) + \big\lfloor r/2 \big\rfloor),
\]
где $s_d(\epsilon) = \frac \ell L (m - \epsilon k)$,
$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}

$\square$ \textbf{Доказательство.}

Заметим, что утверждение лемм \ref{lem:ballSyms} и \ref{lem:ballOrbitSize} верно и для
сечения шара центральной плоскостью.
Воспользовавшись следствием \ref{crl:easyFormula} получим
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{m}
    h_L^{\ell, m}(i) \left[\min_{a \in A} n(a, X_i) \leq \frac \ell L(m - \epsilon k)\right].
\]
Напомним, что по определению $i = |\Xl \cap X^m|$. Пусть $r' = \big\lfloor \frac r 2 \big\rfloor$.
Тогда
\[
    \min_{a \in A} n(a, X_i) =
    \begin{cases}
        & 0, \text{ при } i \leq r', \\
        & i - r', \text{ при } i > r'.
    \end{cases}
\]
Следовательно
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{\lfloor s_d(\epsilon)\rfloor + r'}
    h_L^{\ell, m}(i) = H_L^{\ell, m}(s_d(\epsilon) + \big\lfloor r/2 \big\rfloor),
\]
где $s_d(\epsilon) = \frac \ell L (m - \epsilon k)$. $\blacksquare$

\subsection{Унимодальная цепочка}

Напомним, что расстояние между алгоритмами $\rho(a, a')$ определялось как
расстояние Хэмминга между их векторами ошибок:
\[
    \rho(a, a') = \sum_{x \in \XX} |a(x) - a'(x)|.
\]

\begin{Def}
Множество алгоритмов $\{a_0, \dots, a_D\}$ называется
\emph{унимодальной цепочкой}, если выполнены два условия:
\begin{itemize}
    \item[1)]
        монотонность числа ошибок:
        $n(a_i, \XX) = m + i$,\; $i = 0, \dots, D$ при некотором фиксированном~$m$;
    \item[2)]
        поглощение ошибок предыдущего алгоритма:
        $\rho(a_{i}, a_{i-1}) = 1$,\; $i = 1, \dots, D$.
\end{itemize}
\end{Def}
Таким образом, в монотонной цепочке каждый следующий алгоритм ошибается на тех же объектах,
что и предыдущий, и допускает еще одну дополнительную ошибку.

Монотонная цепочка алгоритмов "--- это простейшая модель однопараметрического \emph{связного семейства алгоритмов},
предполагающая, что при непрерывном удалении некоторого параметра от оптимального значения число ошибок на полной выборке только увеличивается.

\begin{Def}
Множество алгоритмов $\{a_0, a_1, \dots, a_D, a'_1, \dots, a'_D\}$ называется
унимодальной цепочкой, если выполнены два условия:
\begin{itemize}
  \item[1)]  левая ветвь $\{a_0, a_1, \dots, a_D\}$ и
        правая ветвь $\{a_0, a'_1, \dots, a'_D\}$
        являются монотонными цепочками.
  \item[2)]  пересечение множества ошибок алгоритмов $a_D$ и $a'_D$ равно
        множеству ошибок алгоритма $a_0$.
\end{itemize}
\end{Def}

Унимодальная цепочка является более реалистичной моделью однопараметрического \emph{связного семейства},
по сравнению с монотонной цепочкой. Если мы имеем лучший алгоритм $a_0$ c оптимальным значением
некоторого вещественного параметра, то отклонение значения этого параметра как в~б\'ольшую, так и в меньшую,
сторону приводит к~увеличению числа ошибок.

\begin{Theorem}
    \label{th:unimodalChain}
    Для унимодальной цепочки с~ветвями длины~$D$ вероятность переобучения
    рандомизированного метода минимизации эмпирического риска равна
    \begin{equation}
        \label{formula:unimodalChain}
        Q_\mu(\epsilon, A) =
            %\frac{1}{C_L^\ell}
            \sum_{h=0}^{D} \sum_{t_1=h}^D \sum_{t_2 = 0}^{D}
            \frac{|\omega_h|}{1 + t_1 + t_2}
            \frac{C_{L'}^{\ell'}}{C_L^\ell}
            H_{L'}^{\ell', m}(s(\epsilon)),
    \end{equation}
    где
        $L' = L {-} t_1 {-} t_2 {-} F$,\;
        $F = [t_1{\neq}D] + [t_2{\neq}D]$,\;
        $\ell' = \ell {-} F$,\;
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m {+} h {-} \epsilon k) \bigr\rfloor$;\;
        $|\omega_h| = 1$ при $h = 0$ и~$|\omega_h| = 2$ при ${h \geq 1}$;\;
        $H_{L'}^{\ell', m}(z) = \frac {1}{C_{L'}^{\ell'}}\sum \limits_{s=0}^{\lfloor z \rfloor} C_m^s C_{L'-m}^{\ell' - s}$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}
$\square$ \textbf{Доказательство.}

Пронумеруем объекты генеральной выборки $\XX$ таким образом, как показано в следующей таблице:
\[
    \bordermatrix{
         & a_0 & a_1 & a_2 & \cdots & a_D & a'_1 & a'_2 & \cdots & a'_D \cr
    x_1  & 0 & 1 & 1 & \cdots & 1 & 0 & 0 & \cdots & 0 \cr
    x_2  & 0 & 0 & 1& \cdots  & 1 & 0 & 0& \cdots  & 0 \cr
         & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots& \cdots  & \cdots \cr
    x_D  & 0 & 0 & 0 & \cdots & 1 & 0 & 0& \cdots  & 0 \vspace{-1.5ex}\cr\cline{2-10}
    x'_1 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 & \cdots & 1 \cr
    x'_2 & 0 & 0 & 0 & \cdots & 0 & 0 & 1& \cdots  & 1 \cr
         & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots& \cdots  & \cdots \cr
    x'_D & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 1 \cr
    }
\]

Перестановками объектов выборки
(${x_1{\,\leftrightarrow\,}x'_1}, \dots, x_D{\,\leftrightarrow\,}x'_D$)
можно поменять левую и~правую ветви местами.
Поэтому идентичные алгоритмы в~унимодальной цепочке "---
это пары алгоритмов с~равным числом ошибок на~полной выборке.

Согласно теореме \ref{th:equalContributionERM} вероятность переобучения записывается в виде:
\[
    Q_\mu(\epsilon, A) = \sum_{h=0}^{D} |\omega_h| \sum_{t_1=h}^D \sum_{t_2 = 0}^{D} \frac1{C_L^\ell} \sum_{\Xl \in N(t_1, t_2)}
                    \frac 1{|A(\Xl)|} \left[ \delta(a_h, \Xl) \geq \epsilon \right].
\]

Здесь индекс $h$ обозначает номер класса идентичных алгоритмов
(таким образом, что все алгоритмы класса $\omega_h$ имеют $m+h$ ошибок);
$|\omega_0| = 1$, и~$|\omega_h| = 2$ при $h \geq 1$.
Для определенности будем брать представителя $a_h$ класса $\omega_h$ из левой ветви цепочки.

Индексы $t_1$ и~$t_2$ параметризуют состав множества $A(\Xl)$.
Для произвольного разбиения $\Xl \in \XXell$ определим $t_1$ как максимальное число,
для которого все объекты $x_1, x_2, \dots, x_t$ находятся в~контроле, а~$x_{t_1+1}$ (при его наличии) --- в~обучении.
Индекс $t_2$ определяется аналогично для объектов правой ветви.
Множество $N(t_1, t_2) \subset \XXell$ есть множество всех разбиений выборки с параметрами $t_1$ и $t_2$.

Из определения $t_1$ и~$t_2$ следует, что $|A(\Xl)| = \frac{1}{1 + t_1 + t_2}$.
Индексы $t_1$ и~$t_2$ при суммировании пробегают разные множества значений,
поскольку рассматриваются только разбиения, при которых выбранный из левой ветви представитель $a_h$
лежит в $A(\Xl)$.

Обозначим $F = [t_1{\neq}D] + [t_2{\neq}D]$, $L' = L {-} t_1 {-} t_2 {-} F$, $\ell' = \ell - F$.
Параметр $F$ позволяет учитывать вклад последних алгоритмов $a_D$ и $a'_D$ цепочки.

Вычислим мощность подмножества тех разбиений из $N(t_1, t_2)$,
на которых алгоритм $a_h$ оказывается переобученным.
Пусть $s_0(\epsilon)$ --- максимальное число ошибок на~обучении,
при котором наблюдается переобучение.
По определению уклонения частот находим
$s_0(\epsilon) = \lfloor \frac{\ell}L (m + h - \epsilon k) \rfloor$.
Нам необходимо из~$L'$ объектов выбрать $\ell'$ для обучения таким образом,
что бы из~$m$ свободных ошибок алгоритма $a_h$ в~обучении оказалось не~более $s_0(\epsilon)$ ошибок.
Это число способов дается выражением
$   %H_{L'}^{\ell',\, m}(s_0(\epsilon))
    %=
    \sum \limits_{s = 0}^{s_0(\epsilon)}
    C_{m}^{s}C_{L'-m}^{\ell'-s}$.

Собирая все результаты, приходим к~окончательной формуле:
\[
    Q_\mu(\epsilon, A) =
        \sum_{h=0}^{D} |\omega_h| \sum_{t_1=h}^D \sum_{t_2 = 0}^{D}
        \frac{1}{1 + t_1 + t_2} \frac {C_{L'}^{\ell'} } {C_L^\ell} H_{L'}^{\ell', \, m}(s_0(\epsilon)). \;\;\blacksquare
\]

\subsection{Связка из монотонных цепочек}

\emph{Связкой из~$p$ монотонных цепочек} называется множество алгоритмов,
полученное объединением $p$ монотонных цепочек равной длины,
с~общим первым алгоритмом. Как и в случае унимодальной цепочки, предполагается,
что множества объектов, на которых ошибаются алгоритмы ветвей, не~пересекаются.

Группа симметрии связки~из~$p$ монотонных цепочек является
симметрической группой $S_p$, действующей на~ветви связки всевозможными перестановками.
Таким образом, классы идентичных алгоритмов
"--- это подмножества алгоритмов с~одинаковым числом ошибок на~полной выборке,
называемые \emph{слоями}~\cite{voron09mmro}.

В следующей теореме будет дана явная формула вероятности переобучения
для связки из~$p$~монотонных цепочек.
Введём \emph{комбинаторный коэффициент}
$R_{D,p}^h(S,F)$, который зависит от~параметров $S$~и~$F$,
от числа монотонных цепочек~$p$ и~от их длины~$D$,
а также от~$h$ "--- минимального значения параметра~$S$.
Коэффициент $R_{D,p}^h(S,F)$ равен числу способов представить число~$S$
в виде суммы $p$~неотрицательных слагаемых, $S = t_1+ \ldots + t_p$,
каждое из~которых не~превосходит~$D$.
При этом ровно $F$~слагаемых не~должно равняться~$D$,
а на~первое слагаемое накладывается дополнительное ограничение $t_1 \geq h$.

\begin{Theorem}
\label{th:pMonot}
    Пусть в~связке из~$p$ монотонных цепочек
    лучший алгоритм допускает $m$~ошибок на~полной выборке,
    длина каждой ветви без учета лучшего алгоритма равна~$D$.
    \mbox{Тогда} при обучении рандомизированным методом
    вероятность переобучения может быть записана в~виде:
    \begin{equation}
    \label{formula:pChainsUnion}
            Q_\mu(\epsilon, A) =
                \sum_{h=0}^{D}
                \sum_{S=h}^{p D}
                \sum_{F = 0}^{p}
                \frac{|\omega_h| R_{D, p}^h(S, F)}{1 + S}
                \frac{C_{L'}^{\ell'}}{C_L^\ell}
                H_{L'}^{\ell', m}(s(\epsilon)),
    \end{equation}
    где
        $L' = L {-} S {-} F$,\;
        $\ell' = \ell {-} F$,\;
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m {+} h {-} \epsilon k) \bigr\rfloor$;\;
        $|\omega_h| = 1$ при $h = 0$ и~$|\omega_h| = p$ при ${h \geq 1}$;\;
        $H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}


\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=84mm,height=63mm]{frey_monot.eps}
    \hfill
    \caption{Зависимость $Q_\mu(\epsilon, A)$ от~$\epsilon$ для монотонной цепочки при $L=100$, $\ell=60$, $D=40$, $m=20$.}
    \label{fig:Monot}
    \medskip
    \hfill
    \includegraphics[width=84mm,height=63mm]{frey_unit.eps}
    \hfill
    \caption{Зависимость $Q_\mu(\epsilon, A)$ от~$\epsilon$ для единичной окрестности при $L=100$, $\ell=60$, $p=10$, $m=20$.}
    \label{fig:Unit}
    \end {multicols}
\end{figure}

$\square$ \textbf{Доказательство.}
Естественным образом обобщая рассуждения, приведенные для унимодальной цепочки, получаем формулу
\[
    Q_\mu(\epsilon, A) =
        \sum_{h=0}^{D} |\omega_h| \sum_{t_1=h}^D \sum_{t_2 = 0}^{D} \ldots \sum_{t_p = 0}^{D}
        \frac{1}{1 + t_1 + t_2 + \ldots + t_p} \frac {C_{L'}^{\ell'}} {C_L^\ell} H_{L'}^{\ell', m}(s(\epsilon)),
\]
где
$L' = L - \sum\limits_{i=1}^{p}t_i - \sum\limits_{i=1}^{p}[t_i \neq D]$,\;
$\ell' = \ell - \sum\limits_{i=1}^{p}[t_i \neq D]$,\;
$s_0(\epsilon) = \lfloor \frac{\ell}L (m + h - \epsilon k) \rfloor$.

Упростим запись, введя дополнительные обозначения
${S = \sum\limits_{i=1}^{p}t_i}$,\;
${F = \sum\limits_{i=1}^{p}[t_i \neq D]}$.
Параметр~$S$ определяет мощность множества $A(\Xl)$.
\[
    Q_\mu(\epsilon, A) =
        %\frac{1}{C_L^\ell}
        \sum_{h=0}^{D} |\omega_h| \sum_{t_1=h}^D \sum_{t_2 = 0}^{D} \ldots \sum_{t_p = 0}^{D}
        \frac{1}{1 + S}
        \frac {C_{L'}^{\ell'}}{C_L^\ell}
        H_{L'}^{\ell', m}(s_0(\epsilon)),
\]
где
$L' = L - S - F$,\;
$\ell' = \ell - F$,\;
$s_0(\epsilon) = \lfloor \frac{\ell}L (m + h - \epsilon k) \rfloor$.

Теперь от суммирования по параметрам $t_i$
можно перейти к~суммированию по~множеству возможных значений $S$~и~$F$:
\[
    Q_\mu(\epsilon, A) =
        %\frac{1}{C_L^\ell}
        \sum_{h=0}^{D} |\omega_h| \sum_{S=h}^{p D} \sum_{F = 0}^{p}
        \frac{R_{D,p}^h(S, F)}{1 + S}
        \frac {C_{L'}^{\ell'}}{C_L^\ell}
        H_{L'}^{\ell', m}(s_0(\epsilon)),
\]
где $R_{D,p}^h(S, F)$ "--- определенный выше комбинаторный коэффициент. $\blacksquare$

Связка из~$2p$ монотонных цепочек является
моделью $p$"~параметрического семейства алгоритмов,
в~котором разрешено изменять любой из~$p$ параметров при фиксированных остальных,
а~одновременное изменение нескольких параметров не~допускается.
Данное семейство можно также рассматривать как обобщение трёх частных случаев,
рассмотренных в~\cite{voron09dan}:
монотонной цепочки ($p=1$),
унимодальной цепочки ($p=2$)
и~единичной окрестности лучшего алгоритма ($D=1$).

Формула для вероятности переобучения унимодальной цепочки уже была получена в~теореме~\ref{th:unimodalChain}.
Для получения явных формул для двух оставшихся семейств достаточно найти явное выражение для комбинаторного коэффициента $R_{D, p}^h(S, F)$.

\begin{Corollary}
    Для монотонной цепочки длины $D+1$ вероятность переобучения равна
    \begin{equation}
    \label{formula:monotonicChain}
        Q_\mu(\epsilon, A)=
            \frac 1{C_L^\ell}
            \sum_{h=0}^{D} \sum_{S=h}^D
            \frac{1}{1 + S}
%            \frac{C_{L'}^{\ell'}}{C_L^\ell}
            H_{L'}^{\ell', m}(s(\epsilon)),
    \end{equation}
    где
    $L' = L - S - [S{\neq}D]$,\;
    $\ell' = \ell - [S{\neq}D]$.
\end{Corollary}

\begin{Corollary}
    Для единичной окрестности из~${p+1}$ алгоритма вероятность переобучения равна
    \begin{equation}
    \label{formula:unitVicinity}
        Q_\mu(\epsilon, A) =
            \frac 1{C_L^\ell}
            \sum_{h=0}^{1}
            \sum_{S=h}^{p}
            \frac{|\omega_h| C_{p-h}^{S-h}}{1 + S}
%            \frac{C_{L'}^{\ell'}}{C_L^\ell}
            H_{L'}^{\ell', m}(s(\epsilon)),
    \end{equation}
    где
    $L' = L {-} p$,\;
    $\ell' = \ell {+} S {-} p$.
\end{Corollary}

%\subsubsection{Численный эксперимент}

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=84mm,height=63mm]{frey_p.eps}
    \hfill
    \caption{Зависимость $Q_\mu(\epsilon, A)$ от~$p$ для связки
             из монотонных цепочек
             при $L=300$, $\ell=150$, $m=15$, $D = 1, 2, 3, 5, 10$, $\epsilon = 0.05$.}
    \label{fig:pBinding}
    \medskip
    \hfill
    \includegraphics[width=84mm,height=63mm]{frey_D.eps}
    \hfill
    \caption{Зависимость $Q_\mu(\epsilon, A)$ от~$D$ для связки
            из $p = 1, 2, 3, 5, 10$ монотонных цепочек
            при $L=300$, $\ell=150$, $m=15$, $\epsilon = 0.05$.}
    \label{fig:dBinding}
    \end {multicols}
\end{figure}

На~рис.\,\ref{fig:Monot}~и~рис.\,\ref{fig:Unit} представлены результаты численных экспериментов,
в~которых сравнивались вероятности переобучения для различных вариантов минимизации эмпирического риска.
Из~четырех кривых на~каждом графике верхняя (жирная) соответствует пессимистической минимизации эмпирического риска~\cite{voron09dan,voron09mmro},
нижняя "--- оптимистической.
Две почти сливающиеся кривые между ними соответствуют рандомизированной минимизации эмпирического риска.
\mbox{Одна из~них} вычислена по~доказанным формулам,
вторая построена методом Монте-Карло по~$10^5$ случайных разбиений,
при равновероятном выборе лучшего алгоритма в~случаях неопределенности.
Различия этих двух кривых находятся в~пределах погрешности метода Монте"=Карло.

На~рис.\,\ref{fig:pBinding}~и~рис.\,\ref{fig:dBinding} представлены зависимости вероятности переобучения
от~числа~$p$ ветвей в~связке и~от их~длины~$D$.
Графики построены для рандомизированного метода минимизации эмпирического риска.
Рис.\,\ref{fig:dBinding} показывает, что при увеличении длин цепочек~$D$ вероятность переобучения
практически перестаёт расти уже при $D=7$.
Это~связано с~\emph{эффектом расслоения} "--- лишь алгоритмы из нижних слоёв
имеют существенно отличную от~нуля вероятность быть выбранными
методом минимизации эмпирического риска.
Добавление <<слишком плохих>> алгоритмов не~увеличивает вероятность переобучения.
Рис.\,\ref{fig:pBinding} показывает, что
при увеличении числа~$p$ цепочек в~связке вероятность переобучения продолжает расти.
Однако скорость роста сублинейна по~$p$, благодаря \emph{эффекту связности} "---
все алгоритмы находятся на~хэмминговом расстоянии не~более~$D$ от~лучшего алгоритма.

%\section{Верхние оценки вероятности переобучения}

\subsection{Монотонная сетка}

Введём целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \dots, h} |d_j|$,
$|\vec d| = |d_1| + \dots + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из неравенств строгое.

\begin{Def}
    Множество алгоритмов
    $A_M = \bigl\{a_{\vec d}\bigr\}$, где $\vec d \geq 0$ и $\|\vec d\|\leq D$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов},
    если существует $h \in \NN$ и упорядоченные наборы объектов
    $X_j = \{x_j^1, x_j^2, \dots, x_j^D\} \subset \XX$, для всех $j = 1, \dots, h$,
    а так же множества $U_1 \subset \XX$ и $U_0 \subset \XX$,
    такие что выполнены условия:
    \begin{enumerate*}
        \item Набор $\Big\{ U_0, U_1,\{X_j\}_{j=1}^h \Big\}$ является разбиением множества $\XX$
              на непересекающиеся множества;
        \item $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate*}
\end{Def}

Обозначим $|U_1| = m$.
Из определения следует, что $n(a_{\vec d}, \XX) = m + |d|$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в сетке}.
Множество алгоритмов с~равным числом ошибок $t+m \brop= n(a_{\vec d}, \XX)$ называются \emph{$t$-слоем} сетки.

\begin{Example}
    Монотонная двумерная сетка при $m = 0$ и $L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}

Число алгоритмов в $h$-мерной монотонной сетке с ветвями длины $D$ равно $(D + 1)^h$.
\emph{Укороченной} $h$-мерной монотонной сеткой $\tilde{A}_M \subset A_M$
назовем первые $D$ слоев из $A_M$.
Таким образом $\tilde{A}_M = \{a_{\vec d} \in A_M, |\vec d| \leq D\}$.
Число алгоритмов в $\tilde{A}_M$ равно $C_{D+h}^h$.

\begin{figure}[t]
    \label{fig:MonotonicSets}
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_M.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_MS.eps}
    \hfill
    \end {multicols}
    \caption{Матрица ошибок монотонной сетки (слева) и укороченной монотонной сетки (справа)
    при $D=20$, $h=2$, $m=5$, $L=60$.}
\end{figure}

Впервые укороченные монотонны сетки произвольной размерности были изучены П. Ботовым в \cite{botov09mmro}.
Там же были получены формулы для вероятности переобучения \emph{пессимистического} метода минимизации
эмпирического риска.

Численные эксперименты показывают, что при разумных сочетаниях параметров
вероятности переобучения для укороченной $\tilde{A}_M$ и простой $A_M$ монотонных сеток различаются крайне мало.
Поэтому в дальнейшем мы ограничимся исследованием не-укороченных монотонных сеток.
Для этого класса семейств алгоритмов будут получены явные формулы вероятности переобучения
рандомизированного метода минимизации эмпирического риска.

\begin{Lemma}
Группа симметрии монотонной сетки размерности $h$ содержит в качестве подгруппы группу $S_h$ всевозможных
перестановок множеств $X_1, \dots, X_h$.
\end{Lemma}

$\square$ \textbf{Доказательство.}

Рассмотрим алгоритм $a_{\vec d} \in A_M$ и произвольную $\pi \in S_h$.
По данному выше определению действия $\pi$ на $\XX$ получаем, что
$\pi a_{\vec d} = a_{ \pi \vec d}$, где действие $\pi$ на вектор $\vec d$
определяется соответствующей перестановкой его координат.
Множество $\{0, \dots, D\}^h$ сохраняется при применении к нему произвольной перестановки координат $\pi \in S_h$.
Поэтому $\forall \vec d \in \{0, \dots, D\}^h$ выполнено $\pi \vec d \in \{0, \dots, D\}^h$.
А следовательно $a_{\pi \vec d} \in A_M$.
$\; \blacksquare$

\begin{Def}
\emph{Диаграммой Юнга порядка $p$} будем называть не-возрастающую последовательность
неотрицательных чисел
$\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n \geq 0$,
такую что $\sum_{j=1}^n \lambda_j = p$.
\end{Def}

Диаграммы Юнга находятся во взаимно-однозначном соответствии c гистограммами следующего вида:
\[
\;\;\; \includegraphics[height=20mm]{young_diagramm.eps} \;\;\;
\]

Множество диаграмм Юнга порядка $p$ будем обозначать через $Y_p$.
Множество диаграмм Юнга из $h$ столбцов, в которых $\lambda_1 \leq D$, обозначим через $Y_p^{h, D}$.
Обозначим $Y_{*}^{h, D} = \bigcup \limits_{p = 0}^{D * h} Y_p^{h, D}$

\begin{Lemma}
\label{eq:monotonicSetOrbits}
Множество орбит монотонной сетки $A_M = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$
под действием $S_h$ индексировано всевозможными диаграммами Юнга из $Y_{*}^{h, D}$.
Число алгоритмов в орбите $\omega_\lambda$, где $\lambda = (\lambda_1, \dots, \lambda_h)$ равно
числу различных слов длины $h$, состоящих из символов $\lambda_1, \dots, \lambda_h$:
$|\omega_\lambda| = |S_h \lambda|$.
\end{Lemma}

$\square$ \textbf{Доказательство.}
Напомним, что вместо действия $S_h$ на $A_M = \{ a_{\vec d} \}$ можно рассматривать действие
$S_h$ на вектор индексов $\vec d$, заданное перестановками координат.

Рассмотрим орбиту произвольного алгоритма $a_{\vec d}$.
Возьмем перестановку $\pi \in S_h$, упорядочивающую координаты $\vec d$ в порядке не-возрастания,
и положим $\lambda = \pi \vec d$. Получаем, что $\lambda \in Y_{*}^{h, D}$ "--- диаграмма Юнга.
При этом различным диаграммам Юнга $\lambda_1$ и $\lambda_2$ будут соответствовать
различные орбиты действия группы $S_h$ на $\{a_{\vec d}\}$.

Взаимно-однозначное соответствие между словами длины $h$ из символов $\lambda_1, \dots, \lambda_h$ и
количеством элементами орбиты $|\omega_\lambda|$ очевидно.
$\blacksquare$

\begin{Theorem}
\label{th:monotonicNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к монотонной сетке $A_M = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
дается выражением:
\[
    Q_\mu(\epsilon, A_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \lambda, \\\|\vec t\| \leq D}}
                         \frac {|S_h \lambda|} {T(\vec t)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\]
где
$T(\vec t) = \prod_j (t_j + 1)$,
$\ell' = \ell - \sum_{j = 1}^h [t_j \neq D]$,
$k' = k - |\vec t|$, $L' = \ell' + k'$,
$s_0 = \frac \ell L [m + |\lambda| - \epsilon k]$,
$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}

$\square$ \textbf{Доказательство.}

\begin{figure}[t]
    \centering
    \includegraphics[height=100mm]{monot_net_illustration.eps}
    \caption{Строение множества $A_M(X)$ для двумерной монотонной сетки.}
\end{figure}

Согласно теореме \ref{th:equalContributionERM} вероятность переобучения записывается в виде:
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\Xl \in \XXell}
                         \frac {[a_\lambda \in A_M(\Xl)]} {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 1. }Зафиксируем $\Xl \in \XXell$.
Обозначим через $t_j$ максимальный индекс из $\{0, \dots, D\}$, при котором все объекты
$\{x_j^1, \dots, x_j^{t_j}\}$ содержатся в $\Xk$, а $x_j^{t_j + 1}$, при его наличии, лежит в $\Xl$.
Положим $\vec t = \{t_j\}_{j = 1}^h$.
Тогда условие $a_\lambda \in A_M(X)$ перепишется  как $\vec t \geq \lambda$.

Действительно, заметим что для всех $a \in A_M$ и $\Xl \in \XXell$ выполнено $n(a, \Xl) \geq n(a_0, \Xl)$.
Следовательно, алгоритм $a_\lambda$ может быть выбран, только если объекты
$x_j^i$ при всех $j = 1, \dots, h$ и $i \leq \lambda_j$ лежат в контроле.
В терминах $\vec t$ это записывается как $\vec t \geq \lambda$.

Обозначим множество разбиений на обучение и контроль с фиксированным значением параметра $\vec t$
через $\XXell_{\vec t}$. Тогда
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \lambda, \\\|\vec t\| \leq D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 2. } Пусть $\Xl \in \XXell_{\vec t}$.
Заметим, что алгоритм $a_{\vec d} \in A_M(\Xl)$ тогда и только тогда, когда $\vec d \leq \vec t$.
Следовательно $|A_M(\Xl)| = (t_1 + 1) (t_2 + 1) \dots (t_h + 1)$. Обозначим $T(\vec t) = \prod_j (t_j + 1)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из $U_1$, лежащих в обучении.
Тогда $\delta (a_\lambda, \Xl) = \frac{m - s + |\lambda|}{k} - \frac{s}{\ell}$, и условие
$\delta (a_\lambda, \Xl) \geq \epsilon$ запишется в виде $s \leq \frac \ell L [m + |\lambda| - \epsilon k] \equiv s_0$.
Множество всех разбиений из $\XXell_{\vec t}$ с фиксированным параметром $s$ обозначим
через $\XXell_{\vec t, s}$. Тогда
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \lambda, \\\|\vec t\| \leq D}}
                         \frac 1 {T(\vec t)}
                         {\sum_{s = 0}^{s_0} |\XXell_{\vec t, s}|}.
\]

\textbf{Шаг 4. } Вычислим мощность множества $\XXell_{\vec t, s}$.

Введем обозначения $\ell' = \ell - \sum_{j = 1}^h [t_j \neq D]$,
$k' = k - |\vec t|$, $L' = \ell' + k'$. Тогда простое комбинаторное вычисление показывает, что
$|\XXell_{\vec t, s}| = C_m^s C_{L' - m}^{k' - s}$.
Следовательно,
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \lambda, \\\|\vec t\| \leq D}}
                         \frac 1 {T(\vec t)}
                         {\sum_{s = 0}^{s_0} C_m^s C_{L' - m}^{k' - s}}.
\]
Напомним, что
$H_{L'}^{\ell', m}(z) = \frac {1}{C_{L'}^{\ell'}}\sum \limits_{s=0}^{\lfloor z \rfloor} C_m^s C_{L'-m}^{\ell' - s}$
"--- функция гипергеометрического распределения \cite{voron09mmro}. Тогда
\[
    Q_\mu(\epsilon, A_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \lambda, \\\|\vec t\| \leq D}}
                         \frac {|S_h \lambda|} {T(\vec t)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0). \; \blacksquare
\]

\subsection{Унимодальная сетка}

\begin{Def}
    Множество алгоритмов
    $A_U = \bigl\{a_{\vec d}\bigr\}$, где $\|\vec d\|\leq D$
    называется \emph{унимодальной $h$"~мерной сеткой алгоритмов},
    если существует $h \in \NN$ и упорядоченные наборы объектов
    $X_j = \{x_j^1, x_j^2, \dots, x_j^D\} \subset \XX$,
    $Y_j = \{y_j^1, y_j^2, \dots, y_j^D\} \subset \XX$,
    для всех $j = 1, \dots, h$,
    а так же множества $U_1 \subset \XX$ и $U_0 \subset \XX$,
    такие что выполнены условия:
    \begin{enumerate*}
        \item Набор $\Big\{ U_0, U_1,\{X_j\}_{j=1}^h, \{Y_j\}_{j=1}^h \Big\}$ является разбиением множества $\XX$
              на непересекающиеся множества;
        \item $a_d(x_j^i) = [d_j > 0] \left[ i \leq |d_j| \right]$, где $x_j^i \in  X_j$;
        \item $a_d(y_j^i) = [d_j < 0] \left[ i \leq |d_j| \right]$, где $y_j^i \in  Y_j$;
        \item $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate*}
\end{Def}

Заметим, что данное определение отличается от определения монотонной сетки отсутствием ограничения
$\vec d \geq 0$. Число алгоритмов в $h$-мерной унимодальной сетке с ветвями длины $D$ составляет
$(2 D + 1)^h$.
\emph{Укороченной} $h$-мерной унимодальной сеткой $\tilde{A}_U$ назовем множество первых $D$ слоев из $A_U$:
$\tilde{A}_U = \{a_{\vec d} \in A_U \colon n(a_{\vec d}, \XX) \leq m + D\}$.

\begin{figure}[t]
    \label{fig:UnimodalSets}
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_U.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_US.eps}
    \hfill
    \end {multicols}
    \caption{Матрица ошибок унимодальной сетки (слева) и укороченной унимодальной сетки (справа)
    при $D=10$, $h=2$, $m=5$, $L=60$.}
\end{figure}

Формула для вероятности переобучения \emph{пессимистического} метода минимизации эмпирического риска
на укороченных унимодальных сетках так же была получена в \cite{botov09mmro}.
Ниже рассматриваются не-укороченные унимодальные сетки и
случай \emph{рандомизированного} метода минимизации эмпирического риска.

\begin{Lemma}
Группа симметрии унимодальной сетки размерности $h$ содержит в качестве подгруппы группу $\Sym(A_U) = (S_2)^h \times S_h$.
Группа $S_h$ действует на множестве пар $\left(X_j, Y_j\right)_{j=1}^h$ всеми возможными перестановками;
$j$-тая группа $S_2$ переставляет объекты множества $X_j$ и $Y_j$ местами, сохраняя относительный порядок объектов.
\end{Lemma}

$\square$ \textbf{Доказательство.}

Рассмотрим алгоритм $a_{\vec d} \in A_U$ и произвольную $\pi = (z_1, \dots, z_h) \times \pi_0 \in \Sym(A_U)$,
где $z_j \in S_2$, $\pi_0 \in S_h$.
По данному выше определению действия $\pi$ на $\XX$ получаем, что
$\pi a_{\vec d} = a_{ \pi \vec d}$, где действие $\pi$ на вектор $\vec d$
определяется перестановкой его координат с помощью $\pi_0$ и инверсией знаков
для всех $j$, таких что $z_j \neq id$ "--- транспозиция.
Множество $\{-D, \dots, D\}^h$ сохраняется при применении к нему произвольной перестановки координат $\pi \in \Sym(A_U)$.
Поэтому $\forall \vec d \in \{-D, \dots, D\}^h$ выполнено $\pi \vec d \in \{-D, \dots, D\}^h$.
А следовательно $a_{\pi \vec d} \in A_U$.
$\; \blacksquare$

\begin{Lemma}
\label{eq:unimodalSetOrbits}
Множество орбит унимодальной сетки $A_U = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$
под действием $\Sym(A_U)$ индексировано всевозможными диаграммами Юнга из $Y_{*}^{h, D}$.
Пусть $\lambda = (\lambda_1, \dots, \lambda_h) \in Y_{*}^{h, D}$. Обозначим через $|S_h \lambda|$
число различных слов длины $h$, состоящих из символов $\lambda_1, \dots, \lambda_h$
Пусть $|\lambda > 0|$ "--- число строго положительных компонент вектора $\lambda$.

Тогда число алгоритмов в орбите $\omega_\lambda$ равно $|S_h \lambda| \cdot 2^{|\lambda > 0|}$.
\end{Lemma}

$\square$ \; \textbf{Доказательство} полностью повторяет рассуждения леммы \ref{eq:monotonicSetOrbits}.
Множитель $2^{|\lambda > 0|}$ соответствует возможности сменить знак у всех
не-нулевых компонент вектора $\vec d$.
$\blacksquare$

\begin{Theorem}
\label{th:unimodalNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к унимодальной сетке $A_U = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
дается выражением:
\[
    Q_\mu(\epsilon, A_U) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec  t   \geq \lambda, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}} {T(\vec t, \vec {t'})}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\]
где
$T(\vec t, \vec {t'}) = \prod_j (t_j + t'_j + 1)$,
$\ell' = \ell - \sum \limits_{j = 1}^h \left( [t_j \neq D] + [t'_j \neq D] \right)$,
$k' = k - |\vec t| - |\vec t'|$, $L' = \ell' + k'$,
$s_0 = \frac \ell L [m + |\lambda| - \epsilon k]$,
$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}

$\square$ \textbf{Доказательство.}

\begin{figure}[t]
    \centering
    \includegraphics[height=140mm]{unimod_net_illustration.eps}
    \caption{Строение множества $A_U(X)$ для двумерной унимодальной сетки.}
\end{figure}

\textbf{Шаг 1. }
Выберем в качестве представителя $a_\lambda$ орбиты $\omega_\lambda$ алгоритм, не допускающий ошибок на
множестве $Y = \bigcup_{j=1}^h Y_j$. Этого можно добиться, взяв произвольный  $a_{\vec d} \in \omega_\lambda$ и поменяв
знаки у всех $d_j < 0$ c помощью транспозиции $z_j$.

Введя обозначения $\vec t$ и $\XXell_{\vec t}$ так же, как и на первом шаге вывода формулы для монотонной сетки,
получим
\[
    Q_\mu(\epsilon, A_U) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec t \geq \lambda, \\\|\vec t\| \leq D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|A_U(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 2. }
Обозначим через $t'_j$ максимальный индекс из $\{0, \dots, D\}$, при котором все объекты
$\{y_j^1, \dots, y_j^{t'_j}\}$ содержатся в $\Xk$, а $y_j^{t'_j + 1}$, при его наличии, лежит в $\Xl$.
Положим $\vec {t'} = \{t'_j\}_{j = 1}^h$. Заметим, что вектор $\vec {t'}$ играет для набора $\{Y_j\}$ ту же
роль, что $\vec t$ для $\{X_j\}$. Обозначим через $\XXell_{\vec t, \vec {t'}}$ множество разбиений с
фиксированными параметрами $\vec t$ и $\vec {t'}$.

Пусть $\Xl \in \XXell_{\vec t, \vec {t'}}$.
Заметим, что алгоритм $[a_{\vec d} \in A_U(\Xl)] = [-\vec {t'} \leq \vec d \leq \vec t]$.
Следовательно $|A_U(\Xl)| = (t_1 + t'_1 + 1) (t_2 + t'_2 + 1) \dots (t_h + t'_h + 1)$.
Обозначим $T(\vec t, \vec {t'}) = \prod_j (t_j + t'_j + 1)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из $U_1$, лежащих в обучении.
Пусть $s_0 \equiv \frac \ell L [m + |\lambda| - \epsilon k]$.
Повторяя рассуждения аналогичного шага доказательства для монотонной сетки получим
\[
    Q_\mu(\epsilon, A_U) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec  t   \geq \lambda, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \frac 1 {T(\vec t, \vec {t'})}
                         {\sum_{s = 0}^{s_0} |\XXell_{\vec t, \vec {t'}, s}|}.
\]

\textbf{Шаг 4. } Посчитаем мощность множества $\XXell_{\vec t, \vec {t'}, s}$.

Обозначим $\ell' = \ell - \sum \limits_{j = 1}^h \left( [t_j \neq D] + [t'_j \neq D] \right)$,
$k' = k - |\vec t| - |\vec t'|$, $L' = \ell' + k'$. Тогда
$|\XXell_{\vec t, \vec {t'}, s}| = C_m^s C_{L' - m}^{k' - s}$.
Воспользовавшись определением функции гипергеометрического распределения получим:
\[
    Q_\mu(\epsilon, A_U) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec  t   \geq \lambda, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}} {T(\vec t, \vec {t'})}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0). \;\blacksquare
\]

\subsection {Разреженная монотонная сетка}

\begin{Def}
Пусть $A_M=\{a_{\vec d}\}$, где $\|d\| \leq \rho D$ "--- $h$-мерная монотонная сетка.
Обозначим $m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной монотонной сеткой $\ddot{A}_M$ плотности~$\rho$}
будем называть следующее подмножество $A_M$:
\[
    \ddot{A}_M = \left\{a \in A_M \colon \exists i \in \ZZ, \text{ такое что } n(a, \XX) = m + i \rho\right\}.
\]
\end{Def}

Отметим, что при $\rho > 1$ граф смежности разреженной монотонной сетки состоит из изолированных точек.

\begin{Example}
На следующем рисунке выделено подмножество двумерной монотонной сетки с параметром $D = 8$,
соответствующее разреженной монотонной сетке с параметрами $\rho = 2$, $D = 4$.
\begin{figure}[H]
    \begin{centering}
    \includegraphics[height=80mm]{monot_net_rarefield.eps}
    \caption{Двумерная разреженная монотонная сетка при $\rho = 2$, $D = 4$.}
    \end{centering}
\end{figure}
\end{Example}

%Отметим, что лемма \ref{eq:monotonicSetOrbits} справедлива и для разреженной монотонной сетки.

\begin{Theorem}
Пусть $\ddot{A}_M$ "--- разреженная монотонная сетка с размерностью $h$ и плотностью $\rho$.
Тогда вероятность переобучения $Q_\mu(\epsilon, \ddot{A}_M)$
для рандомизированного метода минимизации эмпирического риска $\mu$
дается формулой
\[
    Q_\mu(\epsilon, \ddot{A}_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T(\lfloor \vec t / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\]
где
$Y_{*}^{h, D}$ "--- множество невозрастающих последовательностей длины $h$ и не превосходящих $D$,
$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на $\lambda$,
$T(\vec {t}) = \prod_j (t_j + 1)$,
$\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,
$k' = k - |\vec t|$, $L' = \ell' + k'$,
$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,
$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения.
\end{Theorem}

%$\square$ \textbf{Доказательство.}
%
%Тривиальным образом обобщая доказательство теоремы \ref{th:monotonicNetFormula} приходим к указанной выше формуле.
%$\blacksquare$

\subsection {Разреженная унимодальная сетка}

%Определим разреженную унимодальную сетку по аналогии с разреженной монотонной сеткой.

\begin{Def}
Пусть $A_U=\{a_{\vec d}\}$, где $\|d\| \leq \rho D$ "--- $h$-мерная унимодальная сетка.
Обозначим $m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной унимодальной сеткой $\ddot{A}_U$ плотности~$\rho$}
будем называть следующее подмножество $A_U$:
\[
    \ddot{A}_U = \left\{a \in A_U \colon \exists i \in \ZZ, \text{ такое что } n(a, \XX) = m + i \rho\right\}.
\]
\end{Def}

\begin{Theorem}
Пусть $\ddot{A}_M$ "--- разреженная унимодальная сетка с размерностью $h$ и плотностью $\rho$.
Тогда вероятность переобучения $Q_\mu(\epsilon, \ddot{A}_M)$
для рандомизированного метода минимизации эмпирического риска $\mu$
дается формулой
\[
    Q_\mu(\epsilon, A_U) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq \rho D}}
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                            {T(\lfloor \vec t / \rho \rfloor, \lfloor \vec {t'} / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\]
где
$Y_{*}^{h, D}$ "--- множество невозрастающих последовательностей длины $h$ и не превосходящих $D$,
$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на $\lambda$,
$T(\vec t, \vec {t'}) = \prod_j (t_j + t'_j + 1)$,
$\ell' = \ell - \sum \limits_{j = 1}^h \left( [t_j \neq \rho D] + [t'_j \neq \rho D] \right)$,
$k' = k - |\vec t| - |\vec t'|$, $L' = \ell' + k'$,
$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,
$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}

%$\square$ \textbf{Доказательство.}

%Тривиальным образом обобщая доказательство теоремы \ref{th:unimodalNetFormula} приходим к указанной выше формуле.
%$\blacksquare$

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=80mm,height=60mm]{rarefield_monotonic.eps}
    \hfill
    \caption{Зависимость $Q_\mu(\epsilon, \ddot{A}_M)$ от разреженности $\rho$ монотонной сетки
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1,2,3,4$.}
    \label{fig:MonotRarefieldNet}
    \medskip
    \hfill
    \includegraphics[width=80mm,height=60mm]{rarefield_unimodal_vs_monotonic.eps}
    \hfill
    \caption{Сравнение $Q_\mu(\epsilon, \ddot{A}_M)$ и $Q_\mu(\epsilon, \ddot{A}_U)$ от~$\rho$
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1(2), 2(4)$.}
    \label{fig:UnimodRarefieldNet}
    \end {multicols}
\end{figure}

%В заключение приведем результаты ряда экспериментов.

На рис. \ref{fig:MonotRarefieldNet} изображена зависимость вероятности переобучения от разреженности монотонной сетки.
Сравниваются монотонные сетки размерностей $h=1, 2, 3, 4$. При увеличении размерности вероятности переобучения
также возрастает. Отметим, что при увеличении разреженности $\rho$ переобучение выходит на константу, соответствующую
вероятности переобучения лучшего алгоритма семейства $a_0$.

На рис. \ref{fig:UnimodRarefieldNet} приведены результаты сравнения разреженных $h$-мерных унимодальных сеток
с $2h$-мерными монотонными сетками, при $h=1$ и $h=2$.
Видно, что при больших значениях параметра $\rho$ вероятность переобучения унимодальных сеток практически
совпадает с вероятностью переобучения монотонных сеток удвоенной размерности.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% История: следующие два параграфа (разреженная монотонная и унимодальная сетка - 2)
% являются совмещением предыдущих двух параграфов. Нужно проверить, что они не выбросили ничего нужного,
% и выбросить предыдущие параграфы.

\subsection{Разреженная монотонная сетка-2}

Введём целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \dots, h} |d_j|$,
$|\vec d| = |d_1| + \dots + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из неравенств строгое.

\begin{Def}
    \label{eq:monotonicSet}
    Множество алгоритмов
    $A_M = \bigl\{a_{\vec d}\bigr\}$, где~$\vec d \geq 0$ и~$\|\vec d\|\leq D$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов длины $D$},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j = \{x_j^1, x_j^2, \dots, x_j^D\} \subset \XX$, для всех $j = 1, \dots, h$,
    а~так~же множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что:
    \begin{enumerate}
        \item набор $\Big\{ U_0, U_1,\{X_j\}_{j=1}^h \Big\}$ является разбиением множества $\XX$
              на непересекающиеся подмножества;
        \item $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Def}

Монотонная сетка алгоритмов "--- это модель параметрического \emph{связного семейства алгоритмов},
предполагающая, что при непрерывном удалении каждой компоненты вектора параметров
от~оптимального значения число ошибок на~полной выборке только увеличивается.

Обозначим $|U_1| = m$.
Из определения следует, что $n(a_{\vec d}, \XX) = m + |d|$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в сетке}.
Множество алгоритмов с~равным числом ошибок $t+m \brop= n(a_{\vec d}, \XX)$ называются \emph{$t$-слоем} сетки.

\begin{Example}
    Монотонная двумерная сетка при $m = 0$ и $L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}

Число алгоритмов в $h$-мерной монотонной сетке с ветвями длины $D$ равно $(D + 1)^h$.
\emph{Укороченной} $h$-мерной монотонной сеткой $\tilde{A}_M \subset A_M$
назовем первые $D$ слоев из $A_M$.
Таким образом $\tilde{A}_M = \{a_{\vec d} \in A_M, |\vec d| \leq D\}$.
Число алгоритмов в $\tilde{A}_M$ равно $C_{D+h}^h$.

\begin{figure}[t]
    \label{fig:MonotonicSets}
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_M.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_MS.eps}
    \hfill
    \end {multicols}
    \caption{Матрица ошибок монотонной сетки (слева) и укороченной монотонной сетки (справа)
    при $D=20$, $h=2$, $m=5$, $L=60$.}
\end{figure}

Впервые монотонны сетки произвольной размерности были изучены П. Ботовым в \cite{botov09mmro}.
Там же были получены формулы для вероятности переобучения \emph{пессимистического} метода минимизации
эмпирического риска.

Численные эксперименты показывают, что при разумных сочетаниях параметров
вероятности переобучения для укороченной $\tilde{A}_M$ и простой $A_M$ монотонных сеток различаются крайне мало.
Поэтому в дальнейшем мы ограничимся исследованием не-укороченных монотонных сеток.
Для этого класса семейств алгоритмов будут получены явные формулы вероятности переобучения
рандомизированного метода минимизации эмпирического риска.

\begin{Def}
Пусть~$\rho \in \NN$ "--- целочисленный параметр;
$A_M=\{a_{\vec d}\}$ \,"---\, $h$"~мерная монотонная сетка длины $\rho D$;
$m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной монотонной сеткой $\ddot{A}_M$ плотности~$\rho$ и длины $D$}
будем называть подмножество $A_M$, заданное условием:
\[
\begin{aligned}
    \ddot{A}_M =
        %\left\{a_{\vec d} \in A_M \, \big| \, \forall j = 1, \dots, h \; \exists d'_j \in \ZZ \colon d_j = \rho d'_j \right\}.
        \left\{a_{\vec d} \in A_M \, \big| \, \vec d \in \left( \tiny \rho \ZZ \tiny \right)^h \right\}.
\end{aligned}
\]
\end{Def}

Отметим, что при $\rho > 1$ граф смежности разреженной монотонной сетки состоит из изолированных точек.

\begin{Example}
На рисунке \ref{fig:sparseMonotonicNetExample}
выделено подмножество двумерной монотонной сетки с параметром $D = 8$,
соответствующее разреженной монотонной сетке с параметрами $\rho = 2$, $D = 4$.
\begin{figure}[h]
    \label{fig:sparseMonotonicNetExample}
    \begin{centering}
    \includegraphics[height=80mm]{monot_net_rarefield.eps}
    \caption{Двумерная разреженная монотонная сетка при $\rho = 2$, $D = 4$.}
    \end{centering}
\end{figure}
\end{Example}

\begin{Lemma}
Группа симметрии разреженной монотонной сетки размерности $h$ содержит в качестве подгруппы группу $S_h$ всевозможных
перестановок множеств $X_1, \dots, X_h$.
\end{Lemma}

$\square$ \textbf{Доказательство.}

Все алгоритмы разреженной $h$"~мерной монотонной сетки длины $D$ индексированы множеством вектор-индексов $\vec d \in \{0, \dots, D\}^h$. Тут число ошибок алгоритма $a_{\vec d} = m + \rho |\vec d|$.

Рассмотрим алгоритм $a_{\vec d} \in \ddot{A}_M$ и произвольную $\pi \in S_h$.
По данному выше определению действия $\pi$ на $\XX$ получаем, что
$\pi a_{\vec d} = a_{ \pi \vec d}$, где действие $\pi$ на вектор $\vec d$
определяется соответствующей перестановкой его координат.
Множество $\{0, \dots, D\}^h$ сохраняется при применении к нему произвольной перестановки координат $\pi \in S_h$.
Поэтому $\forall \vec d \in \{0, \dots, D\}^h$ выполнено $\pi \vec d \in \{0, \dots, D\}^h$.
А следовательно $a_{\pi \vec d} \in \ddot{A}_M$.
$\; \blacksquare$

\begin{Def}
\emph{Диаграммой Юнга порядка $p$} будем называть не-возрастающую последовательность
неотрицательных чисел
$\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n \geq 0$,
такую что $\sum_{j=1}^n \lambda_j = p$.
\end{Def}

Диаграммы Юнга находятся во взаимно-однозначном соответствии c гистограммами следующего вида:
\[
\;\;\; \includegraphics[height=20mm]{young_diagramm.eps} \;\;\;
\]

Множество диаграмм Юнга порядка $p$ будем обозначать через $Y_p$.
Множество диаграмм Юнга из $h$ столбцов, в которых $\lambda_1 \leq D$, обозначим через $Y_p^{h, D}$.
Обозначим $Y_{*}^{h, D} = \bigcup \limits_{p = 0}^{D * h} Y_p^{h, D}$

\begin{Lemma}
\label{eq:monotonicSetOrbits}
Множество орбит разреженной монотонной сетки $\ddot{A}_M = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$
под действием $S_h$ индексировано всевозможными диаграммами Юнга из $Y_{*}^{h, D}$.
Число алгоритмов в орбите $\omega_\lambda$, где $\lambda = (\lambda_1, \dots, \lambda_h)$ равно
числу различных слов длины $h$, состоящих из символов $\lambda_1, \dots, \lambda_h$:
$|\omega_\lambda| = |S_h \lambda|$.
\end{Lemma}

$\square$ \textbf{Доказательство.}
Напомним, что вместо действия $S_h$ на $\ddot{A}_M = \{ a_{\vec d} \}$ можно рассматривать действие
$S_h$ на вектор индексов $\vec d$, заданное перестановками координат.

Рассмотрим орбиту произвольного алгоритма $a_{\vec d}$.
Возьмем перестановку $\pi \in S_h$, упорядочивающую координаты $\vec d$ в порядке не-возрастания,
и положим $\lambda = \pi \vec d$. Получаем, что $\lambda \in Y_{*}^{h, D}$ "--- диаграмма Юнга.
При этом различным диаграммам Юнга $\lambda_1$ и $\lambda_2$ будут соответствовать
различные орбиты действия группы $S_h$ на $\{a_{\vec d}\}$.

Взаимно-однозначное соответствие между словами длины $h$ из символов $\lambda_1, \dots, \lambda_h$ и
количеством элементами орбиты $|\omega_\lambda|$ очевидно.
$\blacksquare$

\begin{Theorem}
\label{th:monotonicNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к разреженной монотонной сетке $\ddot{A}_M = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
дается выражением:
\begin{equation}
\label{eq:monotonicNetFormula}
    Q_\mu(\epsilon, \ddot{A}_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T(\lfloor \vec t / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\end{equation}
где
$Y_{*}^{h, D}$ "--- множество целочисленных неотрицательных невозрастающих последовательностей
длины $h$ и не превосходящих $D$,
$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на $\lambda$,
$T(\vec {t}) = \prod_j (t_j + 1)$,
$\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,
$k' = k - |\vec t|$, $L' = \ell' + k'$,
$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,
$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения.
\end{Theorem}

$\square$ \textbf{Доказательство.}

\begin{figure}[t]
    \centering
    \includegraphics[height=100mm]{monot_net_illustration.eps}
    \caption{Строение множества $A_M(X)$ для двумерной монотонной сетки.}
\end{figure}

Согласно теореме \ref{th:equalContributionERM} вероятность переобучения записывается в виде:
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\Xl \in \XXell}
                         \frac {[a_\lambda \in A_M(\Xl)]} {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 1. }Зафиксируем $\Xl \in \XXell$.
Обозначим через $t_j$ максимальный индекс из $\{0, \dots, \rho D\}$, при котором все объекты
$\{x_j^1, \dots, x_j^{t_j}\}$ содержатся в $\Xk$, а $x_j^{t_j + 1}$, при его наличии, лежит в $\Xl$.
Положим $\vec t = \{t_j\}_{j = 1}^h$.
Тогда условие $a_\lambda \in A_M(X)$ перепишется  как $\vec t \geq \rho \lambda$.

Действительно, заметим что для всех $a \in A_M$ и $\Xl \in \XXell$ выполнено $n(a, \Xl) \geq n(a_0, \Xl)$.
Следовательно, алгоритм $a_\lambda$ может быть выбран, только если объекты
$x_j^i$ при всех $j = 1, \dots, h$ и $i \leq \rho \lambda_j$ лежат в контроле.
В терминах $\vec t$ это записывается как $\vec t \geq \rho \lambda$.

Обозначим множество разбиений на обучение и контроль с фиксированным значением параметра $\vec t$
через $\XXell_{\vec t}$. Тогда
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 2. } Пусть $\Xl \in \XXell_{\vec t}$.
Заметим, что алгоритм $a_{\vec d} \in A_M(\Xl)$ тогда и только тогда, когда $\rho \vec d \leq \vec t$.
Следовательно $|A_M(\Xl)| = (\lfloor t_1 / \rho \rfloor + 1) (\lfloor t_2 / \rho \rfloor + 1) \dots (\lfloor t_h / \rho \rfloor + 1)$. Обозначим $T(\vec v) = \prod_j (v_j + 1)$. Тогда $|A(\Xl)| = T(\lfloor \vec t / \rho \rfloor)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из $U_1$, лежащих в обучении.
Тогда $\delta (a_\lambda, \Xl) = \frac{m - s + \rho |\lambda|}{k} - \frac{s}{\ell}$, и условие
$\delta (a_\lambda, \Xl) \geq \epsilon$ запишется в виде $s \leq \frac \ell L [m + \rho |\lambda| - \epsilon k] \equiv s_0$.
Множество всех разбиений из $\XXell_{\vec t}$ с фиксированным параметром $s$ обозначим
через $\XXell_{\vec t, s}$. Тогда
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac 1 {T(\lfloor \vec t / \rho \rfloor)}
                         {\sum_{s = 0}^{s_0} |\XXell_{\vec t, s}|}.
\]

\textbf{Шаг 4. } Вычислим мощность множества $\XXell_{\vec t, s}$.

Введем обозначения $\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,
$k' = k - |\vec t|$, $L' = \ell' + k'$. Тогда простое комбинаторное вычисление показывает, что
$|\XXell_{\vec t, s}| = C_m^s C_{L' - m}^{k' - s}$.
Следовательно,
\[
    Q_\mu(\epsilon, A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac 1 {T(\lfloor \vec t / \rho \rfloor)}
                         {\sum_{s = 0}^{s_0} C_m^s C_{L' - m}^{k' - s}}.
\]
Напомним, что
$H_{L'}^{\ell', m}(z) = \frac {1}{C_{L'}^{\ell'}}\sum \limits_{s=0}^{\lfloor z \rfloor} C_m^s C_{L'-m}^{\ell' - s}$
"--- функция гипергеометрического распределения \cite{voron09mmro}. Тогда
\[
    Q_\mu(\epsilon, A_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T(\lfloor \vec t / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0). \; \blacksquare
\]

\subsection{Разреженная унимодальная сетка-2}

Унимодальная сетка является более реалистичной моделью связного параметрического семейства,
по~сравнению с~монотонной сеткой. Если мы имеем лучший алгоритм~$a_0$ c~оптимальным значением
вектора вещественных параметров, то~отклонение значений компонент этого вектора как в~б\'ольшую, так~и~в~меньшую,
сторону приводит к~увеличению числа ошибок.

\begin{Def}
    Множество алгоритмов
    $A_U = \bigl\{a_{\vec d}\bigr\}$, где $\|\vec d\|\leq D$
    называется \emph{унимодальной $h$"~мерной сеткой алгоритмов},
    если существует $h \in \NN$ и упорядоченные наборы объектов
    $X_j = \{x_j^1, x_j^2, \dots, x_j^D\} \subset \XX$,
    $Y_j = \{y_j^1, y_j^2, \dots, y_j^D\} \subset \XX$,
    для всех $j = 1, \dots, h$,
    а так же множества $U_1 \subset \XX$ и $U_0 \subset \XX$,
    такие что выполнены условия:
    \begin{enumerate}
        \item Набор $\Big\{ U_0, U_1,\{X_j\}_{j=1}^h, \{Y_j\}_{j=1}^h \Big\}$ является разбиением множества $\XX$
              на непересекающиеся множества;
        \item $a_d(x_j^i) = [d_j > 0] \left[ i \leq |d_j| \right]$, где $x_j^i \in  X_j$;
        \item $a_d(y_j^i) = [d_j < 0] \left[ i \leq |d_j| \right]$, где $y_j^i \in  Y_j$;
        \item $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Def}

Заметим, что данное определение отличается от определения монотонной сетки отсутствием ограничения
$\vec d \geq 0$. Число алгоритмов в $h$-мерной унимодальной сетке с ветвями длины $D$ составляет
$(2 D + 1)^h$.
\emph{Укороченной} $h$-мерной унимодальной сеткой $\tilde{A}_U$ назовем множество первых $D$ слоев из $A_U$:
$\tilde{A}_U = \{a_{\vec d} \in A_U \colon n(a_{\vec d}, \XX) \leq m + D\}$.

\begin{figure}[t]
    \label{fig:UnimodalSets}
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_U.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=75mm,height=56mm]{netpics_US.eps}
    \hfill
    \end {multicols}
    \caption{Матрица ошибок унимодальной сетки (слева) и укороченной унимодальной сетки (справа)
    при $D=10$, $h=2$, $m=5$, $L=60$.}
\end{figure}

Формула для вероятности переобучения \emph{пессимистического} метода минимизации эмпирического риска
на укороченных унимодальных сетках так же была получена в \cite{botov09mmro}.
Ниже рассматриваются не-укороченные унимодальные сетки и
случай \emph{рандомизированного} метода минимизации эмпирического риска.

\begin{Def}
Пусть~$\rho \in \NN$ "--- целочисленный параметр;
$A_U=\{a_{\vec d}\}$, где $\|d\| \leq \rho D$ "--- $h$"~мерная унимодальная сетка;
$m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$"~мерной унимодальной сеткой $\ddot{A}_U$ плотности~$\rho$}
будем называть следующее подмножество $A_U$:
\[
\begin{aligned}
    \ddot{A}_U =
        \left\{a_{\vec d} \in A_U \, \big| \, \vec d \in \left( \tiny \rho \ZZ \tiny \right)^h \right\}.
\end{aligned}
\]
\end{Def}

\begin{Lemma}
Группа симметрии разреженной унимодальной сетки размерности $h$ содержит в качестве подгруппы группу $\Sym(A_U) = (S_2)^h \times S_h$.
Группа $S_h$ действует на множестве пар $\left(X_j, Y_j\right)_{j=1}^h$ всеми возможными перестановками;
$j$-тая группа $S_2$ переставляет объекты множества $X_j$ и $Y_j$ местами, сохраняя относительный порядок объектов.
\end{Lemma}

$\square$ \textbf{Доказательство.}

Все алгоритмы разреженной $h$"~мерной унимодальной сетки длины $D$
индексированы множеством вектор-индексов $\vec d \in \{-D, \dots, D\}^h$.
Тут число ошибок алгоритма $a_{\vec d} = m + \rho |\vec d|$.

Рассмотрим алгоритм $a_{\vec d} \in \ddot{A}_U$ и произвольную $\pi = (z_1, \dots, z_h) \times \pi_0 \in \Sym(\ddot{A}_U)$,
где $z_j \in S_2$, $\pi_0 \in S_h$.
По данному выше определению действия $\pi$ на $\XX$ получаем, что
$\pi a_{\vec d} = a_{ \pi \vec d}$, где действие $\pi$ на вектор $\vec d$
определяется перестановкой его координат с помощью $\pi_0$ и инверсией знаков
для всех $j$, таких что $z_j \neq id$ "--- транспозиция.
Множество $\{-D, \dots, D\}^h$ сохраняется при применении к нему
произвольной перестановки координат $\pi \in (S_2)^h \times S_h$.
Поэтому $\forall \vec d \in \{-D, \dots, D\}^h$ выполнено $\pi \vec d \in \{-D, \dots, D\}^h$.
А следовательно $a_{\pi \vec d} \in \ddot{A}_U$.
$\; \blacksquare$

\begin{Lemma}
\label{eq:unimodalSetOrbits}
Множество орбит разреженной унимодальной сетки $\ddot{A}_U = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$
под действием $\Sym(\ddot{A}_U)$ индексировано всевозможными диаграммами Юнга из $Y_{*}^{h, D}$.
Пусть $\lambda = (\lambda_1, \dots, \lambda_h) \in Y_{*}^{h, D}$. Обозначим через $|S_h \lambda|$
число различных слов длины $h$, состоящих из символов $\lambda_1, \dots, \lambda_h$
Пусть $|\lambda > 0|$ "--- число строго положительных компонент вектора $\lambda$.

Тогда число алгоритмов в орбите $\omega_\lambda$ равно $|S_h \lambda| \cdot 2^{|\lambda > 0|}$.
\end{Lemma}

$\square$ \; \textbf{Доказательство} полностью повторяет рассуждения леммы \ref{eq:monotonicSetOrbits}.
Множитель $2^{|\lambda > 0|}$ соответствует возможности сменить знак у всех
не-нулевых компонент вектора $\vec d$.
$\blacksquare$

\begin{Theorem}
\label{th:unimodalNetFormula}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к разреженной унимодальной сетке $\ddot{A}_U = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
дается выражением:
\begin{equation}
\label{eq:unimodalNetFormula}
\begin{aligned}
    Q_\mu( \epsilon, A_U) & =
                         \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq \rho D}}
                         \mathbb{S}(\lambda, \vec t, \vec {t'}), \\
                          \mathbb{S}(\lambda, \vec t, \vec {t'})
                          & =
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                            {T(\lfloor \vec t / \rho \rfloor + \lfloor \vec {t'} / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\end{aligned}
\end{equation}
где
%$Y_{*}^{h, D}$ "--- множество невозрастающих последовательностей длины $h$ и не превосходящих $D$,
%$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на $\lambda$,
%$T(\vec t, \vec {t'}) = \prod_j (t_j + t'_j + 1)$,
$\ell' = \ell - \sum \limits_{j = 1}^h \left( [t_j \neq \rho D] + [t'_j \neq \rho D] \right)$,
$k' = k - |\vec t| - |\vec t'|$,
%$L' = \ell' + k'$,
а~остальные обозначения совпадают с~обозначениями теоремы~\ref{th:monotonicNetFormula}.
%$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,
%$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения \cite{voron09mmro}.
\end{Theorem}

$\square$ \textbf{Доказательство.}

\begin{figure}[t]
    \centering
    \includegraphics[height=140mm]{unimod_net_illustration.eps}
    \caption{Строение множества $A_U(X)$ для двумерной унимодальной сетки.}
\end{figure}

\textbf{Шаг 1. }
Выберем в качестве представителя $a_\lambda$ орбиты $\omega_\lambda$ алгоритм, не допускающий ошибок на
множестве $Y = \bigcup_{j=1}^h Y_j$. Этого можно добиться, взяв произвольный  $a_{\vec d} \in \omega_\lambda$ и поменяв
знаки у всех $d_j < 0$ c помощью транспозиции $z_j$.

Введя обозначения $\vec t$ и $\XXell_{\vec t}$ так же, как и на первом шаге вывода формулы для монотонной сетки,
получим
\[
    Q_\mu(\epsilon, \ddot{A}_U) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|\ddot{A}_U(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 2. }
Обозначим через $t'_j$ максимальный индекс из $\{0, \dots, \rho D\}$, при котором все объекты
$\{y_j^1, \dots, y_j^{t'_j}\}$ содержатся в $\Xk$, а $y_j^{t'_j + 1}$, при его наличии, лежит в $\Xl$.
Положим $\vec {t'} = \{t'_j\}_{j = 1}^h$. Заметим, что вектор $\vec {t'}$ играет для набора $\{Y_j\}$ ту же
роль, что $\vec t$ для $\{X_j\}$. Обозначим через $\XXell_{\vec t, \vec {t'}}$ множество разбиений с
фиксированными параметрами $\vec t$ и $\vec {t'}$.

Пусть $\Xl \in \XXell_{\vec t, \vec {t'}}$.
Заметим, что алгоритм $[a_{\vec d} \in \ddot{A}_U(\Xl)] = [-\vec {t'} \leq \rho \vec d \leq \vec t]$.
Следовательно $|\ddot{A}_U(\Xl)| = T(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из $U_1$, лежащих в обучении.
Пусть $s_0 \equiv \frac \ell L [m + \rho |\lambda| - \epsilon k]$.
Повторяя рассуждения аналогичного шага доказательства для монотонной сетки получим
\[
    Q_\mu(\epsilon, \ddot{A}_U) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                \]

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=80mm,height=60mm]{rarefield_monotonic.eps}
    \hfill
    \caption{Зависимость $Q_\mu(\epsilon, \ddot{A}_M)$ от разреженности $\rho$ монотонной сетки
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1,2,3,4$.}
    \label{fig:MonotRarefieldNet}
    \medskip
    \hfill
    \includegraphics[width=80mm,height=60mm]{rarefield_unimodal_vs_monotonic.eps}
    \hfill
    \caption{Сравнение $Q_\mu(\epsilon, \ddot{A}_M)$ и $Q_\mu(\epsilon, \ddot{A}_U)$ от~$\rho$
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1(2), 2(4)$.}
    \label{fig:UnimodRarefieldNet}
    \end {multicols}
\end{figure}

Приведем результаты численных расчетов,
иллюстрирующих поведение вероятности переобучения
монотонной и унимодальной разреженных сеток.
Расчеты выполнены с помощью доказанных выше формул
\eqref{eq:monotonicNetFormula}, \eqref{eq:unimodalNetFormula}.

На~рис. \ref{fig:MonotRarefieldNet} изображена зависимость
вероятности переобучения $h$"~мерной монотонной сетки при $h=1, 2, 3, 4$ от~разреженности $\rho$.
При~увеличении размерности вероятности переобучения также возрастает.
При~увеличении разреженности $\rho$ вероятность переобучения падает, и вскоре выходит на~константу,
соответствующую вероятности переобучения лучшего алгоритма семейства $a_0$.
Это связано с~тем, что с~уменьшением плотности семейства
возрастает роль \emph{явления расслоения} \cite{voron09dan, voron09mmro}.

На~рис. \ref{fig:UnimodRarefieldNet} приведены результаты сравнения разреженных
$h$"~мерных унимодальных сеток
с~разреженными $2h$-мерными монотонными сетками, при~$h=1$ и~$h=2$.
Тонкая серая кривая соответствует вероятности переобучения для~унимодальной сетки.
Полученные результаты подтверждают гипотезу \cite{botov09mmro} о~связи вероятности переобучения для~унимодальных
сеток с~вероятностью переобучения монотонных сеток удвоенной размерности.
\[
         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq \rho D}}
                         \frac 1 {T(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor)}
                         {\sum_{s = 0}^{s_0} |\XXell_{\vec t, \vec {t'}, s}|}.
\]

\textbf{Шаг 4. } Посчитаем мощность множества $\XXell_{\vec t, \vec {t'}, s}$.

Обозначим $\ell' = \ell - \sum \limits_{j = 1}^h \left( [t_j \neq \rho D] + [t'_j \neq \rho D] \right)$,
$k' = k - |\vec t| - |\vec t'|$, $L' = \ell' + k'$. Тогда
$|\XXell_{\vec t, \vec {t'}, s}| = C_m^s C_{L' - m}^{k' - s}$.
Воспользовавшись определением функции гипергеометрического распределения получим:
\[
    Q_\mu(\epsilon, A_U) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,            \\\|\vec {t'}\| \leq \rho D}}
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                               {T(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0). \;\blacksquare
\]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\subsection {Опорное подмножество алгоритмов}

Множество ошибок алгоритма $a \in A$ обозначим через $E(a) \subset \XX$.

\begin{Def}
    Подмножество $B \subset A$ множества алгоритмов $A$ будем называть \emph{опорным}, если
    для всех $a \in A$ найдется натуральное число $k \in \NN$ и набор $\{b_i\}_{i=1}^k$,
    такой что для всех $i = 1, \dots, k$ множество ошибок $E(b_i) \subset E(a)$, и кроме того
    $\bigcup \limits_{i=1}^k {E(b_i)} = E(a)$.
    Опорное множество $B \subset A$ назовем \emph{минимальным},
    если любое его собственное подмножество уже не является опорным.
\end{Def}

%\textcolor{red}{[ToDo]} Проверить, может ли в множестве алгоритмов быть два различных минимальных опорных множества,
%или сформулировать условия, при которых минимальное опорное множество единственно.

Перечислим без доказательства очевидные свойства произвольного опорного множества $B \subset A$.
\begin{State}
Пусть $\Xl \in \XXell$, $a \in A$, $\{b_i\}_{i=1}^k \subset B$ "--- некоторое разложение $a$
по опорному множеству $B$. Тогда
\begin{itemize}
  \item $\min \limits_{a \in A} n(a, \Xl) = \min \limits_{b \in B} n(b, \Xl)$;
  \item $B(\Xl) = A(\Xl) \cap B$;
  \item Если $a \in A(\Xl)$ то все $b_i$ тоже лежат в $A(\Xl)$;
  \item Пусть $a \in A(\Xl)$. Тогда для всех $b_i$ выполнено $\delta (b_i, \Xl) \leq \delta (a, \Xl)$.
\end{itemize}
\end{State}

Легко установить, что связка из $h$ монотонных цепочек является минимальным опорным подмножеством
для монотонной сетки размерности $h$.
Связка из $2h$ монотонных цепочек является минимальным опорном множеством унимодальной сетки
размерности $h$.
Монотонная сетка размерности $2h$ содержит в качестве опорного множества
унимодальную сетку размерности $h$.

\begin{Hypothesis}
\label{hyp:supportingAlgs}
Пусть $B \subset A$ "--- опорное подмножество.
Тогда $Q_\mu(\epsilon, B) \leq Q_\mu(\epsilon, A)$.
\end{Hypothesis}

На рис. \ref{fig:netcomp_eps} и \ref{fig:netcomp_h} приведены результаты численных экспериментов,
подтверждающих данную гипотезу для случая связок из монотонных цепочек, монотонных и
унимодальных сеток. Верхняя кривая на всех рисунках соответствует монотонной сетке,
средняя кривая "--- унимодальной сетке, нижняя "--- связке монотонных цепочек.
В эксперименте использовались точные формулы, полученные в предыдущих параграфах.

Увы, в общем случае гипотеза \ref{hyp:supportingAlgs} не верна. Известны контрпримеры.

\begin{figure}[p]
    \centering
    \includegraphics[height=80mm]{netcomparison_D5.eps}
    \caption{
        Сравнение $Q(\epsilon)$ для
        связки из $p = 4$ монотонных цепочек,
        монотонной сетки размерности $h = 4$
        и унимодальной сетки размерности $h = 2$.
        Значения параметров $D=5$, $m = 5$, $L = 50$, $\ell = 30$.
    }
    \label{fig:netcomp_eps}
    \vspace{1cm}
    \begin {multicols}{3}
    \centering
    \hfill
    \includegraphics[width=48mm,height=36mm]{netcomparison_h12.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=48mm,height=36mm]{netcomparison_h24.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=48mm,height=36mm]{netcomparison_h36.eps}
    \hfill
    \end {multicols}
    \caption{
        Сравнение $Q(D)$ для
        связки из $2h$ монотонных цепочек,
        $2h$-мерной монотонной сетки
        и $h$-мерной унимодальной сетки.
    Значения параметров $\epsilon = 0.04$, $m = 5$, $L = 50$, $\ell = 30$, $D = 1, \dots, 5$;
    размерности унимодальных сеток $h = 1, 2, 3$ (слева направо).}
    \label{fig:netcomp_h}
\end{figure}

\newpage

\section {\textcolor{red}{[ToDo]}}

\begin{itemize}
  \item Проверить в eLT формулы для шара и сечения шара центральным слоем
  \item Прочитать статью Дениса Кочедыкова
  \item (+) Доказать или опровергнуть гипотезу \ref{hyp:supportingAlgs} о опорных подмножествах
  \item Сделать раздел <<неравенства для оценивания вероятности переобучения>>
  \item (+) Разобраться с Error correction codes и придумать несвязное множество, рассеянное по слою/шару/кубу
  \item Выбрать прикладную задачу и применить к ней данную теорию
\end{itemize}

\section{Заключение}

Свойство симметрии семейств алгоритмов позволяет получать
вычислительно эффективные формулы вероятности переобучения.
Для~монотонной цепочки, унимодальной цепочки и~единичной окрестности
такие формулы получены как следствие одной теоремы,
в~то~время как ранее аналогичные оценки доказывались независимо
и~при неестественном предположении об~априорной упорядоченности алгоритмов в~семействе~\cite{voron09dan}.
Примененный подход позволил получать оценки для семейств с экспоненциально растущим числом алгоритмов
(полный слой алгоритмов, куб и шар алгоритмов).

Получены формулы для вероятности переобучения рандомизированного метода минимизации эмпирического риска
на монотонных и унимодальных сетках произвольной размерности. Экспериментально показано, что в
широком диапазоне параметров вероятность переобучения и монотонных, и унимодальных сеток
оценивается снизу вероятностью переобучения связки из соответствующего количества монотонных цепочек.

Работа поддержана РФФИ (проект \No\,08-07-00422) и~программой ОМН~РАН
<<Алгебраические и~комбинаторные методы математической кибернетики
и~информационные системы нового поколения>>.

\newpage

\def\BibAuthor#1{\emph{#1}}
\def\BibTitle#1{#1}
\def\BibUrl#1{{\small\url{#1}}}
\def\BibHttp#1{{\small\url{http://#1}}}
\def\BibFtp#1{{\small\url{ftp://#1}}}
\def\typeBibItem{\small\sloppy}

\begin{thebibliography}{1}
\bibitem{vapnik74rus}
    \BibAuthor{Вапник\;В.\,Н., Червоненкис\;А.\,Я.}
    \BibTitle{Теория распознавания  образов}. "---
    М.:~Наука, 1974.
\bibitem{vapnik98stat}
    \BibAuthor{Vapnik~V.}
    \BibTitle{Statistical Learning Theory}. "---
    New York: Wiley, 1998.
\bibitem{voron09dan}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Точные оценки вероятности переобучения}~//
    Доклады РАН, 2009. "--- Т.\,429, \No\,1.  "--- С.\,15--18.
\bibitem{voron09mmro}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Комбинаторный подход к~проблеме переобучения}~//
    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009. "---  \mbox{С.\,18--21}.
\bibitem{botov09mmro}
    \BibAuthor{Ботов\;П.\,В.}
    \BibTitle{Точные оценки вероятности переобучения для монотонных и~унимодальных семейств алгоритмов}~//
    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,7--10}.
\bibitem{frey09mmro}
    \BibAuthor{Фрей\;А.\,И.}
    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов}~//
    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,66--69}.
\bibitem{knuth98concrete}
    \BibAuthor{Грэхем\;Р., Кнут\;Д., Паташник\;О.}
    \BibTitle{Конкретная математика}. "---
    М.:~Мир, 1998.
\bibitem{vinberg2001}
    \BibAuthor{Винберг\;Э.\,Б.}
    \BibTitle{Курс алгебры}~//
    М.:~Факториал Пресс, 2001. "--- 544~с.
\end{thebibliography}

\end{document}
