\documentclass[twoside]{article}
\usepackage{iip8}
\NOREVIEWERNOTES

\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}
\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\epsilon}{\varepsilon}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mur}[3]{\mu({#1}, {#2})({#3})}

\begin{document}

\title{Вероятность переобучения плотных и~разреженных многомерных~сеток алгоритмов}
\author{Фрей~А.\,И.}
\email{sashafrey@gmail.com}
\organization{Московский Физико-технический институт}
\thanks{Работа поддержана РФФИ (проект \No\,08-07-00422) и~программой ОМН~РАН
    <<Алгебраические и~комбинаторные методы математической кибернетики
    и~информационные системы нового поколения>>.}
\abstract{
    Известно, что для получения высокоточных оценок обобщающей способности в~задачах обучения по прецедентам
    необходимо вводить более <<тонкие>> характеристики семейства алгоритмов, чем размерность.
    Предлагаются две такие характеристики "--- высота и~разреженность.
    Показывается, что для монотонных и~унимодальных многомерных сеток алгоритмов
    этих характеристик достаточно для получения точной оценки вероятности переобучения.
    Исследуется зависимость вероятности переобучения от~размерности, высоты и~разреженности
    для метода рандомизированной минимизации эмпирического риска.
}

\titleEng{The probability of overfitting for dense and sparse multidimensional~grids~of~classifiers}
\authorEng{Frei~A.\,I.}
\organizationEng{Moscow Institute of Physics and Technology, Moscow, Russia}
\abstractEng{
    The dimensional characteristics of the hypotheses set are known to be insufficient for obtaining tight generalization bounds.
    We~propose two novel characteristics~--- the height and the sparsity of the hypotheses set,
    and show that they are sufficient to obtain exact generalization bounds for two special sets~---
    the monotonic and unimodal multidimensional grids of classifiers.
    Then we study how the probability of~overfitting depends on~dimension, height, and sparsity
    in~the case of randomized empirical risk minimization.
}
\maketitle

\section{Введение}

Для построения надёжных методов обучения по~прецедентам
необходимо иметь как можно более точные оценки обобщающей способности,
например, в~виде верхних оценок вероятности переобучения.
Известные оценки используют различные характеристики
семейства алгоритмов, метода обучения и~обучающей выборки~\cite{boucheron05theory},
в~первую очередь, сложность (размерность) семейства алгоритмов.
Точность таких оценок может быть недостаточной для практических применений~\cite{voron08pria-eng}.
Показано~\cite{voron09roai2008,voron10pria-eng},
что надёжное обучение возможно только для семейств, обладающих одновременно двумя свойствами "---
расслоением алгоритмов по~частотам ошибок и~сходством алгоритмов.

\emph{Эффект расслоения} состоит в~том, что при фиксированной обучающей выборке
малую частоту ошибок имеет лишь малая доля алгоритмов.
\mbox{Вероятность} получить алгоритм в~результате обучения резко падает с~ростом его частоты ошибок.
Поэтому достаточно оценивать сложность лишь нижних слоёв семейства,
иначе оценки вероятности переобучения окажутся сильно завышенными.

\emph{Эффект сходства} состоит в~том, что
схожие алгоритмы вносят существенно меньший вклад в~вероятность переобучения, чем несхожие.
Поэтому вероятность переобучения у~семейств, непрерывных по~параметрам,
может оказаться вполне приемлемой даже при высокой размерности.

В~\cite{voron09roai2008,voron10pria-eng} было также показано, что
учёт этих двух эффектов по~отдельности
в~общем случае не~даёт численно точных оценок вероятности переобучения.
Но~при этом не~было предложено каких"=либо удобных (желательно, скалярных)
численных характеристик расслоения и сходства.
Векторную характеристику \emph{профиля расслоения} пока удавалось оценивать только
с~помощью весьма трудоёмкого метода Монте"=Карло~\cite{kochedykov09mmro}.

В~данной работе вводятся скалярные характеристики \emph{высоты} и \emph{разреженности} семейства алгоритмов,
связанные со~свойствами расслоения и~сходства, соответственно.
Рассматриваются модельные семейства "--- многомерные монотонные и~унимодальные сетки алгоритмов,
для которых этих двух характеристик (в~дополнение к~длине выборке и~размерности)
оказывается достаточно для получения точной оценки вероятности переобучения.
Эти~семейства впервые были рассмотрены в~\cite{botov09mmro},
а~их одномерные частные случаи "--- монотонные и~унимодальные цепочки алгоритмов "---
в~\cite{voron09roai2008,voron10pria-eng}.
Отличие данной работы в~том, что
вводится понятие разреженности семейства
и~применяется теоретико"=групповой подход~\cite{frey09mmro}
для рандомизированного (а~не~пессимистичного, как в~\cite{botov09mmro})
метода минимизации эмпирического риска.

Таким образом, показана принципиальная возможность получения точных оценок вероятности переобучения,
в~которых свойства размерности, расслоения и~сходства выражаются тремя скалярными характеристиками, соответственно.

\section{Постановка задачи}

Пусть ${\XX= \left(x_i\right)_{i=1}^L}$ "--- генеральная выборка, состоящая из $L$~объектов.
Отображения $a\colon\XX\to\{0,1\}$ будем называть алгоритмами
и~говорить, что алгоритм~$a$ допускает ошибку на~объекте~$x_i$, если ${a(x_i) = 1}$.
Каждому алгоритму взаимно однозначно соответствует
бинарный вектор ошибок~$\bigl( a(x_i) \bigr){}_{i=1}^L$.

Величина $n(a, U) = \sum \limits_{x \in U}a(x)$ называется
\emph{числом ошибок} алгоритма~$a$ на~подвыборке ${U \subseteq \XX}$.

Величина ${\nu(a, U) = n(a, U) / |U|}$ называется
\emph{частотой ошибок} алгоритма~$a$ на~подвыборке ${U \subseteq \XX}$.

Обозначим
через $\AA = \{0, 1\}^L$ множество всех $2^L$ бинарных векторов длины~$L$,
тогда $2^{\AA}$ "--- это множество всех подмножеств~$\AA$.

Обозначим через~$\XXell$ множество всех $\ell$"=элементных подмножеств генеральной выборки~$\XX$.

\emph{Детерминированным методом обучения} назовем произвольное отображение
${\mu \colon 2^\AA \times \XXell \rightarrow \AA}$,
которое по~обучающей выборке~${\Xl \in \XXell}$
выбирает из~подмножества ${A \subseteq \AA}$
некоторый алгоритм ${a=\mu(A,\Xl)}$.
Метод~$\mu$ называется \emph{минимизацией эмпирического риска} (МЭР), если
выбираемый им алгоритм допускает наименьшее число ошибок на~обучении:
для всех $\Xl \in \XXell$ и~$A \subseteq \AA$
\[
    \mu (A, \Xl) \in A(\Xl)
    \equiv
    \mathrm{Arg}\min_{a \in A} n(a, \Xl).
\]

Вопрос о~том, какой именно алгоритм $a$~из~$A(X)$ выдать в~результате обучения, может решаться по"=разному.
В~пессимистичном методе МЭР выбирается алгоритм с~максимальным $n(a,\XX)$,
что приводит к~верхним оценкам вероятности переобучения~\cite{voron10pria-eng,botov09mmro}.
Мы~рассматриваем \emph{рандомизированный метод обучения}~\cite{frey09mmro,frey10pria},
который произвольным ${A \in 2^\AA}$ и~${\Xl \in \XXell}$
ставит в~соответствие не~один алгоритм~$a$,
а~нормированную функцию~$f(a)$, значение которой можно интерпретировать как
вероятность получить алгоритм~$a$ в~результате обучения:
\begin{equation}
    \label{eq:randomizedSearchMethodDraft}
    \mu\colon 2^\AA \times \XXell
    \rightarrow
    \Bigl\{
        f\colon \AA \rightarrow [0, 1]
    \Bigm|
        \tsum_{a \in \AA} f(a) = 1
    \Bigr\}.
\end{equation}

Примером рандомизированного метода обучения
является \emph{рандомизированный метод МЭР},
основанный на равновероятном выборе $a$~из~$A(\Xl)$:
\begin{equation}
    \label{eq:randomizedRiskMinimization}
    \mur{A}{X}{a}
    =
    \bigl[ a \in A(\Xl) \bigr] / \bigl| A(\Xl) \bigr|.
\end{equation}
Тут и~далее квадратные скобки переводят логическое выражение в~число:
$[\text{истина}] = 1$, $[\text{ложь}] = 0$.

Пусть ${\Xl \sqcup \Xk = \XX}$ "--- произвольное разбиение генеральной выборки
на~обучающую выборку ${\Xl \in \XXell}$ и~контрольную ${\Xk = \XX \backslash \Xl}$.
\emph{Уклонением частот} назовем разность частот ошибок на~контроле и~на~обучении:
$\delta(a, \Xl) = \nu(a, \Xk) - \nu(a, \Xl)$.

Зафиксируем параметр $\epsilon \in (0, 1]$.

Будем говорить, что алгоритм~$a$ \emph{переобучен} при~разбиении $\Xl \sqcup \Xk$,
если $\delta(a, \Xl) \geq \epsilon$.

Примем единственное вероятностное предположение, что
все разбиения генеральной выборки $\XX$ на две подвыборки "---
наблюдаемую обучающую~$\Xl$ и~скрытую контрольную~$\Xk$ "---
равновероятны~\cite{voron10pria-eng}.
\mbox{Тогда} \emph{вероятность переобучения}
для детерминированного метода обучения~$\mu$ определяется как
\[
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell} \sum_{\Xl \in \XXell}
    \bigl[ \delta(\mu(A, X), \Xl) \geq \epsilon \bigr],
\]
а для рандомизированного метода~$\mu$ "--- как
\[
    Q_\mu(\epsilon, A)
    =
    \frac1{C_L^\ell}
        \sum_{\Xl \in \XXell}
        \sum_{a \in A}
    \mur{A}{\Xl}{a}
    \bigl[ \delta(a, \Xl) \geq \epsilon \bigr].
\]

\section{Монотонная сетка алгоритмов}

Пусть $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$ "--- целочисленный вектор индексов.
Обозначим $\|\vec d\| = \max \limits_{j=1, \dots, h} |d_j|$,
$|\vec d| = |d_1| +\, \dots\, + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d \leq \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$;
и~$\vec d < \vec d'$ если хотя~бы одно из~неравенств $d_j \leq d'_j$ строгое.

\begin{Def}
    \label{eq:monotonicSet}
    Множество алгоритмов
    $A_M = \bigl\{a_{\vec d}\colon \vec d \geq \vec 0,\; \|\vec d\|\leq D \bigr\}$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов высоты $D$},
    если $\XX$ разбивается на непересекающиеся подмножества
    $U_0$, $U_1$ и~$X_j = \{x_j^1, \dots, x_j^D\}$,\; ${j = 1, \dots, h}$,
    такие, что:
    \begin{enumerate*}
        \item $a_{\vec d}(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item $a_{\vec d}(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_{\vec d}(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate*}
\end{Def}

Монотонная сетка "--- это модель семейства алгоритмов с~$h$~непрерывными параметрами,
предполагающая, что по~мере непрерывного увеличения $j$-го параметра при фиксированных остальных
ошибки возникают последовательно на~объектах $x_j^1, \dots, x_j^D$.

Обозначим $m \equiv|U_1|$.
Из~определения \ref{eq:monotonicSet} следует, что $n(a_{\vec d}, \XX) = m + |\vec d|$.
Число алгоритмов в~$h$"~мерной монотонной сетке высоты~$D$ составляет $(D + 1)^h$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в сетке}.

\begin{Example}
    Двумерная ($h = 2$) монотонная сетка при $m = 0$ и~$L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} \hspace{-1ex} & a_{1,0} \hspace{-1ex} & a_{2,0} \hspace{-1ex} & a_{0,1} \hspace{-1ex} & a_{1,1} \hspace{-1ex} & a_{2,1} \hspace{-1ex} & a_{0,2} \hspace{-1ex} & a_{1,2} \hspace{-1ex} & a_{2,2} \hspace{-1ex} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}
\begin{Def}
    \emph{Разреженностью}~$\rho$ множества алгоритмов $A$ назовём
    минимальное хэммингово расстояние между векторами ошибок:
    \[
        \rho = \min_{a,a'\in A} \sum_{i=1}^L \bigl| a(x_i)-a'(x_i) \bigr|.
    \]
\end{Def}
\begin{Def}
    \emph{Разреженной $h$-мерной монотонной сеткой}
    c~\emph{разреженностью}~$\rho$
    называется подмножество
    $\ddot{A}_M =
            \bigl\{
                a_{\vec d} \in A_M \colon \vec d \in (\rho\ZZ)^h
            \bigr\}$.
\end{Def}

Если исходная сетка $A_M$ имела высоту $D$,
то~величину $\lfloor D \slash \rho \rfloor$
будем называть высотой сетки $\ddot{A}_M$.

%%%С~увеличением разреженности множества алгоритмов происходит
%%%уменьшение его \emph{связности}, но~возрастает роль \emph{расслоения}.
%%%Следовательно, изучение зависимости вероятности переобучения
%%%от~разреженности позволит сравнить вклады указанных явлений.

%Отметим, что при $\rho > 1$ граф смежности \cite{voron09dan, voron09mmro}
%разреженной монотонной сетки состоит из изолированных точек.

\begin{Example}
На~рис.\,\ref{fig:grid2D} выделено подмножество алгоритмов двумерной монотонной сетки высоты~$D = 8$,
соответствующее разреженной монотонной сетке с параметрами~$\rho = 2$, $D = 4$.
\end{Example}
\begin{figure}[t]
    \centering
    \includegraphics[height=45mm]{frey_fig1.eps}
    \caption{Разреженная монотонная сетка. Узлы сетки соответствуют алгоритмам,
        направление стрелок "--- возрастанию числа ошибок алгоритмов.}
    \label{fig:grid2D}
\end{figure}

Введем дополнительные обозначения:

$C_n^k$
"--- биномиальные коэффициенты, причём
будем считать, что $C_n^k = 0$ при $k < 0$ и $k > n$;

$H_{L}^{\ell, m}(z) =
    \frac {1}{C_{L}^{\ell}}\sum \limits_{s=0}^{\lfloor z \rfloor}
    C_m^s C_{L-m}^{\ell-s} $
"--- функция гипергеометрического распределения, $z\in[0,\ell]$;

$Y_{*}^{h, D} \subset \ZZ^h$
"--- множество целочисленных невозрастающих неотрицательных последовательностей длины~$h$,
первый член которых не~превосходит~$D$;

$S_h$
"--- группа перестановок элементов последовательности $\vec \lambda \in Y_{*}^{h, D}$;

$|S_h \vec \lambda|$
"--- мощность орбиты действия $S_h$ на $\vec \lambda$.

\REVIEWERNOTE{>правильно ли я понимаю, что только если в~последовательности есть повторы,
    то группа перестановок элементов такой последовательности не совпадает с~обычной симметрической группой?

[Саша] Да, именно так. По-сути $|S_h \vec \lambda|$ равно количеству различных слов, которые можно составить
из символов $\{\lambda_1, \dots, \lambda_h\}$. При наличии повторов <<букв>> это число будет меньше, чем $h!$.
}

\begin{Theorem}
\label{th:rarefieldMonotonicNet}
Пусть $\ddot{A}_M$ "--- разреженная $h$"~мерная монотонная сетка высоты~$D$ и~разреженности~$\rho$.
Тогда вероятность переобучения $Q_\mu(\epsilon, \ddot{A}_M)$
для рандомизированного метода минимизации эмпирического риска
дается формулой
\[
    Q_\mu(\epsilon, \ddot{A}_M) = \sum_{\vec \lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \rho \vec  \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \vec \lambda|} {T(\lfloor \vec t / \rho \rfloor)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}\left(s_0(\vec \lambda)\right),
\]
где
${\ell' = \ell - \sum\limits_{j = 1}^h [t_j {\neq} \rho D]}$,
${k' = k - |\vec t|}$,
${L' = \ell' + k'}$,
${T(\vec {t}) = \prod\limits_j (t_j + 1)}$,~~
${s_0(\vec \lambda) = \frac \ell L \left(m + \rho |\vec \lambda| - \epsilon k\right)}$.

\end{Theorem}

При $\rho = 1$ данная формула дает вероятность переобучения не-разреженной сетки алгоритмов $A_M$.

%\REVIEWERNOTE{При $\rho=1$ это формула даёт оценку в.п. исходной сетки~$A_M$?
%    Если да, то это стоит отметить.}

\REVIEWERNOTE{А~нельзя~ли посчитать аналитически
    $\sum_{\vec \lambda \in Y_{*}^{h, D}}  |S_h \vec \lambda|$?
    Наверняка сумма по~значениям мощности орбит будет иметь существенно меньше слагаемых.

    [Саша] Я думаю что посчитать это можно,
    но к сожалению <<факторизовать>> сумму по мощности множества $|S_h(\lambda)|$ не получится "---
    $s_0$ зависит от $\lambda$. Я исправил обозначение $s_0$ на $s_0(\vec \lambda)$.
}

\section{Унимодальная сетка алгоритмов}

Унимодальная сетка является более реалистичной моделью семейства c~$h$ непрерывными параметрами,
по~сравнению с~монотонной сеткой.
Предполагается, что непрерывное отклонение $j$-го параметра
не~только в~б\'ольшую, но~и~в~меньшую, сторону от~оптимального значения
приводит к~увеличению числа ошибок.

\begin{Def}
    Множество алгоритмов
    $A_U = \bigl\{{a_{\vec d}\colon \|\vec d\|\leq D }\bigr\}$
    называется \emph{унимодальной $h$"=мерной сеткой алгоритмов высоты $D$},
    если $\XX$ разбивается на~непересекающиеся подмножества
    $U_1$, $U_0$,
    $X_j = \{x_j^1, \dots, x_j^D\}$,\;
    $Y_j = \{y_j^1, \dots, y_j^D\}$,\; ${j = 1, \dots, h}$,
    такие, что:
    \begin{enumerate*}
        \item $a_{\vec d}(x_j^i) = \bigl[d_j > 0\bigr] \bigl[ i \leq |d_j| \bigr]$, где $x_j^i \in  X_j$;
        \item $a_{\vec d}(y_j^i) = \bigl[d_j < 0\bigr] \bigl[ i \leq |d_j| \bigr]$, где $y_j^i \in  Y_j$;
        \item $a_{\vec d}(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_{\vec d}(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate*}
\end{Def}

Данное определение отличается от~определения монотонной сетки отсутствием ограничения $\vec d \geq \vec 0$.
Число алгоритмов в~$h$"~мерной унимодальной сетке высоты~$D$ равно $(2 D + 1)^h$.
Как~и~для монотонной сетки,
число ошибок $n(a_{\vec d}, \XX)$ алгоритма $a_{\vec d}$ равно $m + |d|$, где $m \equiv|U_1|$.

\begin{Def}
\emph{Разреженной $h$"~мерной унимодальной сеткой с~разреженностью~$\rho$} называется подмножество
$\ddot{A}_U =
    \bigl\{
        a_{\vec d} \in A_U \colon
        \vec d \in (\rho \ZZ)^h
    \bigr\}$.
\end{Def}

Если исходная сетка $A_U$ имела высоту $D$,
то величину $\lfloor D \slash \rho \rfloor$ будем называть высотой сетки $\ddot{A}_U$.

Обозначим через $n(\vec \lambda)$ число ненулевых компонент последовательности $\vec \lambda \in Y_{*}^{h, D}$.

\begin{Theorem}
Пусть $\ddot{A}_U$ "--- разреженная $h$"=мерная унимодальная сетка высоты~$D$ и~разреженности~$\rho$.
Тогда вероятность переобучения $Q_\mu(\epsilon, \ddot{A}_U)$
для рандомизированного метода минимизации эмпирического риска
дается формулой
\begin{align*}
    Q_\mu( \epsilon, \ddot{A}_U) & =
        \sum_{\vec \lambda \in Y_{*}^{h, D}}
        \sum_{\substack{\vec  t   \geq \rho \vec \lambda, \\\|\vec  t  \| \leq \rho D}}
        \sum_{\substack{\vec {t'} \geq \vec 0,            \\\|\vec {t'}\| \leq \rho D}} \!\!\!
        \mathbb{S}(\vec \lambda, \vec t, \vec {t'}),
\\
    \mathbb{S}(\vec \lambda, \vec t, \vec {t'}) &=
        \frac
            {|S_h \vec \lambda| \cdot 2^{n(\vec \lambda)}}
            {T(\lfloor \vec t / \rho \rfloor + \lfloor \vec {t'} / \rho \rfloor)}
        \frac
            {C_{L'}^{\ell'}}
            {C_L^\ell}
        H_{L'}^{\ell', m}\left(s_0(\vec \lambda)\right),
\end{align*}
$\ell' = \ell - \sum \limits_{j = 1}^h \bigl( [t_j {\neq} \rho D] + [t'_j {\neq} \rho D] \bigr)$,\;
$k' = k - |\vec t| - |\vec t'|$,
остальные обозначения те~же, что в~теореме~\ref{th:rarefieldMonotonicNet}.
\end{Theorem}

\section{Вычислительный эксперимент}
На~рис.\,\ref{fig:MonotdNetFromHeight} показана зависимость вероятности переобучения
$h$"~мерной монотонной сетки от~ее~высоты.
При увеличении высоты сетки свыше $D=5$ вероятность переобучения выходит на константу.
\mbox{Поэтому} дальнейшие графики построены при малом значении параметра $D = 3$.

На~рис.\,\ref{fig:MonotRarefieldNet} изображена зависимость
вероятности переобучения $h$"~мерной монотонной сетки при $h=1, 2, 3, 4$ от~разреженности~$\rho$.
%При~увеличении размерности вероятность переобучения также возрастает.
При~увеличении разреженности~$\rho$ вероятность переобучения падает, и вскоре выходит на~константу,
соответствующую вероятности переобучения лучшего алгоритма семейства~$a_0$.
Это связано с~тем, что с~увеличением $\rho$
вероятность получить в~результате обучения алгоритм~$a_0$ стремится~к единице.

На~рис.\,\ref{fig:UnimodRarefieldNet} приведены результаты сравнения разреженных
$h$"~мерных унимодальных сеток
с~разреженными $2h$-мерными монотонными сетками, при~$h=1$ и~$h=2$.
Серая кривая соответствует вероятности переобучения унимодальной сетки.
Результаты подтверждают гипотезу~\cite{botov09mmro} о~том, что
вероятность переобучения унимодальной сетки и~монотонной сетки двойной размерности очень близки.

\begin{figure}[t]
    \centering
    \includegraphics[width=70mm,height=50mm]{frey_fig2.eps}
    \XYtext(-35mm,39.5mm){$h=4$}
    \XYtext(-35mm,32.5mm){$h=3$}
    \XYtext(-35mm,22mm){$h=2$}
    \XYtext(-35mm,14mm){$h=1$}
    \caption{Зависимость $Q_\mu(\epsilon, \ddot{A}_M)$ от высоты $D$ монотонной сетки
    при ${L=150}$,\; ${\ell=90}$,\; ${\epsilon = 0.05}$,\; ${m=5}$,\; ${\rho = 1}$.}
    \label{fig:MonotdNetFromHeight}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=70mm,height=45mm]{frey_fig3.eps}
    \XYtext(-57mm,37mm){$h\!=\!4$}
    \XYtext(-64mm,29mm){$h\!=\!3$}
    \XYtext(-64.5mm,21.5mm){$h\!=\!2$}
    \XYtext(-64mm,14.7mm){$h\!=\!1$}
    \caption{Зависимость $Q_\mu(\epsilon, \ddot{A}_M)$ от разреженности $\rho$
    при ${L=150}$,\; ${\ell=90}$,\; ${\epsilon = 0{,}05}$,\; ${D=3}$,\; ${m=5}$.\;}
    \label{fig:MonotRarefieldNet}
\end{figure}

\section{Выводы}

В~работе предложены два новых параметра множества алгоритмов "--- \emph{разреженность} и~\emph{высота}.
Для~разреженных монотонных и~унимодальных сеток показано, что
эти параметры, наряду с~длиной выборки и~размерностью, определяют вероятность переобучения.
Получены точные формулы вероятности переобучения
для~плотных (${\rho=1}$) и~разреженных (${\rho>1}$) многомерных монотонных и~унимодальных сеток
в~случае рандомизированной минимизации эмпирического риска.
С~помощью полученных формул экспериментально установлено, что в~широком диапазоне параметров
для~монотонных и~унимодальных сеток вероятность переобучения определяется несколькими нижними слоями семейства.
Также показано, что с~увеличением разреженности семейства вероятность получить
лучший алгоритм в~результате обучения стремится к~единице.

\begin{figure}[t]
    \centering
    \includegraphics[width=70mm,height=45mm]{frey_fig4.eps}
    \XYtext(-48.5mm,18.5mm){$h\!=\!2(4)$}
    \XYtext(-56mm,8.5mm){$h\!=\!1(2)$}
    \caption{Сравнение $Q_\mu(\epsilon, \ddot{A}_M)$ и $Q_\mu(\epsilon, \ddot{A}_U)$ от~$\rho$ при
    ${L=150}$,\, ${\ell=90}$,\, ${\epsilon = 0{,}05}$,\, ${D=3}$,\, ${m=5}$,\, ${h=1(2), 2(4)}$.}
    \label{fig:UnimodRarefieldNet}
\end{figure}

%%%\def\BibUrl#1.{}
%%%\bibliographystyle{gost71s}
%%%\bibliography{MachLearn}

\begin{thebibliography}{1}
\bibitem{botov09mmro}
    \BibAuthor{Ботов~П.\,В.}
    Точные оценки вероятности переобучения для монотонных и~унимодальных семейств алгоритмов~//
    Всеросс. конф. ММРО-14. "---
    М.:~МАКС Пресс, 2009. "---
    С.\,7--10.
\bibitem{kochedykov09mmro}
    \BibAuthor{Кочедыков~Д.\,А.}
    Структуры сходства в~семействах алгоритмов классификации и~оценки обобщающей способности~//
    Всеросс. конф. ММРО-14. "---
    М.:~МАКС Пресс, 2009. "---
    С.\,45--48.
\bibitem{frey09mmro}
    \BibAuthor{Фрей~А.\,И.}
    Точные оценки вероятности переобучения для симметричных семейств алгоритмов~//
    Всеросс. конф. ММРО-14. "---
    М.:~МАКС Пресс, 2009. "---
    С.\,66--69.
\bibitem{boucheron05theory}
    \BibAuthor{Boucheron~S., Bousquet~O., Lugosi~G.}
    Theory of classification: A~survey of some recent advances~//
    {ESAIM: Probability and Statistics}. "---
    2005. "---
    no.\,9. "---
    Pp.\,323--375.
\bibitem{frey10pria}
    \BibAuthor{Frey\;A.\,I.}
    %\BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов и рандомизированных методов обучения}~//
    \BibTitle{Accurate Estimates of the Generalization Ability for Symmetric Sets of Predictors and Randomized Learning Algorithms}~//
    Pattern Recognition and Image Analysis. "---
    2010. "---
    Vol.\,20, no.\,3. "---
    P.\,241--250.
\bibitem{voron08pria-eng}
    \BibAuthor{Vorontsov~K.\,V.}
    Combinatorial probability and the tightness of generalization bounds~//
    {Pattern Recognition and Image Analysis}.  "---
    2008. "---
    Vol.\,18, no.\,2. "---
    Pp.\,243--259.
\bibitem{voron09roai2008}
    \BibAuthor{Vorontsov~K.\,V.}
    Splitting and similarity phenomena in the sets of classifiers and their effect on the probability of overfitting~//
    {Pattern Recognition and Image Analysis}. "---
    2009. "---
    Vol.\,19, no.\,3. "---
    Pp.\,412--420.
\bibitem{voron10pria-eng}
    \BibAuthor{Vorontsov~K.\,V.}
    Exact combinatorial bounds on the probability of overfitting for empirical risk minimization~//
    {Pattern Recognition and Image Analysis}. "---
    2010. "---
    Vol.\,20, no.\,3. "---
    P.\,269--285.
\end{thebibliography}

%\REVIEWERNOTE{К~статье \cite{frey10pria}:
%    впиши английское название и все выходные данные (том, номер, страницы), которые у тебя уже есть (из препринта).}

%\begin{thebibliography}{1}
%\bibitem{vapnik74rus}
%    \BibAuthor{Вапник\;В.\,Н., Червоненкис\;А.\,Я.}
%    \BibTitle{Теория распознавания  образов}. "---
%    М.:~Наука, 1974.
%\bibitem{vapnik98stat}
%    \BibAuthor{Vapnik~V.}
%    \BibTitle{Statistical Learning Theory}. "---
%    New York: Wiley, 1998.
%\bibitem{voron09dan}
%    \BibAuthor{Воронцов\;К.\,В.}
%    \BibTitle{Точные оценки вероятности переобучения}~//
%    Доклады РАН, 2009. "--- Т.\,429, \No\,1.  "--- С.\,15--18.
%\bibitem{voron09mmro}
%    \BibAuthor{Воронцов\;К.\,В.}
%    \BibTitle{Комбинаторный подход к~проблеме переобучения}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009. "---  \mbox{С.\,18--21}.
%\bibitem{frey09mmro}
%    \BibAuthor{Фрей\;А.\,И.}
%    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,66--69}.
%\bibitem{frey10pria}
%    \BibAuthor{Фрей\;А.\,И.}
%    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов и рандомизированных методов обучения}~//
%    Pattern Recognition and Image Analysis. "--- 2010.
%\bibitem{botov09mmro}
%    \BibAuthor{Ботов\;П.\,В.}
%    \BibTitle{Точные оценки вероятности переобучения для монотонных и~унимодальных семейств алгоритмов}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,7--10}.
%\end{thebibliography}

\end{document}
