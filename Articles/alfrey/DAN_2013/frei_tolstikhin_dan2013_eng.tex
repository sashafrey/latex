\documentclass[unicode,14pt]{extarticle}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs}
\usepackage[russian]{babel}
\usepackage{theorem}
\usepackage{graphicx}
\usepackage{color}
\usepackage{indentfirst}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=0mm
\topmargin=-1cm
\parindent=24pt
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance=3000
\renewcommand{\baselinestretch}{1.19} %для печати с большим интервалом

% Абзацный отступ и пропорциональные ему отступы
\parindent=24pt
%\mathindent=24pt

\makeatletter
\theorembodyfont{\rmfamily\slshape}
\theoremstyle{plain}
% Шаманские притоптывания, чтобы ставить точку после номера теоремы
\gdef\th@plain{\normalfont
    \def\@begintheorem##1##2{%
        \item[\hskip\labelsep\theorem@headerfont ##1\ ##2.\;]}%
    \def\@opargbegintheorem##1##2##3{%
        \item[\hskip\labelsep\theorem@headerfont ##1\ ##2 (##3).\;]}%
}
\newtheorem{vkProblem}{Task}%[section]
\newtheorem{vkAxiom}{Axiom}%[section]
\newtheorem{vkTheorem}{Theorem}%[section]
\newtheorem{vkLemma}[vkTheorem]{Lemma}
\newtheorem{vkHyp}{Hypothesis}%[section]
\newtheorem{vkCorr}{Corollary}
\newtheorem{vkDef}{Definition}%[section]
\theorembodyfont{\rmfamily}
\newtheorem{vkExample}{Example}%[section]
\newtheorem{vkRemark}{Remark}%[section]
\newenvironment{vkProof}[1][.\;]%
    {\par\noindent{\bf Proof#1}}%
    {\hfill$\scriptstyle\blacksquare$\par\medskip}

%\newcommand{\proof}[1][Доказательство.]{\smallskip\noindent{\em #1}}
%\def\endproof{\hfill\par\medskip}
%\renewcommand{\refname}{Литература}

% Переопределение колонтитулов
\def\MYheadfoot{
\renewcommand{\@oddfoot}{}
\renewcommand{\@oddhead}{\hfil-- \thepage\ --\hfil}
\renewcommand{\@evenfoot}{}
\renewcommand{\@evenhead}{\hfil-- \thepage\ --\hfil}
} \MYheadfoot

% Нумерацию переподчиним разделам
\@addtoreset{equation}{section}
%\def\theequation{\thesection.\arabic{equation}}

% Оформление плавающих иллюстраций
\def\@caption@left@right@skip{\leftskip=24pt\rightskip=24pt}
\def\nocaptionskips{\def\@caption@left@right@skip{}}
\renewcommand\@makecaption[2]{%
    \vskip\abovecaptionskip
    \sbox\@tempboxa{\footnotesize\textbf{#1.} #2}%
    \ifdim\wd\@tempboxa >\hsize
        {\@caption@left@right@skip\footnotesize\textbf{#1.} #2\par}
    \else
        \global\@minipagefalse
        \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
    \fi
}

% Команды для формул
\newcommand{\XX}{\mathbb{X}}
%\newcommand{\XX}{U}
\newcommand{\XXell}{[\XX]^\ell}
\newcommand{\X}{\bar X}
\newcommand{\YY}{Y}
%\newcommand{\vA}{A}
%\newcommand{\va}{\tilde a}
%\renewcommand{\AA}{A}
%\renewcommand{\AA}{\mathbb{A}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\emptyset}{\varnothing}\newcommand{\emset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}\newcommand{\eps}{\varepsilon}
\renewcommand{\kappa}{\varkappa}
\renewcommand{\phi}{\varphi}
\newcommand{\Expect}{\mathsf{E}}
\def\Pr[#1]{\Prob\left[#1\right]}
\def\Prbig[#1]{\Prob\bigl[#1\bigr]}
\def\PrBig[#1]{\Prob\Bigl[#1\Bigr]}
\newcommand{\const}{\mathrm{const}}
\newcommand{\sign}{\mathop{\rm sign}\limits}
\newcommand{\@hyper@geom}[5]{{#1}_{#2}^{#4,#3}\left(#5\right)}
\newcommand{\hyper}[4]{\@hyper@geom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\@hyper@geom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\@hyper@geom{\bar{H}}{#1}{#2}{#3}{#4}}
\newcommand{\bv}{\vec}

\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}
\newcommand{\Q}{Q_\eps}
\newcommand{\Binom}[2]{C_{#1}^{#2}}
\newcommand{\CLl}{\Binom{L}{\ell}}
\newcommand{\Sym}{\mathop{\rm Sym}\limits}

\def\CC_#1^#2{\tbinom{#1}{#2}}

% Команды для текста
\renewcommand{\em}{\it}
\newcommand{\TODO}[1]{{%
    \color{red}\small{#1}%
    \marginpar{\raisebox{1ex}{\color{red}ToDo}}
}}
%\renewcommand{\TODO}[1]{}

% Скромная нумерация
\newcounter{vkItem}
\renewcommand{\thevkItem}{\arabic{vkItem}}
\newenvironment{vkItemize}{\setcounter{vkItem}{0}}{}
\newcommand{\vkItem}{\par\refstepcounter{vkItem}\thevkItem.\enspace}%  \hspace{\me}

% Перенос знака операции на следующую строку
\newcommand\brop[1]{#1\discretionary{}{\hbox{$#1$}}{}}

\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Cover-based combinatorial bounds on probability of overfitting}
\author{A.\,Frey, I.\,Tolstikhin}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\thispagestyle{empty}

For the last 40 years the problem of accurate overfitting bounds remains an active area of research within Statistical Learning Theory \cite{vapnik71convergence, boucheron05survey}.
For some special cases exact bounds were for the first time derived in combinatorial approach \cite{voron09dan, voron10pria}.
Further work \cite{voron11premi} derives general but less accurate combinatorial bounds based on the principle of protective and prohibitive sets.
Those bounds were successfully applied to the family of threshold conjunction rules,
and lead to a better quality of logical classification in practical machine learning tasks. 
However, those bounds were only applicable to connected sets of classifiers of a small cardinality.

In this work we propose better overfitting bounds which mitigate some of limitations in combinatorial approach.
New bounds are based on a specific cover of the original set of classifieres
such that each element of the cover have an exact overfitting bound.
    
Let $\XX=\bigl( x_1, \ldots, x_L)$ be a finite instance space
and $A$ be a set of classifiers.
By $I \colon A \times \XX \rightarrow \{0, 1\}$ denote a binary loss function
such that $I(a, x) = 1$ if classifier $a \in A$ produces an error on object $x \in \XX$.
A binary vector $(a_i) = (I(a, x_i))_{i=1}^L$ is called an \emph{error vector} of classifier $a \in A$.
For an arbitrary $U \subset \XX$ and $a \in A$ let $n(a, U) = \sum \limits_{x_i \in U}I(a, x_i)$ denote the \emph{number of errors},
and let $\nu(a, U) = n(a, U) / |U|$ denote the \emph{error rate} of classifier $a$.

By $\XXell$ denote the set of all
$\CC_L^\ell = \tfrac{L!}{\ell!(L-\ell)!}$
subsets $X\subset\XX$ of a fixed size~$\ell$.
Denote $k = L - \ell$ and $\bar X = \XX \backslash X$.
A set $X \in \XXell$ is said to be a \emph{train sample},
and the corresponding $\X = \XX \backslash X$ is said to be a \emph{test sample}.
A \emph{learning algorithm} is a mapping of the form $\mu \colon \XXell \rightarrow A$,
which takes each train sample $X \in \XXell$ to classifier $\mu X \in A$.
A classifier $\mu X$ is said to be \emph{overfitted} if
the deviation between the test and train error rates $\delta(a, X) = \nu(a, \X) - \nu(a, X)$
exceeds a given threshold $\eps > 0$.
Then the \emph{probability of overfitting} $\Q(\mu)$ is defined as the fraction of $X \in \XXell$
such that $\mu X$ is overfitted:
\begin{equation}
    \label{def:probOverfit}
    \Q(\mu) = \Prob[\delta(\mu X, X) \geq \eps],
\end{equation}
where $\Prob[\phi\,] \equiv \frac{1}{\CLl}\sum\limits_{X \in \XXell} \phi(X)$,
and $\phi$ is an arbitrary predicate on~$\XXell$.
Here and in the sequel $[\textit{true}]=1$, $[\textit{false}]=0$.

Consider a disjoint partitioning of a set of classifeirs $A$:
\begin{equation}
    \label{clustersDecomposition}
    A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t
\end{equation}
such that for every $A_i$ we have explicit conditions for $\mu X \in A_i$.

\begin{vkHyp}
\label{generalizedPZM}
Let $A$ be a set of classifiers, and
$A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$ be a disjoint partitioning of $A$.
Suppose for all $i = 1, \dots, t$ there exist two disjoint sets $X_i \subset \XX$ and $X'_i \subset \XX$ such that
for all $X \in \XXell$
\[
    \mu X \in A_i \Rightarrow (X_i \subset X) \text{ and } (X'_i \subset \X).
\]
In addition, suppose that all classifiers $a \in A_i$ have zero errors on $X_i$, and have an error on each object $x \in X'_i$.
\end{vkHyp}
The set $X_i$ is called \emph{protective} for $A_i$,
the set $X'_i$ is called \emph{prohibitive} for $A_i$.
Hypothesis~\ref{generalizedPZM} imply that $\mu X \in A_i$
require all objects from the protective set to belong to the train sample $X$,
and all objects from the prohibitive set to belong to the test sample $\bar X$.
The set of remaining objects $\YY_i \equiv \XX \backslash X_i \backslash X'_i$
is called \emph{neutral} for $A_i$.

For all $i = 1, \dots, t$ denote $L_i = L - |X_i| - |X'_i|$, $\ell_i = \ell - |X_i|$, $k_i = k - |X'_i|$.
By~$Q_\eps(A_i)$ denote the upper bound on probability of overfitting for an arbitrary method $\mu \colon [\YY_i]^{\ell_i} \rightarrow A_i$:
\begin{equation}
    \label{def:probOverfitReduction}
    Q_\eps(A_i) = \frac{1}{C_{L_i}^{\ell_i}} \sum_{Y \in [\YY_i]^{\ell_i}} 
    [\max_{a \in A_i}\delta(a, Y) \geq \eps],
\end{equation}
where $[\YY_i]^{\ell_i}$ denotes the set of all samples $Y \subset \YY_i$ of size $\ell_i$.

\begin{vkTheorem}[Splitting and similarity bound]
\label{th:generalizedPZM}
Consider a partitioning $A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$.
Suppose that for every $A_i$ there exist a number $m_i$
such that all classifiers $a \in A_i$ have an equal number of errors $n(a, \XX) = m_i$.
Then under the assumptions of hypothesis \ref{generalizedPZM} we have
\begin{equation}
    \label{generalizedPZMtheorem}
    Q_\eps(\mu) \leq \sum_{i = 1}^t P_i \, Q_{\eps_i}(A_i),
\end{equation}
where $P_i = \frac{C_{L_i}^{\ell_i}}{\CLl}$,
$\eps_i = \frac{L_i}{\ell_i k_i} \frac{\ell \, k}{L} \eps + \big(1 - \frac{\ell \, L_i}{L \ell_i}\big) \frac{m_i}{k_i} - \frac{|X'_i|}{k_i}$,
and $Q_\eps(A_i)$ is the probability of overfitting \eqref{def:probOverfitReduction} on the set of neutral objects $\YY_i$.
\end{vkTheorem}

Practical usage of bound \eqref{generalizedPZMtheorem} require
a~generic method for constructing protective and prohibitive subsets
for every $A_i$,\; $i=1,\dots,t$.
This in turn require a specific learning method.

\begin{vkDef}
A learning algorithm $\mu$ is said to be a \emph{pessimistic empirical risk minimisation}
(or, for short, \emph{pessimistic ERM}), if
$\forall X \in \XXell$ it satisfies $\mu X \in \Argmax_{a \in A(X)} n(a, \XX)$,
where $A(X) \equiv \Argmin_{a\in A} n(a,\Xl)$.
\end{vkDef}

A \emph{partial order ``$\leq$'' on classifiers }is defined as an element-wise comparison of their error vectors:
$a\leq b \Leftrightarrow I(a, x_i) \leq I(b, x_i), \forall x \in \XX$.
Let $\rho(a,b) = \sum_{i=1}^L |a_i-b_i|$ denote the Hamming distance between the error vectors of classifiers $a$ and $b$.
A pair of classifieres $(a, b)$ is said to be \emph{connected} if $\rho(a,b) = 1$.
A \emph{precedence} relation $a\prec b$ is defined as
$\bigl(a\leq b\bigr) \wedge \bigl( \rho(a,b)=1 \bigr)$.
Then the protective and prohibitive sets for a single classifier $a \in A$ are defined
in the \cite{voron10pria} as follows:
\begin{equation}
    \label{standardPZM}
    \begin{split}
        X_a  & = \{x \in X \colon \exists b \in A \colon a \prec b, I(a, x) < I(b, x)\}, \\
        X'_a & = \{x \in X \colon \exists b \in A \colon b < a,     I(b, x) < I(a, x)\}.
    \end{split}
\end{equation}

\begin{vkLemma}
    \label{lemmaKlasterPZM}
    Suppose $\mu$ is a pessimistic ERM. Consider the following sets:
    \begin{equation}
        \label{klasterPZM}
        X_i = \bigcap\limits_{a \in A_i} X_a,\;\; X'_i = \bigcap\limits_{a \in A_i} X'_a,
    \end{equation}
    where $X_a$ and $X'_a$ are defined by \eqref{standardPZM}.
    Then the set $X_i$ is a protective set for $A_i$, and
    the set $X_i'$ is a protective set for $A_i$ in terms of hypothesis \ref{generalizedPZM}.
\end{vkLemma}

Lemma \ref{lemmaKlasterPZM} enables us to calculate all parts of bound \eqref{generalizedPZMtheorem},
except for $Q_{\eps_i}(A_i)$.
To handle this we extend each subset~$A_i$ to $B_i \supseteq A_i$,
and replace $Q_{\eps_i}(A_i)$ with $Q_{\eps_i}(B_i)$.

\begin{vkTheorem}
\label{th3}
    Let $A_i$ and $B_i$ be two sets of classifiers such that $A_i \subset B_i$.
    Suppose all $a \in B_i$ have an equal number of errors on $\XX$.
    Then
    \begin{equation}
        \label{upperAccession}
        Q_\eps(A_i) \leq Q_\eps(B_i).
    \end{equation}
\end{vkTheorem}

Theorem \ref{th3} implies that the sets $B_i$ have a simple structure and support fast computation of $Q_\eps(B_i)$.
The following two definitions provide such examples of $B_i$.

\begin{vkDef}
\label{exCentralBallSlice}
Suppose classifier $a_0 \in A$ has $m$ errors on $\XX$,
$r$ be a natural number, $r < m$.
Then the set $B_r^m(a_0)$ is defined as the following subset of $\{0, 1\}^L$:
\[
    B_r^m(a_0) = \{a \in \{0, 1\}^L \colon \rho(a, a_0) \leq r \text { and } n(a, \XX) = m\}.
\]
\end{vkDef}

\begin{vkDef}
\label{def:localVicinity}
Let $\XX = X_0 \sqcup X_1 \sqcup X_r$ be a partitioning of the instance space $\XX$.
Let $|X_r| = r$, $|X_1| = m$, and $\rho$ is a number such that $\rho \leq r$.
Then a set $B_{r,\rho}^{m}$ is defined as a subset of $\{0, 1\}^L$ such that
\begin{itemize}
    \item $B_{r,\rho}^{m}$ consists of all classifiers with $\rho$ errors on $X_r$,
    \item classifiers from $B_{r,\rho}^{m}$ have no errors on $X_0$,
    \item classifiers from $B_{r,\rho}^{m}$ have an error on each $x \in X_1$.
\end{itemize}
\end{vkDef}

Inference of explicit formulas for $Q_\eps(B_r^m(a_0))$ and~$Q_\eps(B_{r,\rho}^{m})$
utilize intrinsic symmetries of the sets $B_r^m(a_0)$ and $B_{r,\rho}^{m}$,
and is based on randomized learning algorithm \cite{frey10pria, tolstihin10ioi}.
These formulas are applicable for deterministic ERM
because in both sets all classifiers share the same number of errors on $\XX$.

\emph{Randomized ERM} \cite{frey10pria} is a learning algorithm that for a given $X \in \XXell$
selects a random classifier from the set~$A(X) \equiv \Argmin_{a\in A} n(a,\Xl)$.
The definition of the probability of overfitting \eqref{def:probOverfit} is adjusted as follows:
\begin{equation}
\label{QepsRERM.sym}
    \Q(A)
    =
    \frac 1 \CLl\sum_{X \in \XXell}
    \frac1{|A(X)|} \sum_{a\in A(X)}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\end{equation}

Let $S_L = \{ \pi \colon \XX \rightarrow \XX \}$ be a symmetric group,
which acts on the set $\XX$ by permutation of objects.
Any $\pi \in S_L$ also acts on a classifier $a \in A$ by permutation of the coordinates
of the corresponding error vector:
$(\pi a)(x_i)= a(\pi^{-1}x_i)$.
For any $X \in \XXell$ and any set $A \subset \{0, 1\}^L$ the actions
$\pi X$ and $\pi A$ are defined as follows:
$\pi X = \{ \pi x \colon x \in X \}$, $\pi A = \{ \pi a \colon a \in A\}$.
\begin{vkDef}
    \label{def:symmetryGroup}
    The \emph{symmetry group} $\Sym(A)$ for a set of classifiers $A \subset \{0, 1\}^L$
    is defined as the stationary subgroup of $S_L$:
    \[
        \Sym(A) = \{\pi \in S_L \colon \pi A = A\}.
    \]
\end{vkDef}
Denote by $\Omega(\XXell)$ the set of all orbits of $A$ under the action of group $\Sym(A)$ on $\XXell$.
Let $\Xl_\tau$ be an arbitrary element from an orbit $\tau \in \Omega(\XXell)$.

\begin{vkTheorem}
Let $A$ be a set of classifiers, and $\Sym(A)$ be its symmetry group.
Then the probability of overfitting \eqref{QepsRERM.sym} can be written as follows:
\begin{equation}
\label{eq:factorization}
 \Q(A) =
    \sum_{\tau \in \Omega(\XXell)} \frac {|\tau|}{|A(X_\tau)|} \sum\limits_{a \in A(X_\tau)}[\delta(a, X_\tau) \geq \eps].
\end{equation}
\end{vkTheorem}
This theorem yields the following expressions for $Q_\eps(B_r^m(a_0))$ and $Q_\eps(B_{r,\rho}^{m})$.

\begin{vkTheorem}
\label{thBallSclice}
Suppose $B_r^m(a_0)$ is a central slice of classifieres (Definition \ref{exCentralBallSlice})
such that $r \leq 2 m$ and $n(a_0, \XX) = m$.
Then
\begin{equation}
    \label{eq:ballSclice}
    \Q(B_r^m(a_0))
    =
       \Hyper{L}{m}{\ell}{s(\epsilon) + \big\lfloor r/2 \big\rfloor} \cdot [m \geq \eps k],
\end{equation}
where $s(\epsilon) = \frac \ell L (m - \epsilon k)$,
$\Hyper{L}{m}{\ell}{s} = \sum\limits_{t = 0}^{\lfloor s \rfloor} C_m^t C_{L-m}^{\ell - t} / \CLl$ is the function of hypergeometric distribution \cite{voron09dan}.
\end{vkTheorem}

\begin{vkTheorem}
\label{th:crazyFormula}
Suppose $ B_{r,\rho}^{m}$ is a heap of classifiers (Definition \ref{def:localVicinity}).
Then
\begin{equation}
\label{crazyFormula}
    Q_{\eps}(B_{r,\rho}^{m}) =
        \frac 1{\CLl}
            \sum_{i = 0}^{\min(m, \ell)} \sum_{j = 0}^{\min(r, \ell - i)}
                C_m^i C_r^j C_{L - m - r}^{\ell - i - j} \Big[\frac{m + \rho - t}{k} - \frac{t}{\ell} \geq \eps\Big],
\end{equation}
where $t = i + \max(0, \rho - r - j)$.
\end{vkTheorem}

Let us describe the final computation scheme for our new bound on the probability of overfitting for an arbitrary set $A$.
We start with an arbitrary partitioning of the set of classifier $A$ into subsets \eqref{clustersDecomposition}.
For each subset we build the protective and prohibitive subsets according to \eqref{klasterPZM}.
Then we embed each subset~$A_i$ into a larger superset~$B_i$ with know formula for the probability of overfitting
(for example, \eqref{eq:ballSclice} or \eqref{crazyFormula}).
The final bound is given by \eqref{generalizedPZMtheorem}.

We evaluated the new bound on 11 datasets from UCI repository,
and compare the actual test error rates with our prediction.
The prediction of our new bound exceed the actual test error rate by 5 to 50\,\% (depending on a dataset).
This is sharper than the prediction of the splitting-connectivity bound \cite{voron10pria},
which in our experiments exceeded the actual test error rate by 17 to 63\,\%.
Both combinatorial bounds are much sharper than the latest PAC-Bayesian bounds \cite{jin2012pacbayes},
which exceeded the actual test error rate from 2.5 to 10 times.

This work was supported by the Russian Foundation for Basic Research
(project no.\,11-07-00480, no.\,12-07-33099\,-mol-a-ved)
and by the program ``Algebraic and Combinatorial Methods in
Mathematical Cybernetics and New Generation Information Systems''
of the Department of Mathematical Sciences of the Russian Academy of Sciences.

\clearpage\newpage
%\def\BibUrl#1{\\{\def~{\char126}\small\tt\url{http://#1}}}
\def\BibUrl#1.{}
\bibliographystyle{gost71u}
%\bibliography{MachLearn}

\def\BibAuthor#1{\emph{#1}}
\def\BibTitle#1{#1}
\def\BibUrl#1{{\small\url{#1}}}
\def\BibHttp#1{{\small\url{http://#1}}}
\def\BibFtp#1{{\small\url{ftp://#1}}}
\def\typeBibItem{\small\sloppy}

\begin{thebibliography}{1}

\bibitem{vapnik71convergence}
     Vapnik\;V.\,N., Chervonenkis\;A.\,Y.~(1971)
     On the uniform convergence of relative frequencies of events to their probabilities.
     \emph{Theory of Probability and Its Applications}, 16(2), 264--280.

\bibitem{boucheron05survey}
     Boucheron\;S., Bousquet\;O., Lugosi\;G.~(2005)
     Theory of classification: A survey of some recent advances.
     \emph{ESAIM: probability and statistics}, 9(1), 323--375.

%\bibitem{langford02pacbayes}
%    Shawe-Taylor\;J., Langford\;J.,~(2003)
%    PAC-Bayes \& margins.
%    \emph{In Advances in Neural Information Processing Systems: Proceedings from the 2002 Conference} (Vol. 15, p. 439). The MIT Press.

\bibitem{voron09dan}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Точные оценки вероятности переобучения}~//
    Доклады РАН, 2009. "--- Т.\,429, \No\,1.  "--- С.\,15--18.

\bibitem{voron11premi}
    Vorontsov\;K.\,V., Ivahnenko\;A.\,A.~(2011)
    Tight combinatorial generalization bounds for threshold conjunction rules.
    \emph{4-th Int'l Conf. on Pattern Recognition and Machine Intelligence (PReMI'11)}.
    Lecture Notes in Computer Science, Springer-Verlag, 66--73.

\bibitem{voron10pria}
    \BibAuthor{Vorontsov\;K.\,V.}
    Exact combinatorial bounds on the probability of overfitting for empirical risk minimization~//
    {Pattern Recognition and Image Analysis.} ~---
    2010.~---
    Vol.\,20, No.\,3.~---
    Pp.\,269--285.

\bibitem{frey10pria}
    \BibAuthor{Фрей\;А.\,И.}
    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов и рандомизированных методов обучения}~//
    Pattern Recognition and Image Analysis. "--- 2010.

\bibitem{tolstihin10ioi}
    \BibAuthor{Толстихин\;И.\,О.}
    \BibTitle{Вероятность переобучения плотных и разреженных семейств алгоритмов}~//
    Междунар. конф. ИОИ-8 "--- М.:~МАКС Пресс, 2010.  "---  \mbox{С.\,83--86}.

\bibitem{jin2012pacbayes}
     Jin\;C., Wang\;L.~(2012)
     Dimensionality Dependent PAC-Bayes Margin Bound.
     \emph{In Advances in Neural Information Processing Systems}, 25, 1043--1051.

\end{thebibliography}

\newpage
УДК 519.7:004.855.5

\section*{Перевод названия, имени и~фамилии авторов}

Cover-based combinatorial bounds on probability of overfitting

Alexander Frey, Ilya Tolstikhin

\section*{Данные об авторах}

1. Фрей Александр Ильич

Тел.:\,\verb|+7(903)175-80-77|;

Email: \verb|sashafrey@gmail.com|;

Московский Физико-Технический Институт (Государственный Университет)

2. Толстихин Илья Олегович

Тел.:\,\verb|+7(916)136-49-78|;

Email: \verb|iliya.tolstikhin@gmail.com|;

Учреждение Российской академии наук
Вычислительный центр
им.~А.~А.~Дородницына РАН.

\newpage

\section*{Abstract}

\textbf{Cover-based combinatorial bounds on probability of overfitting}

\textit{A.\,Frey, I.\,Tolstikhin}

The paper improves existing combinatorial bounds on probability of overfitting.
A new bound is based on partitioning of a set of classifiers into non-overlapping clusters,
and then embedding each cluster into a superset with known exact formula for the probability of overfitting.
Such approach makes the bound sharper because it accounts for similarities between classifiers within each cluster.

\end{document}
