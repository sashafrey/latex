\documentclass[twoside]{article}
\usepackage{iip9}
\NOREVIEWERNOTES

\def\eps{\varepsilon}
\def\XX{\mathbb{X}}
\def\X{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\newcommand{\Arg}{\mathop{\rm Arg}\limits}

\def\CCfont#1{\ifmmode{\mathsf{#1}}\else\mbox{\rm\textsf{#1}}\fi}
\def\P{\CCfont{P}}

\newcommand{\hypergeom}[5]{{#1}_{#2}^{#3, #4}\left(#5\right)}
\newcommand{\hyper}[4]{\hypergeom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\hypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\hypergeom{\bar{H}}{#1}{#2}{#3}{#4}}
\newcommand{\Binom}[2]{C_{#1}^{#2}}
%\newcommand{\Binom}[2]{\binom{#1}{#2}}
\newcommand{\CLl}{\Binom{L}{\ell}}


\begin{document}
\title
    [Применение комбинаторных оценок вероятности переобучения в~простом голосовании конъюнкций]
    {Применение комбинаторных оценок вероятности переобучения в~простом голосовании пороговых конъюнкций}
\author
    {Фрей~А.\,И., Ивахненко~А.\,А, Решетняк~И.\,М.} % основной список авторов, выводимый в оглавление
\thanks
    {Работа выполнена при финансовой поддержке гранта молодым кандидатам наук президента РФ, МК-5422.2012.9.}
\email
    {sashafrey@gmail.com, andrej\_iv@mail.ru}
\organization
    {Москва, Вычислительный центр им.~А.\,А.\,Дородницына РАН}
\abstract
    {Данная работа направлена на~практическое применение
    комбинаторных оценок вероятности переобучения~%
    \cite{voron10pria,voron10roai,voron11mmro,ivahnenko11mmro}
    для повышения качества логических алгоритмов классификации.
    Предлагается эффективный метод вычисления предсказанной информативности,
    максимизация которой улучшает обобщающую способность
    отдельных логических закономерностей и их линейной композиции.
}
\titleEng
    {Combinatorial generalization bounds with application to simple voting of~thresholded conjunction rules}
\authorEng
    {Frey~A.\,I., Ivakhnenko~A.\,A., Reshetnyak~I.\,M.}
\organizationEng
    {Dorodnicyn Computing Centre of RAS, Moscow, Russia}
\abstractEng
    {We~apply tight combinatorial generalization bounds
    recently obtained in\cite{voron10pria,voron10roai,voron11mmro,ivahnenko11mmro}
    to~enhance rule evaluation heuristic in rule-based classifiers.
    Experiments on~7 data sets from the UCI ML Repository show that
    combinatorial bound helps to learn more reliable compositions consisting of less overfitted rules.
}
\maketitle

\section{Логические закономерности}
Рассматривается стандартная постановка задачи классификации.
Задано множество объектов ${\XX=(x_i)_{i=1}^L}$,
описанных $n$~действительными признаками, ${x_i = (x_i^1,\ldots,x_i^n)}$;
каждому объекту~$x_i$ соответствует ответ~$y_i$ из множества~$Y = \{-1, 1\}$.

\emph{Логическим правилом} называется конъюнкция пороговых предикатов~(термов) вида
\begin{equation}
\label{eq:Rule}
    r(x_i) \equiv r(x_i;c^1,\ldots,c^n)  = \prod_{j\in \omega} \bigl[ x_i^j \lessgtr_j c^j \bigr],
\end{equation}
\vskip-2ex\noindent
где
%$x = (x^1,\ldots,x^n) \in \RR^n$,\;
$\omega\subseteq\{1,\ldots,n\}$ --- подмножество признаков,\;
$\lessgtr_j$ "--- одна из операций сравнения $\{\leq,\geq\}$,\;
$c^j$ "--- порог по $j$"=му признаку.
Говорят, что правило~$r$ \emph{выделяет} объект~$x$, если~$r(x)=1$.

\emph{Логическая закономерность}~--- это правило, выделяющее
достаточно много ($p$) объектов выбранного класса~$y$ (положительных примеров)
и~приемлемо мало ($n$) объектов всех остальных классов (отрицательных примеров).
Для поиска закономерностей класса~$y$ по обучающей выборке~${X \subset \XX}$
решается задача двухкритериальной оптимизации:
\begin{align*}
    p(r,X) &= \sum_{x_i\in X} \! r(x_i) \, [y_i = y ] \to \max_r;\\
    n(r,X) &= \sum_{x_i\in X} \! r(x_i) \, [y_i \neq y ] \to \min_r.
\end{align*}

Обычно эта задача сводится к~максимизации выбранного скалярного критерия информативности $H(p,n)$.
В~частности, это может быть точный тест Фишера~\cite{martin97exact},
энтропийный критерий, индекс Джини, тест~$\chi^2$, тест~$\omega^2$ и~другие.
В~обзоре~\cite{furnkranz05rocnrule} приведено более 20~критериев,
но~ни один из них не~является безусловно лучшим.

Для поиска закономерностей применяются методы дискретной оптимизации:
жадные алгоритмы с~последующий редукцией правил~\cite{cohen99simple},
поиск в~ширину~\cite{lbov81methods},
генетические алгоритмы~\cite{Yankovskaya2007b},
асимптотически оптимальные алгоритмы~\cite{Inyakin2008}
и~другие.

Выбор функционала информативности и~метода его оптимизации является эвристикой.

\section{Переобучение закономерностей}
На~практике часто приходится наблюдать эффект переобучения закономерностей~---
на~независимой контрольной выборке
пропорция числа положительных~$p'$ и~отрицательных~$n'$ примеров,
как правило, смещается в~нежелательную сторону:
${n'/p' > n/p}$.
Для сокращения переобучения в~\cite{ivahnenko11mmro,voron11premi}
предлагается использовать функционал \emph{предсказанной информативности}.
Это обычный функционал информативности~$H$,
в~который вместо величин~$p$,~$n$ на~известной обучающей выборке
подставляются оценки соответствующих величин~$p'$,~$n'$ на~неизвестной контрольной выборке,
\[
    \tilde H(p,n)
    %= H(\hat p', \hat n')
    = H(p-\delta',n+\delta''),
\]
где $\delta'$ и~$\delta''$~--- поправки на переобучение,
получаемые из комбинаторных оценок вероятности переобучения.
Преимущество данного подхода в~том, что
он~совместим с~любыми функционалами информативности и~любыми алгоритмами поиска закономерностей,
поэтому его можно встраивать в~стандартные библиотеки.
Эксперименты на~6~реальных задачах классификации из~репозитория~UCI показывают, что
максимизация предсказанной информативности улучшает обобщающую способность
двух типов композиций закономерностей~---
взвешенного голосования и~решающего списка (голосования по~старшинству)~\cite{voron11premi}.

В~то~же время, недостатком предложенного в~\cite{voron11premi} алгоритма
является относительно низкая численная эффективность.
Чтобы вычислить вероятность переобучения для заданного набора признаков,
приходится перебирать все конъюнкции,
находящиеся в~некоторой окрестности выбранной оптимальной конъюнкции.
При этом размер окрестности увеличивается экспоненциально
с~ростом числа признаков (ранга конъюнкции).
Кроме того, в~\cite{voron11premi} предполагается, что
значения каждого признака попарно различны на объектах выборки.

В~данной работе предлагается ряд упрощений,
повышающих численную эффективность и~расширяющих границы применимости
метода максимизации предсказанной информативности.
Во"~первых, в~оценках вероятности переобучения не учитывается связность,
что позволяет применять их для признаков любых типов.
Во"~вторых, вместо полного перебора конъюнкций по~специально построенной окрестности
применяется сокращённый перебор только по~тем конъюнкциям,
для которых в~процессе поиска закономерностей было вычислено значение информативности.
%В~результате
%оценки вероятности переобучения становятся менее точными, причём
%пренебрежение связностью завышает их,
%а~сокращение числа конъюнкций~--- занижает.
%Как будет показано ниже,
%наложение этих двух противоположных тенденций
%в~итоге 
Данный подход
проводит к~улучшению обобщающей способности логических закономерностей
по сравнению с~\cite{voron11premi}.


\section{Оценки вероятности переобучения}

Правило~$r$ класса~${y\in Y}$ индуцирует на $\XX$ бинарный вектор ошибок
$\bigl( I(r, x_i) \bigr){}_{i=1}^L$,
где
$I(r, x_i) = \bigl[ r(x_i) \neq [y_i{=}y] \bigr]$~---
индикатор ошибки правила $r$ на объекте $x_i$.
\REVIEWERNOTE{Был конфликт обозначений: $R$~--- множество всех правил (убрал его совсем) и~выход Алгоритма~2.}
Определим \emph{число} и~\emph{частоту} ошибок правила~$r$ на~выборке~$X \subseteq \XX$:
\[
    m(r,X) = \sum_{x_i\in X} I(r, x_i);
    \quad
    \nu(r,X) = \frac{m(r,X)}{|X|}.
\]

Пусть множество объектов~$\XX$ разбито две непересекающиеся подвыборки:
обучающую~$X$ длины~$\ell$ и~контрольную~$\X$ длины ${k=L-\ell}$.

\emph{Методом обучения} называется отображение,
которое произвольной обучающей выборке ${X\subseteq \XX}$
ставит в~соответствие некоторое правило ${r = \mu X}$.

Метод обучения~$\mu$ называется \emph{монотонным}, \mbox{если}
${\mu X = \mathrm{arg}\min_{r} K(r,X)}$,
где критерий $K(r,X)$~--- строго монотонная функция вектора ошибок:
для любой пары правил $r,v$ и любой выборки ${X\subset\XX}$
\mbox{если} ${I(r,x_i) \leq I(v,x_i)}$ для всех ${x_i\in X}$
и~хотя бы одно из~неравенств строгое,
то ${K(r,X) < K(v,X)}$.

Если функция $H(p,n)$
строго монотонно возрастает по~$p$
и~строго монотонно убывает по~$n$, то критерий
$K(r,X) = -H\bigl( p(r,X), n(r,X) \bigr)$
является монотонным,
а~максимизация информативности~--- монотонным методом обучения~\cite{ivahnenko11mmro}.
Все используемые на~практике критерии
обладают свойством монотонности в~указанном смысле.

Для произвольного разбиения $X \sqcup \X = \XX$
\emph{переобученностью} правила~$r$ называется уклонение частот его ошибок на~контроле и~на обучении:
$\delta(r, X) = \nu(r, \X) - \nu(r, X)$.

Следуя слабой вероятностной аксиоматике~\cite{voron10pria},
будем полагать, что на~множестве~$\XXell$
всех~$C_L^\ell$ разбиений $(X, \X)$
задано равномерное распределение вероятностей.
Тогда вероятность переобучения есть доля разбиений,
при которых переобученность превышает заданный порог~$\eps\in[0,1]$:
\[
    Q(\eps) = \Prob \bigl[ \delta(\mu X, X) \geq \eps \bigr].
\]
Заметим, что $1-Q(\eps)$ есть
функция распределения случайной величины $\delta(\mu X, X)$,
определённой на конечном вероятностном пространстве
$\{\XXell, 2^{\XXell}, \Prob\}$,
где $\Prob$ "--- равномерное распределение.

Пусть $R$~--- некоторое множество правил.
Обозначим через
${X_r = \{x \in \XX \colon I(r, x) = 1\}}$ множество ошибок правила ${r \in R}$ на выборке ${X \subset \XX}$.
Рассмотрим пару правил ${r, v \in R}$, такую, что $X_v \subset X_r$.
Заметим, что правило~$r$ может быть выбрано монотонным методом обучения
только для тех разбиений $(X, \X)$,
где все объекты $\{x \colon x \in \XX_r, x \not\in \XX_v\}$ лежат в контрольной выборке:
\[
    [\mu X = r] \leq [\XX_r \backslash \XX_v \subset \X].
\]

В~терминах метода порождающих и запрещающих множеств~\cite{voron10pria}
множество
%Перебирая все $v \in R$ удовлетворяющие $\XX_v \subset \XX_r$, построим множество
\[
    \XX(r) = \bigcup\limits_{v \colon \XX_v \subset \XX_r} \XX_r \backslash \XX_v.
\]
является запрещающим для правила~$r$.
Следовательно, для любого монотонного метода обучения и~любой выборки~$\XX$
справедлива оценка
\begin{equation}
    \label{eqConSplit}
    Q(\eps) \leq \sum_{r \in R} \frac{\Binom{L-q}{\ell}}{\CLl}\Hyper{L-q}{\ell}{m - q}{s(\eps)}
    \equiv \eta(\eps),
\end{equation}
где
${q = |\XX(r)|}$~--- \emph{неполноценность} правила~$r$,\;
$m = m(r, \XX)$.
%Вклады правил в~сумму~\eqref{eqConSplit} убывают экспоненциально по~$q$,
%поэтому для вычисления $Q(\eps)$ достаточно взять только правила с~наименьшими~$q$.

Пусть $\eps(\eta)$~--- функция, обратная к~$\eta(\eps)$.
Тогда справедливо утверждение, эквивалентное неравенству~\eqref{eqConSplit}:
с~вероятностью не~менее $1-\eta$
\[
    \nu(r, \X) \leq \nu(r, X) + \eps(\eta).
\]

Для построения функционала предсказанной информативности
нужны аналогичные оценки частоты ошибок первого и второго рода.
Введём множества положительных и~отрицательных примеров
%\begin{align*}
\[
    \XX'  = \{x_i \in \XX \colon y_i = y\};
    \quad
    \XX'' = \{x_i \in \XX \colon y_i \neq y\};
\]
%\end{align*}
и~индикаторы ошибки I и II рода, соответственно:
\begin{align*}
    I' (r, x) &= [r(x_i) = 0][y_i = y];\\
    I''(r, x) &= [r(x_i) = 1][y_i \neq y].
\end{align*}

Число и частоту ошибок относительно этих индикаторов обозначим через
$m'(r, X)$, $m''(r, X)$, $\nu'(r, X)$ и $\nu''(r, X)$ соответственно.

Следующие формулы являются обобщением оценки \eqref{eqConSplit}.
Для любого монотонного метода обучения справедливы оценки вероятности переобучения
по~ошибкам первого и второго рода:
\begin{align*}
    \label{eqConSplitSeparate}
    Q'(\eps) &\leq
        %\Prob \{\delta'(\mu X, X) \geq \eps\} =
        \sum_{r \in R} \frac{\Binom{L-q}{\ell}}{\CLl}\Hyper{L-q}{\ell}{m' - q'}{\tfrac{\ell}{L} \left(m' - \eps k\right)} \equiv \eta'(\eps); \\
    Q''(\eps) &\leq
        %\Prob \{\delta''(\mu X, X) \geq \eps\} =
        \sum_{r \in R} \frac{\Binom{L-q}{\ell}}{\CLl}\Hyper{L-q}{\ell}{m'' - q''}{\tfrac{\ell}{L} \left(m'' - \eps k\right)} \equiv \eta''(\eps);
\end{align*}
где ${q = |\XX(r)|}$, ${q' = |\XX(r) \cap \XX'|}$, ${q'' = |\XX(r) \cap \XX''|}$~---
неполноценность правила $r$ относительно индикаторов ошибки $I$, $I'$, $I''$ соответственно;
${m' = m'(r, \XX')}$ и ${m'' = m''(r, \XX'')}$ "--- число ошибок $r$ на~$\XX$ относительно индикаторов ошибки $I'$, $I''$.

Теперь построим критерий предсказанной информативности
для произвольного $H(p, n)$.
Обозначим через $\eps'(\eta)$ и $\eps''(\eta)$
функции, обратные к~$\eta'(\eps)$ и $\eta''(\eps)$.
В~новых обозначениях число положительных и~отрицательных примеров
во~всей выборке~$\XX$ равны, соответственно,
${p = |\XX'| - m'(r,\XX)}$ и
${n = m''(r,\XX)}$
Возьмём в~качестве поправок на переобучение медианные оценки частоты ошибок на~контроле,
получаемые при $\eta=0.5$:
\begin{equation}
\label{eq:Hpn}
    \tilde H(p, n)
    = H\bigl(
        p - L \eps'(0.5),\:
        n + L \eps''(0.5)
    \bigr).
\end{equation}

Полученная оценка не~накладывает никаких ограничений на~то,
как именно выбирается множество правил~$R$.
Оценки расслоения--связности, использованные в~\cite{voron11premi},
довольно жёстко предполагали, что $R$~--- это множество всех правил,
получаемых при фиксации набора признаков~$\omega$ и~знаков неравенств~$\lessgtr_j$
и~варьировании порогов~$c^j$.
В~этом случае максимизация предсказанной информативности $\tilde H(p, n)$
может использоваться только в~качестве критерия отбора признаков~$\omega$.

Теперь же можно ввести более общее представление процесса поиска закономерностей,
считая, что он разбит на~\emph{стадии}.
На~каждой стадии просматривается некоторое множество правил~$R$ и~из них выбирается лучшее.
Критерий~$\tilde H$ предсказывает,
какую информативность выбранное правило будет иметь на~новых данных.
Для этого используется всё множество правил~$R$, учитывается его сложность и~расслоение.
Таким образом,
критерий~$\tilde H$ позволяет правильно отранжировать правила, полученные на разных стадиях,
но~не~позволяет сделать правильный выбор внутри каждой стадии.

В~данной работе для~вычисления поправок на~переобучение правила~$r$ 
в~качестве множества~$R$ использовались все правила того~же целевого класса, что~и~$r$, 
построенные алгоритмом поиска закономерностей для~признаков, входящих в~состав~$r$.

\REVIEWERNOTE{Я так и не понял из исходного изложения, осталась ли старая схема
    (стадия~--- это фиксация набора признаков и~знаков неравенств)
    или множество $R$ формируется как-то по-новому}

\REVIEWERNOTE{А вы вообще имели в~виду такое обобщение со стадиями?}

%В листинге $(3)$ поправок $\bar \delta_h$ настраиваются непосредственно перед вычислением информативности.
%Технически удобнее вычислить все поправки заранее, перебрав все возможные наборы признаков и целевых классов.
%Это накладывает существенные ограничения на максимальный ранг правил.
%По этой причине в данной работе ранг правил был ограничен тремя признаками.

%%\begin{Algorithm}[t]
%%\label{alg:modinfo}
%%\caption{Критерий информативности}
%%\REQUIRE~\\
%%    $X^\ell$~--- обучающая выборка;\\
%%    $\Theta$~--- набор термов, $F(t)$ "--- признак терма $t \in \Theta$;\\
%%    $M, S_1$~--- параметры генератора правила;\\
%%    $H$~--- исходный критерий информативности;\\
%%    $r$~--- правило класса $y$;\\
%%\ENSURE $H(r)$ "--- информативность правила $r$.
%%\STATE Найти термы, зависящие от признаков $r$\\
%%       $\Theta_r := \{t \in \Theta \colon F(t) \in F(r)\}$;
%%\STATE $R$ := Построить правила $(X^\ell, \Theta_r, M, S_1, H)$;
%%\STATE Оставить в $R$ правила класса $y = y(r)$;
%%\STATE Для множества правил $R$ построить распределения $Q'_h(\eps)$, $Q''_h(\eps)$;
%%\STATE Вычислить поправки $\bar \delta'_h$ и $\bar \delta'_h$ как медианы распределений $Q'_h(\eps)$, $Q''_h(\eps)$;
%%\STATE Для правила $r$ вычислить $p$ и $n$ как количество правильно и неправильно покрытых объектов;
%%\RETURN $H(p - \ell \bar \delta'_h, n + \ell \bar \delta''_h)$.
%%\end{Algorithm}

%ToDo: рассказать, что в этом месте у нас есть гипотеза. Уклонение частот сохраняется при масштабировании выборки. Это кажется самоочевидным, но только до тех пор %пока мы не задумались про то, что мы рассматриваем лучший алгоритм в семействе. Есть предположение, что среднее уклонение частот (т.е. среднее переобучение) %падает при масштабировании выборки. Но в первом приближении - можно пользоваться.

%ToDo: подвести итог - как вычислялись поправки с помощью кросс-валидации, и как с помощью комбинаторных формул.

\begin{Algorithm}[t]
\caption{ComBoost (Committee Boosting).}
\label{alg:comboost}
\REQUIRE
    $X$~--- обучающая выборка; \\
    $T, l_0, l_1$~--- параметры;
\ENSURE композиция правил $a_T = (r_1, \dots, r_T)$.
\BEGIN
\STATE
    инициализировать выборку $X'$ и отступы:\\
    $X' := X$;\; $M_i := 0$ для всех $i = 1, \dots, \ell$;
\FORALL{$t = 1, \dots, T$}
    \STATE
        обучить правила $r^y_t$,\; ${y \in Y}$ по выборке $X'$;
    \STATE
        $(r_t,y_t) := \argmin_{(r^y_t,y)\colon y\in Y} \; \sum \limits_{x_i \in X'} [a_t(x_i) \neq y_i]$;
    \STATE
        обновить значения отступов:\\
        $M_i := M_i + y_t y_i r_t(x_i)$,\; $i = 1, \dots, \ell$;
    \STATE
        упорядочить выборку $X$ по возрастанию $M_i$;\\
    \STATE
        $X' := \{x_i \in X \colon \ell_0 < i \leq \ell_1\}$.
\ENDFOR
\end{Algorithm}


%\section{Алгоритм выбора правил}

%На шаге~4 алгоритма~\eqref{alg:comboost} для каждого класса задачи
%применяется алгоритм поиска информативных правил~\eqref{alg:QuasiTemp}.
%В данной работе критерий информативности предполагается из семейства функций
%$f:(P, N)\rightarrow \RR$, то есть функций, ставящих в соответствие правилу
%с~характеристиками $(P, N)$ число из $\RR$. В алгоритме~\eqref{alg:QuasiTemp}
%критерий информативности используется для ранжирования правил и его абсолютное
%значение не важно.

%\begin{algorithm}
%\caption{Усеченный поиск правил в ширину}
%\label{alg:QuasiTemp}
%\begin{algorithmic}[1]
%\REQUIRE~\\
%    $X^\ell$~--- обучающая выборка;\\
%    $\Theta$~--- набор термов;\\
%    $M$~--- максимальная длина правила;\\
%    $S_1, S_2$ "--- параметры поиска правила;\\
%%    $S_1$~--- количество правил оставляемых на каждом этапе для улучшения;\\
%%    $S_2$~--- количество улучшенных от каждого из $S_1$ правил;\\
%    $H$ "--- критерий информативности правил;\\
%\ENSURE $R$~--- набор правил.
%\BEGIN
%    \STATE Инициализировать набор правил, $R=\emptyset$;
%    \STATE Инициализировать набор правил нулевого шага правилом не содержащим термов, $R_0 = \{\emptyset\}$;
%    \FOR{$m=1,\dots,M$}
%        \STATE Инициализировать набор правил $m$-ого шага, $R_m=\emptyset$;
%        \FORALL{$r \in R_{m-1}$}
%            \STATE Инициализировать набор правил получающихся добавлением одного терма к правилу $r$, $Q = \emptyset$;
%            \FORALL{термов $t\in \Theta$, c признаком не входящим в термы $r$}
%                \STATE Добавить в $Q$ правило $q=r \cup t$;
%            \ENDFOR
%            \STATE Оставить в $Q$ правила с информативностью большей чем у $r$,\\
%                $Q=\{q|q\in Q, H(q)>H(r)\}$;
%            \STATE Оставить в $Q$ $S_2$ лучших по критерию $H$ правил;
%            \IF{$Q = \emptyset$}
%                \STATE Сохранить $r$ как неулучшаемое в конечном списке правил, $R = R\cup r$;
%            \ELSE
%                \STATE Сохранить улучшенные правила в наборе правил этого шага, $R_m = R_m \cup Q$;
%            \ENDIF
%        \ENDFOR
%        \IF{$m\neq M$}
%            \STATE Оставить в $R_m$ $S_1$ лучших по критерию $H$ правил;
%        \ELSE
%            \STATE Добавить все найденные правила этого шага в конечный список правил, $R = R\cup R_m$;
%        \ENDIF
%    \ENDFOR
%    \STATE Оставить в конечном списке правил одно самое лучшее по критерию $H$;\\
%    \RET $R$;
%\end{algorithmic}
%\end{algorithm}

\begin{Algorithm}[t]
\caption{Усеченный поиск в ширину.}
\label{alg:QuasiTemp}
\REQUIRE
    $X$~--- обучающая выборка;\\
    $\Theta$~--- семейство термов;\\
    $M$~--- максимальный ранг конъюнкции;\\
    $S_1$ "--- параметр ширины поиска;
\ENSURE $R$~--- набор правил.
\BEGIN
    \STATE инициализация: $R := \emptyset$, $R_0 := \{\emptyset\}$;
    \FOR{$m=1,\dots,M$}
        \STATE $R_m := \emptyset$;
        \FORALL{$r \in R_{m-1}$}
            \STATE нарастить правило $r$ термами $t$:\\
                $R_m := R_m \, \bigcup \, \{r \wedge t \colon t\in\Theta \text{ допустим для } r \}$;
        \ENDFOR
        \STATE выбрать в $R_m$ целевые классы;
        \STATE согласно критерию $\tilde H$ оставить в $R_m$ не более $S_1$ лучших правил за каждый класс;
        \STATE сохранить правила: $R := R \cup R_m$;
    \ENDFOR
    \RET $R$;
\end{Algorithm}

\section{Композиция закономерностей}

\emph{Простое голосование}~---
это один из стандартных способов построения композиции вида
\begin{equation}
\label{eq:SimpleVoting}
    a_t(x) = \sign \biggl(\sum_{r \in R_{+1}} \!\! r(x) - \sum_{r \in R_{-1}} \!\! r(x) \biggr),
\end{equation}
состоящей из $t= |R_{-1}|+|R_{+1}|$ логических закономерностей,
где $R_y$ "--- множество закономерностей класса~$y$.
Для обучения композиции \eqref{eq:SimpleVoting}
используется комитетный бустинг
ComBoost~\cite{matsenov07mmro}.
В~отличие от других разновидностей бустинга,
он не взвешивает объекты выборки, а~только отбирает подвыборки.
Поэтому к~методу обучения базовых закономерностей
применимы комбинаторные оценки переобучения,
существующие только для бинарных функций потерь.
Другое важное преимущество ComBoost в~том, что,
благодаря явной оптимизации распределения отступов,
он стремится набрать минимальное достаточное число базовых закономерностей.

На~шаге~3 Алгоритма~\ref{alg:comboost} для каждого класса~$y$
применяется Алгоритм~\ref{alg:QuasiTemp} поиска информативных правил,
аналогичный алгоритму ТЭМП~\cite{lbov81methods}.

На~шаге~5 Алгоритма~\ref{alg:QuasiTemp} допустимыми для добавления считаются термы,
не~содержащие признаков, которые уже вошли в~правило~$r$.

\REVIEWERNOTE{На шаге~5 ComBoost'а исправил необъяснённые обозначения $y(r_t)$, $r_y$}

\REVIEWERNOTE{Важно сказать, что обучение производится по всей $\XX$, а не по какой-то $X$... если я правильно понимаю.}

\section{Результаты экспериментов}

В эксперименте на~семи реальных задачах классификации из репозитория UCI
сравнивались три варианта ComBoost
с~точным тестом Фишера в~качестве критерия информативности:

A: без поправок на переобучение;

B: с~поправками по~предложенному методу~\eqref{eq:Hpn};

C: с~поправками по эмпирической оценке $Q(\eps)$,
вычисляемой методом Монте-Карло по~случайному подмножеству разбиений.

Во всех задачах кроме australian
варианты B~и~C дают лучшее качество классификации тестовых данных.
Хотя комбинаторные оценки вычисляются неточно,
в~некоторых случаях вариант~B лучше варианта~C.
На~5 из~7 задач вариант~B даёт лучшие результаты,
чем предложенный ранее~\cite{voron11premi},
несмотря на то, что он не~учитывает эффект связности.
Во~всех задачах вариант~B имеет существенно меньшую переобученность~---
разность частоты ошибок между тестовой и~обучающей выборками.

\section{Выводы}

Предложен эффективный метод вычисления предсказанной информативности
для поиска логических закономерностей в~задачах классификации.
Замена обычного критерия информативности на~предсказанную информативность
может быть выполнена для любого стандартного метода поиска закономерностей,
независимо от~вида критерия и~механизма перебора правил.

Улучшение обобщающей способности достигается благодаря
комбинаторным оценкам вероятности переобучения,
учитывающим эффект расслоения семейства правил.

Вычислительная эффективность достигается благодаря тому, что, в~отличие от~предыдущих работ,
не~производится никакого дополнительного перебора и~оценивания правил~---
оценки вычисляются только по тем правилам, которые уже были построены в~процессе перебора.

%\clearpage

%\begin{enumerate}
%  \item Раньше использовался обычный граф связей. Считалось что два правила
%      связаны и имеют общее ребро в графе, если они по разному ошибаются ровно на одном объекте.
%      Теперь используется граф Хассе и для связи правил не обязательно различие в ошибках ровно
%      на одном объекте. Это допущение позволяет использовать для оценки правила, которые
%      рассматриваются во время поиска лучшего для включения в алгоритм (см. следущий пункт).
%  \item Раньше строился граф семейства в который входили все различные правила этого семейства
%      (правила с различными векторами ошибок, представители классов эквивалентности по векторам ошибок),
%      которые можно построить на данной выборке. Теперь только правила рассматриваемые в процессе
%      поиска лучшего правила.
%  \item Раньше поправки первого/второго рода строились независимо - для минимизации эмпирического риска
%      по матрице ошибок первого/второго рода. Теперь используется теория о монотонных методах обучения.
%  \item Раньше, номинвльные признаки приходилось учитывать особенно при построении графа связей,
%      граф разделялся на несколько несвязных частей и далее оценки вычислялись по каждой из них.
%      Теперь, расчет оценки по графу производится одинаково для всех типов признаков. Более того,
%      теперь тип признаков, как и форма правил не имеет значения --- для построения графа связей
%      необходимы только вектора ошибок.
%\end{enumerate}
%
%\paragraph{Мысли вслух.} Новый подход имеет много преимуществ, однако не учитывает (?) форму
%семейства правил (конъюнкиции). Если наложить на текущий подход идеи из моей дисертации, то можно
%сделать вычисления на графе (а так же и его построение) более эффективными. Например, номинальные
%признаки разбивают пространство векторов ошибок на несвязанные зоны и не имеет смысла сравнивать
%(искать связь) правила в которых есть терм на номинальном признаке и пороги в этих термах разные.

%\balance
\begin{table}[t]
  \centering
  \footnotesize
  \tabcolsep=5pt
    \begin{tabular}[t]{|l|r|r|r|r|r|r|}
    \headline
           & \multicolumn{2}{c|}{ComBoost-C}  & \multicolumn{2}{c|}{ComBoost-B} & \multicolumn{2}{c|}{ComBoost-A}\\
    \headline
    задача     & обуч. & тест & обуч. & тест & обуч. & тест \\
    \headline
    australian  &6.8    &14.0   &9.9    &14.9   &6.2    &\textbf{13.8}\\
    echo-card   &0.1    &2.3    &0.2    &\textbf{0.9}    &0.1    &2.4\\
    german      &13.1   &\textbf{25.4}   &18.3   &27.6   &12.9   &26.0\\
    heart dis.  &8.0    &18.9   &11.1   &\textbf{18.5}   &7.6    &19.3\\
    hepatitis   &3.0    &19.9   &7.8    &\textbf{18.0}   &1.8    &21.4\\
    labor       &0.6    &\textbf{8.9}    &1.1    &11.9   &0.5    &10.9\\
    liver       &11.3   &\textbf{31.4}   &33.0   &42.7   &8.3    &32.3\\
    \hline
    \end{tabular}
  \caption{Средняя частота ошибок (в~процентах) на обучающей и тестовой выборке
    по различным задачам и различным методом контроля переобучения.}
  \label{tab:ComBoostResults}
\end{table}

\begin{thebibliography}{00}
%\bibitem{vapnik74rus}
%    \BibAuthor{Вапник\;В\,Н., Червоненкис\;А.\,Я.}
%    Теория распознавания образов. "--- М.:~Наука, 1974.
%
%\bibitem{donskoy92bashta}
%    \BibAuthor{Донской\;В.\,И., Башта\;А.\,И.}
%    \BibTitle{Дискретные модели принятия решений при неполной информации}. "---
%    Симферополь: Таврия, 1992. "--- 166~с.

\bibitem{Inyakin2008}
	\BibAuthor{Дюкова\;Е.\,В., Инякин\;А.\,С.}
	\BibTitle{Об асимптотически оптимальном построении тупиковых покрытий целочисленной матрицы}~//
	 Математические вопросы кибернетики. М.:~Физматлит. "--- 2008. "--- Вып.\,17. "--- С.\,247--262.

\bibitem{voron11mmro}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Комбинаторная теория переобучения: результаты, приложения и открытые проблемы}.~//
    Математические методы распознавания образов:
    15-ая Всеросс. конф.: Докл.~--- М.:~МАКС Пресс, 2011.~--- С.\,40--43.

\bibitem{ivahnenko11mmro}
    \BibAuthor{Ивахненко\;А.\,А., Воронцов\;К.\,В.}
    \BibTitle{Критерии информативности пороговых логических правил с~поправкой на переобучение порогов}.~//
    Математические методы распознавания образов:
    15-ая Всеросс. конф.: Докл.~--- М.:~МАКС Пресс, 2011.~--- С.\,48-51.

%\bibitem{voron07kochedykov}
%    \BibAuthor{Кочедыков\;Д.\,А., Ивахненко\;А.\,А., Воронцов\;К.\,В.}
%    \BibTitle{Применение логических алгоритмов классификации в~задачах кредитного скоринга
%    и~управления риском кредитного портфеля банка}~//
%    13-ая Всеросс. конф.: Докл.~--- М.:~МАКС Пресс, 2007.~--- С.\,484--488.

\bibitem{lbov81methods}
    \BibAuthor{Лбов\;Г.\,С.}
    \BibTitle{Методы обработки разнотипных экспериментальных данных}~---
    Новосибирск: Наука, 1981.

\bibitem{matsenov07mmro}
    \BibAuthor{Маценов\;А.\,А.}
    \BibTitle{Комитетный бустинг: минимизация числа базовых алгоритмов при простом голосовании}~//
    Математические методы распознавания образов:
    13-ая Всеросс. конф.: Докл.~--- М.:~МАКС Пресс, 2007.~--- С.\,180--183.

\English
\bibitem{cohen99simple}
    \BibAuthor{Cohen\;W.\,W., Singer\;Y.}
    \BibTitle{A Simple, Fast and Effective Rule Learner}~//
    Proc. of the 16 National Conference on Artificial Intelligence, 1999.~---
    Pp.\,335--342.

\bibitem{furnkranz05rocnrule}
    \BibAuthor{F\"urnkranz\;J., Flach\;P.\,A.}
    \BibTitle{ROC `n' Rule Learning-Towards a Better Understanding of Covering Algorithms}~//
    Machine Learning.~--- 2005.~--- Vol.\,58, No.\,1.~--- Pp.\,39--77.

\bibitem{martin97exact}
    \BibAuthor{Martin\;J.\,K.}
    \BibTitle{An exact probability metric for decision tree splitting and stopping}~//
    Machine Learning.~--- 1997.~--- Vol.\,28, No.\,2--3.~--- Pp.\,257--291.

%\bibitem{voron08pria}
%    \BibAuthor{Vorontsov\;K.\,V.}
%    Combinatorial probability and the tightness of generalization bounds~//
%    {Pattern Recognition and Image Analysis.} ~---
%    2008.~---
%    Vol.\,18, No.\,2.~---
%    Pp.\,243--259.

%\bibitem{voron09roai2008}
%    \BibAuthor{Vorontsov\;K.\,V.}
%    Splitting and similarity phenomena in the sets of classifiers and their effect on the probability of overfitting~//
%    {Pattern Recognition and Image Analysis.} ~---
%    2009.~---
%    Vol.\,19, No.\,3.~---
%    Pp.\,412--420.

\bibitem{voron10pria}
    \BibAuthor{Vorontsov\;K.\,V.}
    Exact combinatorial bounds on the probability of overfitting for empirical risk minimization~//
    {Pattern Recognition and Image Analysis.} ~---
    2010.~---
    Vol.\,20, No.\,3.~---
    Pp.\,269--285.

\bibitem{voron10roai}
    \BibAuthor{Vorontsov\;K.\,V., Ivahnenko\;A.\,A., Reshetnyak\;I.\,M.}
    \BibTitle{Generalization bound based on the splitting and connectivity graph of the set of classifiers}~//
    Pattern Recognition and Image Analysis: new information technologies (PRIA-10),
    St.~Petersburg, Russian Federation, December 5--12, 2010.

\bibitem{voron11premi}
    \BibAuthor{Vorontsov\;K.\,V., Ivahnenko\;A.\,A.}
    \BibTitle{Tight Combinatorial Generalization Bounds for Threshold Conjunction Rules}~//
    4-th International Conference on Pattern Recognition and Machine Intelligence,
    June~27 -- July~1, 2011.
    Lecture Notes in Computer Science. Springer-Verlag, 2011.~--- Pp. 66--73.

\bibitem{Yankovskaya2007b}
    \BibAuthor{Yankovskaya\;A.\,E,  Tsoy\;Y.\,R.}
    \BibTitle{Selection of optimal set of diagnostic tests with use of evolutionary app\-ro\-ach in intelligent systems}~//
    5-th EUSFLAT Conference New Dimensions in Fuzzy Logic and Related Technologies.~---
    Vol.\;2.~--- Ostrava, Chech Republic, 2007.~--- Pp.\,267--270.
\end{thebibliography}

\end{document}
