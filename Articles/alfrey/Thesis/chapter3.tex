\chapter[Теоретико"=групповой подход]{Теоретико"=групповой подход}
Основная задача данной главы "---
получение новых комбинаторных методов вывода оценок вероятности переобучения.

\label{chap:2}
\section{Рандомизированный метод обучения и~РМЭР}

При минимизации эмпирического риска может возникать неоднозначность "---
несколько алгоритмов из~$A(\Xl)\equiv \Argmin_{a\in A} n(a,X)$
могут иметь одинаковое число ошибок на~обучающей выборке.
В~\cite{voron09mmro} для устранения неоднозначности и~получения точных верхних
оценок вероятности переобучения использовалась \emph{пессимистичная}
минимизация эмпирического риска~\eqref{eq:mu-ERM-pessimistic} "--- предполагалось, что в~случае неоднозначности
выбирается алгоритм с~наибольшим числом ошибок на~генеральной выборке~$\XX$.
Это не~устраняет неоднозначность окончательно.
Возможны ситуации, когда несколько алгоритмов имеют
наименьшее число ошибок на~обучающей выборке~$\Xl$
и~одинаковое число ошибок на~генеральной выборке~$\XX$.
\mbox{В~таких} случаях на~множестве алгоритмов вводился линейный порядок,
и~среди неразличимых алгоритмов выбирался алгоритм с~б\'ольшим порядковым номером.
Введение приоритетности алгоритмов является искусственным приемом,
не~имеющим адекватных аналогов среди известных методов обучения.

\emph{Рандомизированный метод обучения} произвольному множеству алгоритмов $A\subseteq \AA$
и~произвольной обучающей выборке~$\Xl \in \XXell$ ставит в~соответствие функцию
распределения весов на~множестве~алгоритмов:
\begin{equation}
    \label{eq:mu-rand-as-function}
    \mu : 2^\AA \times \XXell \rightarrow \{f : \AA \rightarrow [0, 1]\}.
\end{equation}
Естественно полагать, что эта функция нормирована и~может быть интерпретирована как вероятность
получить каждый алгоритм в~результате обучения.

Детерминированный метод обучения является частным случаем рандомизированного,
когда функция распределения весов~$f(a)$
принимает единичное значение ровно на~одном алгоритме и~нулевое на~всех остальных.

Заметим, что вместо определения~\eqref{eq:mu-rand-as-function}
можно задать то~же самое отображение эквивалентным способом:
\[
    \mu : 2^\AA \times \XXell \times \AA \rightarrow [0, 1].
\]

\emph{Рандомизированный метод минимизации эмпирического риска (РМЭР)}
выбирает произвольный алгоритм из~множества~$A(X)$ случайно и~равновероятно:
\begin{equation}
    \label{eq:mu-rand-ERM}
    \mu(A, X, a) = \frac {[a\in A(X)]}{|A(X)|}.
\end{equation}

Поскольку в~задаче статистического обучения появляется второй независимый источник случайности,
определение вероятности переобучения~$\Q(A)$ приходится модифицировать.
Наиболее естественный вариант модификации "--- усреднение по~множеству~$A(X)$:
\begin{equation}
\label{eq:QEps-rand-ERM}
    \Q(A)
    =
    \Expect
    \frac1{|A(X)|} \sum_{a\in A(X)}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\end{equation}

Переставим местами знаки суммирования, ${\Expect\sum=\sum\Expect}$.
Получим сумму по~всем алгоритмам ${a\in A}$,
каждое слагаемое которой обозначим через $\Q(a, A)$
и~назовем \emph{вкладом алгоритма}~$a$ в~вероятность переобучения:
\[
    \Q(A)
    =
    \sum_{a\in A}
    \Q(a,A),
    \quad
    \Q(a,A)
    =
    \Expect
    \frac{\bigl[a\in A(X)\bigr]}{|A(X)|}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\]

Аналогичным образом введем \emph{вероятность реализации алгоритма}~$a$:
\begin{equation}
\label{eq:Pa-rand-ERM}
    P(a,A)
    =
    \Expect \frac
        {\bigl[a\in A(X)\bigr]}
        {|A(X)|}.
\end{equation}

В~частном случае, когда для всех $X \in \XXell$ множество~$A(X)$ состоит из~единственного алгоритма,
вероятность переобучения~\eqref{eq:QEps-rand-ERM} и~вероятность реализации~\eqref{eq:Pa-rand-ERM}
принимают привычный вид:
\[
    \Q(A) = \Expect
    \bigl[
        \delta(\mu X,X) \geq \eps
    \bigr],
    \quad
    P(a,A) = \Expect
    \bigl[
        a\!=\!\mu X
    \bigr].
\]

Рандомизированный МЭР~$\mu_r$
занимает промежуточное положение между
\emph{оптимистичным} и~\emph{пессимистичным} методами:
\begin{align*}
    \mu_o \Xl &= \arg\!\!\min\limits_{a\in A(\Xl)} n(a,\Xk) \text{ "--- оптимистичный ММЭР}; \\
    \mu_p \Xl &= \arg\!\!\max\limits_{a\in A(\Xl)} n(a,\Xk) \text{ "--- пессимистичный ММЭР.}
\end{align*}
Приводим без доказательства следующее утверждение:
для произвольного множества алгоритмов $A \subseteq \AA$
и~каждого $\epsilon \in (0, 1]$ справедлива цепочка неравенств:
\begin{equation}
    \Q(\mu_o, A) \leq \Q(\mu_o, A) \leq \Q(\mu_p, A).
\end{equation}

\section{Перестановки объектов}

Введем симметрическую группу~$S_L$ всех $L!$~перестановок, действующую на~выборке ${\XX = \{x_1,\ldots,x_L\}}$.
Для произвольной перестановки ${\pi\in S_L}$
обозначим через~$\pi x$ образ объекта~${x\in\XX}$ под действием перестановки~$\pi$.
Действие перестановок на~объектах естественным образом переносится
на~подмножества объектов, на~алгоритмы как бинарные векторы ошибок длины~$L$
и~на~множества алгоритмов.
Эти действия определены следующим образом:

\begin{itemize}
\item действие перестановки~$\pi \in S_L$ на~подмножество объектов:
\[\pi X = \bigl\{ \pi x \colon x\in X \bigr\};\]
\item действие перестановки~$\pi \in S_L$ на~алгоритм:
\[\pi\vec a
    = \bigl( I(\pi a, x_i) \bigr){}_{i=1}^L
    = \bigl( I(a, \pi^{-1}x_i) \bigr){}_{i=1}^L;\]
\item действие перестановки~$\pi \in S_L$ на~множество алгоритмов:
\[\pi A = \bigl\{ \pi a \colon a\in A \bigr\}.\]
\end{itemize}

Заметим, что действие одной и~той~же перестановки~$\pi$
сначала на~выборку~$\XX$, а~затем на~алгоритм~$a$
восстанавливает исходный вектор ошибок алгоритма~$a$.
Благодаря такому определению
действие на~алгоритм обладает рядом полезных свойств.

\begin{Lemma}
\label{lem:piSL}
    Свойства действия произвольной перестановки ${\pi\in S_L}$:

    1) $I(\pi a, \pi x) = I(a,x)$ для любых $a\in A$ и~$x\in\XX$;

    2) $n(\pi a, \XX) = n(a,\XX)$ для любого $a\in A$;

    3) $n(\pi a, \pi X) = n(a, X)$ для любых $a\in A$ и~$X\subseteq\XX$;

    4) $\delta(\pi a, \pi X) = \delta(a, X)$ для любых $a\in A$ и~$X\subseteq\XX$;

    5) $[a \in A(X)] = [\pi a \in (\pi A)(\pi X)]$ для любых $a\in A$ и~$X\subseteq\XX$;

    6) $|A(X)| = |(\pi A)(\pi X)|$ для любых~$A$ и~$X\subseteq\XX$;

    7) $\rho(a, a') = \rho(\pi a, \pi a')$ для любых $a, a' \in A$, где
       $\rho(a, a')$ "--- расстояние Хэмминга векторами между ошибок алгоритмов~$a$ и~$a'$:
\[
    \rho(a, a') = \sum \limits_{x \in \XX} |I(a, x) - I(a', x)|.
\]
\end{Lemma}

\begin{vkProof}
    Свойство 1) следует из~определения:
    \[
        I(\pi a, \pi x)
        = I(a, \pi^{-1} \pi x)
        = I(a,x).
    \]
    Свойство 2) следует из~свойства 1):
    \[
        n(\pi a, \XX)
        = \sum_{i=1}^L I(\pi a, x_i)
        = \sum_{i=1}^L I(\pi a, \pi x_i)
        = \sum_{i=1}^L I(a, x_i)
        = n(a,\XX).
    \]
    Свойство 3) также следует из~свойства 1):
    \[
        n(\pi a, \pi X)
        = \sum_{x\in \pi X} I(\pi a, x)
        = \sum_{x\in X} I(\pi a, \pi x)
        = \sum_{x\in X} I(a, x)
        = n(a,X).
    \]
    Свойство 4) следует из~свойства 3):
    \begin{align*}
        \delta(a, X)
        & = \frac{L-n(a,X)}k - \frac{n(a,X)}\ell = \\
        & = \frac{L-n(\pi a,\pi X)}k - \frac{n(\pi a,\pi X)}\ell
        = \delta(\pi a, \pi X).
    \end{align*}
    Свойство 5) следует из~определения~\eqref{eq:A(X)} и~свойства 1):

    \begin{align*}
        a_0 \in A(X) \;\Leftrightarrow \;\;
        & a_0 \in \argmin_{a \in A} n(a, X) \;\Leftrightarrow {}\\
        & \forall a \in A \rightarrow n(a_0, X) \leq n(a, X) \; \Leftrightarrow {}\\
        & \forall a \in A \rightarrow n\big(\pi a_0, \pi X\big)
        \leq n\big(\pi a, \pi X \big) \; \Leftrightarrow {}\\
        & \forall a' \in \pi A \rightarrow n\big(\pi a_0, \pi X \big)
        \leq n\big(a', \pi X \big) \; \Leftrightarrow {}\\
         & \pi a_0 \in \argmin_{a' \in \pi A} n(a', \pi X) \;\Leftrightarrow \;
            \pi a_0 \in (\pi A)(\pi X).
    \end{align*}

    Свойство 6) следует из~свойства 5):
    \begin{align*}
       |A(X)| = \sum_{a \in A}[a \in A(X)]
            & = \sum_{a \in A}[\pi a \in (\pi A)(\pi X)] = \\
            & = \sum_{a' \in \pi A}[a' \in (\pi A)(\pi X)]
             = |(\pi A)(\pi X)|.
    \end{align*}

    Свойство 7) следует из~свойства 1):
    \begin{align*}
        \rho(\pi a, \pi a')
            & = \sum_{x \in \XX} |I(\pi a, x) - I(\pi a', x)| = {}\\
            &{} = \sum_{x' \in \XX} |I(\pi a, \pi x') - I(\pi a', \pi x')| = {}\\
            &{} = \sum_{x' \in \XX} |I(a, x') - I(a', x')|
              = \rho(a, a').
    \end{align*}
\vskip-5ex
\end{vkProof}

В дальнейшем нам будет нужно доказывать утверждения, аналогичные лемме~\ref{lem:piSL}, но для более сложных функций.
Чтобы упростить эту задачу, введем следующую классификацию функций.

\begin{itemize}
  \item Симметричной функцией \emph{первого рода} будем называть $g \colon \AA \times \XXell \rightarrow \RR$, такую что для всех $\pi \in S_L$ выполнено $g(a, X) = g(\pi a, \pi X)$;
  \item Симметричной функцией \emph{второго рода} будем называть $G \colon 2^\AA \times \XXell \rightarrow 2^\AA$, такую что для всех $\pi \in S_L$ выполнено $\pi G(A, X) = G(\pi A, \pi X)$;
  \item Симметричной функцией \emph{третьего рода} будем называть $f \colon 2^\AA \times \XXell \rightarrow \RR$, такую что для всех $\pi \in S_L$ выполнено $f(A, X) = f(\pi A, \pi X)$.
\end{itemize}

Лемма~\ref{lem:piSL} утверждает, что функции $n(a, X)$ и~$\nu(a, X)$ являются симметричными функциями первого рода, а~$A(X)$ как функция~$A$ и~$X$ является симметричной функцией второго рода.

Две следующие теоремы позволяют строить новые симметричные функции из~уже имеющихся:
\begin{Theorem}
\label{th:sym1}
    Пусть
        $g_1, g_2, \dots, g_p$ "--- симметричные функции первого рода,
        $f_1, f_2, \dots, f_p$ "--- симметричные функции третьего рода,
        $F \colon \RR^p \rightarrow \RR$ "--- произвольная функция многих переменных.
    Тогда
        $F(g_1, g_2, \dots, g_p)$ "--- вновь симметричная функция первого рода,
        $F(f_1, f_2, \dots, f_p)$ "--- симметричная функция третьего рода.
\end{Theorem}
\begin{vkProof}
Проведя элементарные выкладки, получим
\[
    F(\pi a, \pi X) \equiv F(g_1(\pi a, \pi X), \dots, g_p(\pi a, \pi X)) = F(g_1(a, X), \dots, g_p(a, X)) \equiv F(a, X),
\]
и~аналогично для функций третьего рода.
\end{vkProof}

\begin{Theorem}
\label{th:sym2}
    Пусть $g$ "--- симметричная функция первого рода, $G$ "--- симметричная функция второго рода.
    Тогда
    \[
        f(A, X) \equiv |G(A, X)| \text{ и~} f(A, X) \equiv \sum\limits_{a \in G(A, X)} g(a, X)
    \]
являются симметричными функциями третьего рода.
\end{Theorem}
\begin{vkProof}
Заметим, что для любого $A \subset \AA$ выполнено $|A| = |\pi A|$, поскольку $\pi$, как элемент группы, является биекцией. Следовательно,
\[|G(A, X)| = |\pi G(A, X)| = |G(\pi A, \pi X)|\].

Для функции $f(A, X) \equiv \sum\limits_{a \in G(A, X)} g(a, X)$ запишем цепочку равенств:
\begin{align*}
    f(\pi A, \pi X) &= \sum\limits_{a \in G(\pi A, \pi X)} g(a, \pi X) = \sum\limits_{a \in \pi G(A, X)} g(a, \pi X) = \\
     &= \sum\limits_{a \in G(A, X)} g(\pi a, \pi X) = \sum\limits_{a \in G(A, X)} g(a, X) = f(A, X).
\end{align*}
\end{vkProof}

Из приведенных выше теорем следует, что вклад каждого разбиения в~вероятность переобучения РМЭР является симметричной функцией третьего рода.
\[
\Q(A, X) \equiv \frac 1{|A(X)|} \sum\limits_{a \in A(X)}[\delta(a, X) \geq \eps] = \Q(\pi A, \pi X).
\]
Это утверждение, как и~большинство теорем следующего параграфа, оказывается верно не~только для РМЭР, но и~для более широкого класса \emph{корректных} методов обучения.

\begin{Def}
    Рандомизированный метод обучения $\mu : 2^\AA \times \XXell \times \AA \rightarrow [0, 1]$ называется \emph{корректным}, если
    при любых $A \in 2^\AA$,\; ${\Xl \in \XXell}$,\; $a,\, b \in A$ и~$\pi \in S_L$
    выполнены следующие условия:
    \begin{enumerate}
        \item[1)] нормировка:
              \begin{equation}
                \sum\limits_{a \in A} \mu(A, \Xl, a) = 1;
              \end{equation}
        \item[2)] неразличимость алгоритмов с~одинаковой частотой ошибок на~обучении:
              \begin{equation}
              n(a, \Xl) = n(b, \Xl) \;\to\; \mu(A, \Xl, a) = \mu(A, \Xl, b);
              \end{equation}
        \item[3)] инвариантность результата обучения относительно замены множества алгоритмов~$A$ на~$\pi A$:
              \begin{equation}
                    \mu(A, \Xl, a) = \mu \bigl( \pi A, \pi \Xl, \pi a \bigr).
              \end{equation}
    \end{enumerate}
\end{Def}

Первое условие означает <<вероятностную>> нормировку весов алгоритмов
и~обеспечивает нулевую <<вероятность>> алгоритмам, не~принадлежащих множеству~$A$.
Второе условие означает, что при любом разбиении $\XX = \Xl \sqcup \Xk$, $\Xl \in \XXell$
вероятность получить алгоритм в~результате обучения
зависит только от~количества ошибок алгоритма на~обучении.
Третье условие означает, что результат обучения не~изменится,
если подействовать перестановкой~$\pi$ одновременно
и~на~множество объектов~$\XXell$, и~на~множество алгоритмов~$\AA$.

\begin{Theorem}
Рандомизированный МЭР $\mu(A, \Xl, a) = \frac{\bigl[ a \in A(\Xl) \bigr]}{|A(\Xl)|}$
является корректным рандомизированным методом обучения.
\end{Theorem}

\begin{vkProof}
Первое условие проверяется явно:
\[
    \sum \limits_{a\in A}\mu(A,\Xl,a) =
    \sum \limits_{a \in A(\Xl)}\frac1{|A(\Xl)|} = 1.
    \]

Для доказательства второго утверждения достаточно заметить,
что два алгоритма $a_1$ и~$a_2$ с~равным числом ошибок на~обучении
могут лежать в~множестве $A(\Xl)$ только одновременно.
Следовательно, вероятность получить каждый из~алгоритмов в~результате обучения
равна либо нулю, либо $\frac 1{|A(\Xl)|}$.

Для проверки третьего условия достаточно доказать, что
\[
    a_0 \in \Argmin_{a \in A} n(a, \Xl)
    \;\;\Leftrightarrow\;\;
    \pi a_0 \in \Argmin_{a \in \pi A} n(a, \pi \Xl).\]

Используя лемму~\ref{lem:piSL},
проведем цепочку равносильных утверждений:
\[
    \begin{aligned}
    a_0 \in & \Argmin_{a \in A} n(a, \Xl) \;\;\Leftrightarrow \\
    & \;\;\Leftrightarrow \;\;
    \forall a \in A \rightarrow n(a_0, \Xl) \leq n(a, \Xl) \;\; \Leftrightarrow\\
    & \;\; \Leftrightarrow \;\;
    \forall a \in A \rightarrow n\big(\pi a_0, \pi \Xl\big)
            \leq n\big(\pi a, \pi \Xl \big) \;\; \Leftrightarrow\\
    & \;\; \Leftrightarrow \;\;
    \forall a' \in \pi A \rightarrow n\big(\pi a_0, \pi \Xl \big)
            \leq n\big(a', \pi \Xl \big) \;\; \Leftrightarrow\\
    & \;\; \Leftrightarrow \;\;
    \pi a_0 \in \Argmin_{a \in \pi A} n(a, \pi \Xl).
    \end{aligned}
\]

Теорема доказана.
\end{vkProof}

\section{Группа симметрии множества алгоритмов}
\begin{Def}
\emph{Группой симметрии} множества алгоритмов~$A$ называют
множество всех перестановок, действие которых на~$A$ не~меняет его:
\[
    \Sym(A) = \{\pi \in S_L \colon \pi A = A\}.
\]
\end{Def}
Если подействовать любой из~перестановок $\pi\in \Sym(A)$ на~строки матрицы ошибок множества~$A$,
то~получится то~же самое множество столбцов;
переставив столбцы, можно получить исходную матрицу ошибок.
Очевидно, что множество~$\Sym(A)$ является группой.

\begin{Example}
    Рассмотрим множество алгоритмов, заданное матрицей ошибок
    \[
        \bordermatrix{& a_1 & a_2 & a_3 & a_4 & a_5 \cr
            x_1 & 1 & 1 & 1 & 0 & 0 \cr
            x_2 & 0 & 1 & 1 & 1 & 0 \cr
            x_3 & 0 & 0 & 1 & 1 & 1 \cr
            x_4 & 1 & 0 & 0 & 1 & 1 \cr
            x_5 & 1 & 1 & 0 & 0 & 1 \cr
        }
        \hspace{45pt}
        {\catcode`"12
            \xy/r3pc/:
            {\xypolygon5"X"{~={90}~>{-}~><{}~*{x_\xypolynode}}},
            +(0,-1)="bot",
            "X1";"bot"**@{.}
            \endxy
        }
    \]

    Группа симметрий данного множества алгоритмов
    совпадает с~группой симметрий правильного пятиугольника
    и~называется \emph{диэдральной группой}.
    Образующими элементами группы являются
    циклическая перестановка $\pi_1 = (x_1, x_2, x_3, x_4, x_5)$
    и~осевая симметрия $\pi_2 = (x_2, x_5)(x_3, x_4)$.
\end{Example}

Пусть далее $G \subseteq \Sym(A)$ "--- произвольная подгруппа группы~$\Sym(A)$.

Для любой перестановки ${\pi\in G}$ и~любого алгоритма ${a\in A}$
алгоритм~$\pi a$ снова лежит в~$A$.
В~таких случаях говорят, что группа~$G$ \emph{действует} на~множестве~$A$.

\emph{Орбитой} алгоритма~$a\in A$ называется множество алгоритмов
${Ga = \bigl\{ \pi a \colon \pi \in G \bigr\}}$.
Орбита также целиком лежит~в~$A$.
Орбиты двух различных алгоритмов $Ga$ и~$Ga'$ либо совпадают, либо не~пересекаются.
Следовательно, множество~$A$ разбивается на~непересекающиеся подмножества "--- орбиты:
\[
    A
    \;=\!\! \bigsqcup_{\omega\in\Omega(A)} \!\!\! \omega
    \;=\!\! \bigsqcup_{\omega\in\Omega(A)} \!\!\! Ga_\omega,
\]
где
$\Omega(A)$ "--- множество всех орбит в~$A$,
$a_\omega$ "--- произвольный представитель орбиты~$\omega$.

Из~свойства~2) леммы~\ref{lem:piSL} следует, что
алгоритмы одной орбиты обязательно лежат в~одном слое.
Обратное, вообще говоря, неверно.

\begin{Lemma}
\label{th:equalPQa}
    Алгоритмы из~одной орбиты имеют равные вероятности реализации и~равные вклады в~вероятность переобучения:
    для любой перестановки $\pi$ из~$G$
    \[
        P(a,A) = P(\pi a,A),
        \quad
        \Q(a,A) = \Q(\pi a,A).
    \]
\end{Lemma}
\begin{vkProof}
    Воспользуемся определением вероятности реализации~\eqref{eq:Pa-rand-ERM}, свойствами~5), 6)
    из~леммы~\ref{lem:piSL}, и~свойством $A = \pi A$:
    \[
        P(a,A)
        =
        \Expect \frac {\bigl[a\in A(X)\bigr]} {|A(X)|}
        =
        \Expect \frac {\bigl[\pi a\in (\pi A)(\pi X)\bigr]} {|(\pi A)(\pi X)|}
        =
        \Expect \frac {\bigl[\pi a\in A(\pi X)\bigr]} {|A(\pi X)|}.
    \]
    Под знаком $\Expect$ можно всюду заменить $\pi X$ на~$X$,
    так как результат не~зависит от~порядка суммирования разбиений:
    \[
        P(a,A)
        =
        \Expect \frac {\bigl[\pi a\in A(X)\bigr]} {|A(X)|}
        =
        P(\pi a,A).
    \]

    Воспользуемся определением вероятности реализации~\eqref{eq:Pa-rand-ERM},
    свойствами~4), 5), 6)  из~леммы~\ref{lem:piSL}, и~свойством $A = \pi A$:
\begin{align*}
        \Q(a,A)
        & =
        \Expect \frac {\bigl[a\in A(X)\bigr]} {|A(X)|}
                \bigl [\delta(a,X) \geq \eps \bigr] = {}\\
        & {}=
        \Expect \frac {\bigl[\pi a\in (\pi A)(\pi X)\bigr]} {|(\pi A)(\pi X)|}
                \bigl [\delta(\pi a, \pi X) \geq \eps \bigr] = {}\\
        & {}=
        \Expect \frac {\bigl[\pi a\in A(\pi X)\bigr]} {|A(\pi X)|}
                \bigl [\delta(\pi a, \pi X) \geq \eps \bigr].
\end{align*}
    Вновь заменяя $\pi X$ на~$X$ под знаком $\Expect$, получим:
    \[
        \Q(a,A)
        =
         \Expect \frac {\bigl[\pi a\in A(X)\bigr]} {|A(X)|}
                \bigl[ \delta(\pi a, X) \geq \eps \bigr]
        =
        \Q(\pi a,A).
    \]
    \vskip-4ex
\end{vkProof}

\paragraph{Разложение вероятности переобучения по~орбитам множества алгоритмов.}
Из~теоремы о~равном вкладе алгоритмов одной орбиты
немедленно следует формула разложения вероятности переобучения по~орбитам.
Она является основным инструментом получения точных оценок
для РМЭР.

\begin{Theorem}
\label{th:QAorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения по~орбитам множества~$A$:
    \begin{equation}
    \label{eq:QAorbit}
        \Q(A)
        =
        \sum_{\omega\in\Omega(A)} \!\! |\omega| \:
            \Expect
            \frac{\bigl[ a_\omega \in A(X) \bigr]}{|A(X)|}
            \bigl[
                \delta(a_\omega,X) \geq \eps
            \bigr],
    \end{equation}
    где
    $\Omega(A)$ "--- множество всех орбит в~$A$,
    $a_\omega$ "--- произвольный представитель орбиты~$\omega$.
\end{Theorem}
\begin{vkProof}
    Перегруппируем слагаемые в~\eqref{eq:QEps-rand-ERM} по~орбитам множества~$A$,
    затем применим лемму~\ref{th:equalPQa} о~равном вкладе алгоритмов одной орбиты:
    \begin{align*}
        \Q(A)
        & =
        \sum_{\omega\in\Omega(A)} \sum_{a\in\omega}
        \Expect
        \frac{\bigl[a\in A(X)\bigr]}{|A(X)|}
        \bigl[
            \delta(a,X) \geq \eps
        \bigr]
        =
    \\
        & =
        \sum_{\omega\in\Omega(A)} \!\! |\omega| \:
            \Expect
            \frac{\bigl[ a_\omega \in A(X) \bigr]}{|A(X)|}
            \bigl[
                \delta(a_\omega,X) \geq \eps
            \bigr].
    \end{align*}
    \vskip-4ex
\end{vkProof}

\paragraph{Разложение вероятности переобучения по~орбитам множества разбиений.}
В~\cite{tolstikhin10iip} отмечено, что по~аналогии
с~действием группы $\Sym(A) $ на~множестве~$A$
можно рассматривать действие этой же группы на
множестве $\XXell$ всех $\ell$"~элементных подмножеств генеральной выборки~$\XX$.

\emph{Орбитой} выборки ${X\in \XXell}$ называется множество
${GX = \bigl\{ \pi X \colon \pi \in G \bigr\}}$.
Множество всех выборок длины $\ell$ разбивается на~непересекающиеся орбиты:
\[
    \XXell
    \;=\!\! \bigsqcup_{\tau\in\Omega\XXell} \!\!\! \tau
    \;=\!\! \bigsqcup_{\tau\in\Omega\XXell} \!\!\! GX_\tau,
\]
где
$\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
$X_\tau$ "--- представитель орбиты~$\tau$.

\begin{Theorem}[Толстихин,~\cite{tolstikhin10iip}]
\label{th:QAXorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения одновременно
    и~по~орбитам множества~$A$,
    и~по~орбитам множества~$\XXell$:
    \begin{equation}
        \label{eq:QAXorbit}
        \Q(A)
        =
        \sum_{\omega\in\Omega(A)}
        \frac{|\omega|}{\CLl}
        \sum_{\tau\in\Omega\XXell}
        |\{X\in \tau\colon a_{\omega}\in A(X)\}|
        \frac{\bigl[\delta(a_{\omega},X_\tau) \geq \eps \bigr]}{|A(X_\tau)|}.
    \end{equation}
    где
    $\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
    $\Omega(A)$ "--- множество всех орбит в~$A$,
    $a_\omega$ "--- представитель орбиты~$\omega$,
    $X_\tau$ "--- представитель орбиты~$\tau$.
\end{Theorem}
Это разложение одновременно использует и~орбиты на~множестве алгоритмов,
и~орбиты на~множестве разбиений выборки.

В настоящей работе выводится еще одно разложение вероятности переобучения,
в~котором используются лишь орбиты на~множестве разбиений выборки.
Оно оказывается проще, чем~\eqref{eq:QAXorbit},
и~вместе с~тем оказывается полезным при выводе формул вероятности переобучения
для целого ряда модельных семейств.

Представим вероятность переобучения в~виде суммы вкладов разбиений:
\[
    \Q(A)
    =
    \Expect\,
    Q(A,X),
    \quad
    \Q(A,X)
    =
    \frac1{|A(X)|}
    \sum_{a\in A(X)}
    \bigl[\delta(a,X) \geq \eps \bigr],
\]
где $\Q(A,X)$ "--- вклад разбиения~$\Xl\sqcup\Xk$ в~вероятность переобучения.
Поскольку разбиениям $X\sqcup\Xk$ взаимно однозначно соответствуют выборки $X\in\XXell$,
далее будем говорить также о~\emph{вкладе выборки}~$X$ в~вероятность переобучения.


\begin{Lemma}
\label{th:equalQX-decomposition}
Пусть для некоторой функции $f \colon 2^\AA \times \XXell \rightarrow \RR$ для всех $A \subset \AA$, $X~\in~\XXell$ и~всех $\pi \in \Sym(A)$ выполнено условие
$f(A, X) = f(A, \pi X)$. Тогда справедливо следующее разложение:
\[
    \sum_{X \in \XXell} f(A, X) = \sum_{\tau \in \Omega(\XXell)} |\tau| f(A, X_\tau).
\]
\end{Lemma}
\begin{vkProofShort} с~очевидностью следует из~группировки равных слагаемых.\end{vkProofShort}

\begin{Lemma}
\label{th:equalQX}
    Выборки из~одной орбиты имеют равные вклады в~вероятность переобучения:
    $\Q(A,X) = \Q(\pi A,X)$
    для любой перестановки $\pi$ из~$G$.
\end{Lemma}
\begin{vkProof}
Заметим, что выражение
$\frac{\bigl[\delta(a, X) \geq \eps \bigr]}{|A(X)|}$
является симметричной функцией первого рода.
Тогда по~теореме~\ref{th:sym2} выражение $\Q(X, A)$
является симметричной функцией третьего рода.
Следовательно,
\[\Q(A, X) = \Q(\pi A, \pi X) = \Q(A, \pi X).\]
Для завершения доказательства осталось воспользоваться леммой~\ref{th:equalQX-decomposition}.
%    Воспользуемся сначала определением вклада выборки,
%    свойствами~4), 5), 6) из~леммы~\ref{lem:piSL}, и~затем свойством $A = \pi A$:
%    \begin{align*}
%        \Q(A,X)
%        & =
%        \sum_{a\in A(X)} \frac{\bigl[\delta(a, X) \geq \eps \bigr]}{|A(X)|} = {}\\
%        & {}=
%        \sum_{a'\in (\pi A)(\pi X)} \frac{\bigl[\delta(a',\pi X) \geq \eps \bigr]}{|(\pi A)(\pi X)|} = {}\\
%        & {}=
%        \sum_{a'\in A(\pi X)} \frac{\bigl[\delta(a',\pi X) \geq \eps \bigr]}{|A(\pi X)|}
%          =
%          \Q(A, \pi X).
%    \end{align*}
%    \vskip-4ex
\end{vkProof}

\begin{Theorem}
\label{th:QXorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения по~орбитам множества~$\XXell$:
    \begin{equation}
    \label{eq:QXorbit}
        \Q(A)
        =
        \frac1{\CLl}
        \sum_{\tau\in\Omega\XXell}
        \frac{|\tau|}{|A(X_\tau)|}
        \sum_{a\in A(X_\tau)}
        \bigl[\delta(a,X_\tau) \geq \eps \bigr],
    \end{equation}
    где
    $\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
    $X_\tau$ "--- представитель орбиты~$\tau$.
\end{Theorem}
Доказательство аналогично доказательству теоремы~\ref{th:QAorbit}.

В~качестве примера применения полученных формул рассмотрим множество $\AA = \{0, 1\}^L$,
состоящее из~всех возможных бинарных векторов ошибок.

\begin{Theorem}
Вероятность переобучения РМЭР,
примененного к~множеству всех алгоритмов $\AA = \{0, 1\}^L$, дается формулой:
\[
    \Q(\AA) =
    \frac 1{2^k}\!
    \sum_{m = \lceil \epsilon k \rceil}^k
    \!\!\Binom{k}{m}.
\]
\end{Theorem}

\begin{vkProof}
Для всех перестановок $\pi \in S_L$ выполнено $\pi \AA = \AA$.
Следовательно, $\Sym(A) = S_L$.
Заметим, что для каждой пары обучающих выборок~$X$, $X'$
возможно указать перестановку $\pi \in S_L$, такую что $X' = \pi X$.
Такую ситуацию называют <<транзитивным действием группы $S_L$ на~множестве $\XXell$>>.
Для нас это означает, что имеется лишь одна орбита $\tau = \XXell$.
Выбрав произвольную выборку $\Xl$ в~качестве ее представителя
и~воспользовавшись теоремой~\ref{th:QXorbit}, получим
\[
    \Q(\AA) = \frac 1{|\AA(\Xl)|} \sum_{a \in \AA(\Xl)} [\delta(a, \Xl) \geq \eps].
\]
Множество $\AA(\Xl)$ состоит из~всех алгоритмов, не~допускающих ошибок на~$\Xl$.
Следовательно, $|\AA(\Xl)| = 2^k$.
Для завершения доказательства осталось заметить,
что переобученными в~$\AA(\Xl)$ будут те~и~только те~алгоритмы,
у~которых не~менее $\lceil \epsilon k \rceil$ ошибок на~контрольной выборке.
\end{vkProof}

Завершая параграф, интересно рассмотреть частный случай, когда все алгоритмы из~$A$
имеют равное число ошибок на~полной выборке.
\begin{Corollary}
\label{th:QXorbit-layer}
Пусть все $a \in A$ имеют равное число ошибок на~полной выборке: $n(a, \XX) = m$.
Тогда вероятность переобучения РМЭР записывается в~виде
\begin{equation}
    \label{eq:QXorbit-layer}
    \Q(A)
    =
    \frac1{C_L^\ell} \sum_{\tau \in \Omega(\XXell)}
    %\textbf{E}
    \!\!|\tau| \left[\min_{a \in A} n(a, X_\tau) \leq \frac \ell L(m - \epsilon k)\right].
\end{equation}
\end{Corollary}

\begin{vkProof}
Отметим, что в~рассматриваемом случае для~любой обучающей выборки~$X$ все алгоритмы из~множества~$A(X)$ либо переобучены, либо нет.
Действительно, согласно определению~$A(X)$ они имеют равное число ошибок на~обучении.
Число ошибок на~полной выборке одинаково по определению множества $A$.
Следовательно, все алгоритмы из~$A$ имеют равное число ошибок на~контрольной выборке, а также~равные уклонения частот.
Применим формулу~\eqref{eq:QXorbit} и получим следующее выражение для вероятности переобучения:
\[
    \Q(A)
    =
    \frac1{C_L^\ell} \sum_{\tau \in \Omega(\XXell)}
    %\textbf{E}
    \!\!|\tau| \left[\delta(a, X_\tau) > \varepsilon\right].
\]
Для получения формулы~\eqref{eq:QXorbit-layer} осталось выразить уклонение частот $\delta(a, X_\tau)$ через
число ошибок лучшего алгоритма на~обучении и~количество ошибок на~полной выборке~$m$.
\end{vkProof}

\section{Покрытия множества алгоритмов}
Допустим, что исходное множество алгоритмов~$A$ представлено в~виде разбиения на~непересекающиеся подмножества
$A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$ так, что в~каждое $A_i$ попали лишь алгоритмы с~близкими векторами ошибок.
В данной ситуации будем называть множества $A_i$ \emph{кластерами} алгоритмов.
Покажем, что задачу оценивания вероятности переобучения всего множества~$A$ можно свести к~оцениванию вероятности переобучения отдельных кластеров.

\begin{Lemma}
\label{lem:QEps-bool-cluster}
Пусть множество алгоритмов~$A$ произвольным образом представлено в~виде разбиения на~непересекающиеся подмножества
$A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$.
Тогда вероятность переобучения пессимистического метода минимизации эмпирического риска
оценивается сверху следующим выражением:
\begin{equation}
    \label{eq:QEps-bool-cluster}
    \Q(A) \leq \sum_{i=1}^t \Q(A_t).
\end{equation}
\end{Lemma}

\begin{vkProof}
Заметим, что достаточно доказать утверждение для $t = 2$ (общее утверждение получается по~индукции).
Обозначим через $\mu(A, X)$ алгоритм, выбранный пессимистическим методом минимизации эмпирического риска из~множества~$A$ по~обучающей выборке~$X$.
Рассмотрим произвольное разбиение $X \in \XXell$ и~покажем следующее:
\begin{equation}
    \label{lem:QEps-bool-cluster-proof-eq1}
    [\delta(\mu(A, X), X) \geq \eps] \leq [\delta(\mu(A_1, X), X) \geq \eps] +     [\delta(\mu(A_2, X), X) \geq \eps].
\end{equation}

Для разбиения~$X$ и~множеств $A_1, A_2, A$ множества  $A_1(X), A_2(X), A(X)$ определены согласно~\eqref{eq:A(X)}.
Обозначим через $n_1(X), n_2(X)$ и~$n(X)$ число ошибок на~обучающей выборке для алгоритмов из~$A_1(X)$, $A_2(X)$ и~$A(X)$, соответственно.
Очевидно, что $n_1(X) \geq n(X)$ и~$n_2(X) \geq n(X)$, но по~крайней мере одно из~этих неравенств обязательно обращается в~равенство.
Рассмотрим два случая: в~первом случае одно неравенство строгое, во втором оба неравенства обращаются в~равенство.

\textbf{Случай 1.}
Пусть для определенности $n_1(X) > n(X)$. Тогда $A_2(X) = A(X)$, а~следовательно, $\mu(A_2, X) = \mu(A, X)$.
Отсюда немедленно следует~\eqref{lem:QEps-bool-cluster-proof-eq1}.

\textbf{Случай 2.}
Из $n_1(X) = n_2(X) = n(X)$ следует, что $A(X) = A_1(X) \cup A_2(X)$.
Таким образом, либо $\mu(A, X) \in A_1(X)$, либо $\mu(A, X) \in A_2(X)$
(в зависимости от~того, в~какое из~этих двух множеств попал алгоритм с~наибольшим числом ошибок на~полной выборке).
Значит, вновь выполнено~\eqref{lem:QEps-bool-cluster-proof-eq1}.
\end{vkProof}

Для оценки $\Q(A_i)$ предположим, что в~каждом $A_i$ алгоритмы допускают равное число ошибок на~полной выборке.
Тогда, согласно следующей лемме, можно расширить $A_i$ до произвольного множества $B$ с~известной оценкой $\Q(B)$.

\begin{Lemma}[Толстихин,~\cite{tolstikhin10iip}]
\label{th:LayerGr}
Рассмотрим вложенные множества алгоритмов: $A_i \subseteq B \subseteq \AA$.
Допустим, что все алгоритмы $b \in B$ допускают по~$m = n(b, \XX)$ ошибок на~полной выборке.
Тогда для минимизации эмпирического риска для всех $\eps > 0$ выполнено неравенство $\Q(A_i) \leq \Q(B)$.
\end{Lemma}

\begin{vkProof}
Докажем утверждение для частного случая $B = A_i \cup \{b\}$.
Рассмотрим произвольное разбиение $X \in \XXell$. Нас интересуют только разбиения с~$\mu(B, X) = b$,
потому что вклад остальных разбиений в~вероятность переобучения не~изменился.
Пусть $a = \mu(A_i, X)$ "--- алгоритм, выбранный на~разбиении~$X$ методом обучения из~множества $A_i$.
Поскольку $\mu$ является минимизацией эмпирического риска, получим $n(b, X) \leq n(a, X)$.
Поскольку по~условию алгоритмы~$a$ и~$b$ имеют равное число ошибок на~полной выборке, уклонение частоты
$\delta(b, X) \geq \delta(a, X)$.
Следовательно, вклад каждого разбиения от~добавления алгоритма $b$ мог только увеличиться.
\end{vkProof}

%\begin{Lemma}
%Пусть~$A$ --- произвольное множество
%алгоритмов, $b$ - некоторый алгоритма, не~принадлежащий~$A$, но
%такой, что $\exists a \in A: a \leq b$, а~детерминированный метод обучения $\mu$
%является пессимистической минимизацией эмпирического риска. Тогда
%$\Q(A) \leq Q_\eps(A \cup b)$
%\end{Lemma}
%\begin{vkProof}
%    Рассмотрим множество разбиений $T(A)$ на~которых ПМЭР переобучается.
%    При добавлении в~  cемейство нового алгоритма $b$ на~части из~этих разбиений он может быть выбран методом
%    обучения. На остальных разбиениях из~$T(A)$ будут выбраны те же алгоритмы, и~эти разбиения останутся в~$T(A \cup b)$.
%
%    Рассмотрим произвольное разбиение $(X, \X) \in T(A)$, такое что
%    при минимизации эмпирического риска для множества~$A$ был выбран
%    алгоритм $c = \mu A$, и~он оказался переобучен, а~при минимизации
%    эмпирического риска для множества $A \cup b$ был выбран алгоритм $b$.
%
%    Для доказательства леммы нам достаточно показать, что на~данном разбиении $(X, \X)$ $b$ переобучается.
%    Для числа ошибок алгоритмов~$a$, $b$, $c$ на~обучающей выборке~$X$ верны следующие соотношения:
%
%    $n(c, X) \leq n(a, X)$  (иначе из~семейства~$A$ был бы выбран~$a$).
%
%    $n(a, X) \leq n(b, X)$ ($a \leq b$).
%
%    Следовательно $n(c, X) = n(b, X)$ (иначе из$A \cup b$ был бы выбран $c$).
%
%    Так как мы рассматриваем пессимистическую минимизацию эмпирического риска, то
%$n(b) >= n(c)$, если $b$ был выбран вместо $c$.
%
%    Из неравенств $n(c, X) = n(b, X)$,   $n(b) \geq n(c)$ следует что $b$ переобучается, если
%переобучается $c$.
%
%    Лемма доказана.
%\end{vkProof}

\section[Теоремы о порождающих и~запрещающих множествах (ПЗМ)]{Теоремы о порождающих и~запрещающих\\множествах (ПЗМ)}

Первый подход, позволивший получать точные оценки вероятности переобучения
в~рамках слабой вероятностной аксиоматики, основан на~выделении порождающих
и~запрещающих объектов~\cite{voron10pria-eng}.

\begin{Hypothesis}
\label{hyp:pzm}
    Пусть множество~$A$, выборка~$\XX$ и~детерминированный метод обучения~$\mu$ таковы, что
    для каждого алгоритма $a\in A$
    можно указать пару непересекающихся подмножеств
    ${X_a \subset \XX}$ и~${X'_a\subset \XX}$,
    %хотя~бы одно из~которых не~пусто,
    удовлетворяющую условию
    \begin{equation}
    \label{eq:pzm-mu-representation}
        \bigl[ \mu X{=}a \bigr]
        =
        \bigl[  X_a\subseteq  X \bigr]
        \bigl[ X'_a\subseteq \X \bigr]
        \;
        \text{ для всех } X\in \XXell.
    \end{equation}
\end{Hypothesis}

Множество~$X_a$ называется \emph{порождающим},
множество~$X'_a$~--- \emph{запрещающим} для алгоритма~$a$.
Гипотеза~\ref{hyp:pzm} означает, что
метод~$\mu$ выбирает алгоритм~$a$
тогда и~только тогда, когда в~обучающей выборке~$X$
находятся все порождающие объекты и~ни одного запрещающего.
Все остальные объекты $\XX {\setminus} X_a {\setminus} X'_a$ называются
\emph{нейтральными} для~алгоритма~$a$.

Для произвольного алгоритма $a\in A$ введем следующие обозначения:

$L_a = L - |X_a| - |X'_a|$~--- число нейтральных объектов в~генеральной выборке;

$\ell_a = \ell - |X_a|$~--- число нейтральных объектов в~обучающей выборке;

$m_a = n(a,\XX {\setminus} X_a {\setminus} X'_a)$~--- число ошибок алгоритма~$a$ на~нейтральных объектах;

$s_a(\eps) = \tfrac\ell L \bigl( n(a,\XX)-\eps k \bigr) - n(a,X_a)$~---
наибольшее число ошибок алгоритма~$a$ на~нейтральных обучающих объектах $X\setminus X_a$,
при~котором имеет место большое уклонение частот ошибок, $\delta(a,X) \geq \eps$.

Введем функцию гипергеометрического распределения:
\[
    \Hyper{L}{m}{\ell}{z}
    =
	\sum\limits_{s=0}^{\lfloor z \rfloor} \frac{\Binom{m}{s} \Binom{L-m}{\ell-s}}{\CLl}.
\]

\begin{Theorem}
\label{th:pzm}
    Если справедлива гипотеза~\ref{hyp:pzm},
    то~вероятность получить в~результате обучения алгоритм~$a$ равна
    $P_a(A) = \Prob[ \mu X{=}a ] = {C_{L_a}^{\ell_a}} / {C_{L}^{\ell}}$,
    вероятность переобучения равна
    \[
    \Q(A) =
    \sum_{a\in A}
        P_a
        \BHyper{L_a}{m_a}{\ell_a}{s_a(\eps)}.
    \]
\end{Theorem}

Данный результат позволил получить формулы вероятности переобучения
для широкого класса модельных семейств алгоритмов, в~частности
для монотонных и~унимодальных сетей.

Теорема~\ref{th:pzm} получена для детерминированных методов обучения,
для которых результатом обучения является один алгоритм ${a\in A}$.
В следующих параграфах мы приводим два важных обобщения этого результата:
во-первых, на~случай произвольного разложения множества алгоритмов на подмножества;
во-вторых, на~случай рандомизированного метода обучения.

\subsection[ПЗМ для разложения множества алгоритмов на подмножества]{ПЗМ для разложения множества алгоритмов\\на подмножества}

В данном параграфе мы обобщим метод порождающих и~запрещающих множеств~\cite{voron10pria-eng}
так, чтобы он был применим к~произвольному разложению множества алгоритмов на подмножества.

\begin{Hypothesis}
\label{hyp:pzm-cluster}
Пусть множество алгоритмов~$A$ представлено в~виде разбиения на~непересекающиеся подмножества
$A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$.
Пусть выборка $\XX$ и~метод обучения $\mu$ таковы, что для каждого $i=1,\dots,t$ можно указать пару непересекающихся подмножеств $X_i \subset \XX$ и~$X'_i \subset \XX$,
удовлетворяющую условию
\[
    [\mu(A, X) \in A_i] \leq [X_i \subset X][X'_i \subset \X]\;\text{ для всех } X \in \XXell.
\]
Пусть, кроме этого, все алгоритмы $a \in A_i$ не~допускают ошибок на~$X_i$ и~ошибаются на~всех объектах из~$X'_i$.
\end{Hypothesis}
Множество $X_i$ будем называть \emph{порождающим}, множество $X'_i$ "--- \emph{запрещающим} для $A_i$.
Гипотеза~\ref{hyp:pzm-cluster} означает, что результат обучения может принадлежать $A_i$ только в~том случае, если
в~обучающей выборке~$X$ находятся все порождающие объекты и~ни одного запрещающего.
Все остальные объекты $\YY_i \equiv \XX \backslash X_i \backslash X'_i$ будем называть \emph{нейтральными} для~$A_i$.

Пусть $L_i = L - |X_i| - |X'_i|$, $\ell_i = \ell - |X_i|$, $k_i = k - |X'_i|$.
Пусть $Q'_\eps(A_i)$ есть вероятность переобучения на~множестве нейтральных объектов $\YY_i$:
\[
    Q'_\eps(A_i) = \frac{1}{C_{L_i}^{\ell_i}} \sum_{Y \in [\YY_i]^{\ell_i}} [\delta(\mu(A_i, Y), Y) \geq \eps],
\]
где $[\YY_i]^{\ell_i}$ "--- множество разбиений $\YY_i$ на~обучающую выборку~$Y$ длины~$\ell_i$ и~контрольную выборку~$\bar Y$ длины $k_i=L_i-\ell_i$.

\begin{Theorem}
\label{th:pzm-cluster}
Пусть выполнена гипотеза~\ref{hyp:pzm-cluster},
а~на~разбиение $A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$ наложено дополнительное ограничение:
внутри каждого кластера $A_i$ все алгоритмы допускают равное число ошибок (обозначаемое через $m_i$).
Тогда вероятность переобучения $\Q(A)$ ограничена сверху следующей оценкой:
\begin{equation}
    \label{eq:pzm-cluster}
    \Q(A) \leq \sum_{i = 1}^t P_i \, Q'_{\eps_i}(A_i),
\end{equation}
где $P_i = \frac{C_{L_i}^{\ell_i}}{\CLl}$,
$\eps_i = \frac{L_i}{\ell_i k_i} \frac{\ell \, k}{L} \eps + \big(1 - \frac{\ell \, L_i}{L \ell_i}\big) \frac{m_i}{k_i} - \frac{|X'_i|}{k_i}$,
$Q'_\eps(A_i)$ "--- определенная выше вероятность переобучения на~множестве нейтральных объектов.
\end{Theorem}

\begin{vkProofShort} во многом повторяет доказательство аналогичной теоремы из~\cite{voron10pria-eng}.
Распишем определение вероятности переобучения:
\begin{align*}
    \Q(A) &    = \frac{1}{\CLl}\sum_{X \in \XXell} [\delta(\mu(A, X), X) \geq \eps] = \\
              &    = \frac{1}{\CLl} \sum_{i = 1}^t  \sum_{X \in \XXell} [\mu(A, X) \in A_i] [\delta(\mu(A_i, X), X) \geq \eps] \leq \\
              & \leq \frac{1}{\CLl} \sum_{i = 1}^t  \sum_{X \in \XXell} [X_i \subset X][X'_i \subset \X] [\delta(\mu(A_i, X), X) \geq \eps].
\end{align*}
Пусть $Y = X \backslash X_i$.
Тогда $\sum\limits_{X \in \XXell}$ при условии $[X_i \subset X][X'_i \subset \X]$
можно заменить на~суммирование по~$Y \in [\YY_i]^{\ell_i}$.
\begin{equation}
    \label{eq:pzm-cluster-proof-eq1}
    \Q(A) \leq \frac{C_{L_i}^{\ell_i}}{\CLl} \sum_{i = 1}^t \frac {1}{C_{L_i}^{\ell_i}} \sum_{Y \in [\YY_i]^{\ell_i}} [\delta(\mu(A_i, X), X) \geq \eps], \text{ где } X = Y \sqcup X_i.
\end{equation}
Выразим условие $\delta(\mu(A_i, X), X) \geq \eps$ в~терминах $Y$. Обозначим $a = \mu(A_i, X)$, и~пусть $n(a, Y) = s$.
Тогда, используя условие $n(a, X_i) = 0$ и~$n(a, X'_i) = |X'_i|$ из~гипотезы~\ref{hyp:pzm-cluster}, получим
$n(a, X) = s$, $n(a, \X) = m_i - s$, $n(a, \bar Y) = m_i - |X'_i| - s$.
Следовательно, условия переобучения для~$X$ и~$Y$ запишутся следующим образом:
\begin{align*}
    [\delta(\mu(A_i, X), X) \geq \eps]   & = \Big[s \leq \frac{\ell}L (m_i - \eps k)\Big], \\
    [\delta(\mu(A_i, Y), Y) \geq \eps_i] & = \Big[s \leq \frac{\ell_i}{L_i} \big(m_i - |X'_i| - \eps_i k_i\big)\Big].
\end{align*}
Пусть $\eps_i = \frac{L_i}{\ell_i k_i} \frac{\ell \, k}{L} \eps + \big(1 - \frac{\ell \, L_i}{L \ell_i}\big) \frac{m_i}{k_i} - \frac{|X'_i|}{k_i}$.
Непосредственной проверкой убеждаемся, что $[\delta(\mu(A_i, X), X) \geq \eps] = [\delta(\mu(A_i, Y), Y) \geq \eps_i]$.
Подставляя это в~\eqref{eq:pzm-cluster-proof-eq1}, получаем утверждение теоремы.
\end{vkProofShort}

Покажем, как для произвольного разбиения $A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$ построить систему порождающих и~запрещающих множеств.
Следуя~\cite{voron10pria-eng}, введем на~$A$ отношение частичного порядка:
$a \leq b$ тогда и~только тогда, когда $I(a, x) \leq I(b, x) \text{ для всех } x \in \XX$.
Определим $a < b$ если $a \leq b$ и~$a \neq b$.
Если $a < b$ и~при этом $\rho(a, b) = 1$, то~будем говорить, что~$a$ \emph{предшествует} $b$, и~записывать $a \prec b$.

Для отдельного алгоритма $a \in A$ порождающие и~запрещающие множества определены в~\cite{voron10pria-eng}:
\begin{equation}
    \label{eq:pzm-sets}
    \begin{split}
        X_a  & = \{x \in X \colon \exists b \in A \colon a \prec b, I(a, x) < I(b, x)\}, \\
        X'_a & = \{x \in X \colon \exists b \in A \colon b < a,     I(b, x) < I(a, x)\}.
    \end{split}
\end{equation}

Для кластера $A_i$ положим
\begin{equation}
    \label{eq:pzm-sets-cluster}
    X_i = \bigcap\limits_{a \in A_i} X_a,\;\; X'_i = \bigcap\limits_{a \in A_i} X'_a.
\end{equation}

\begin{Lemma}
    \label{th:pzm-sets-cluster}
    Множества $X_i$ и~$X'_i$, определенные в~\eqref{eq:pzm-sets-cluster}, являются, соответственно, порождающим и~запрещающим множествами для кластера $A_i$ в~смысле гипотезы~\ref{hyp:pzm-cluster}.
\end{Lemma}
\begin{vkProof}
Для произвольного разбиения $X \in \XXell$ обозначим $a = \mu X$, и~пусть $a \in A_i$.
В~\cite{voron10pria-eng} показано, что определенные в~\eqref{eq:pzm-sets} множества $X_a$ и~$X'_a$ являются порождающим и~запрещающим множествами для алгоритма~$a$,
т.~е. из~условия $\mu X = a$ следует, что $X_a \subset X$ и~$X'_a \subset \X$.
Из определения $X_i$ и~$X'_i$ следует, что $X_i \subset X_a$ и~$X'_i \subset X'_a$. Следовательно, $X_i \subset X$ и~$X'_i \subset \X$.

Условие <<все алгоритмы $a \in A_i$ не~допускают ошибок на~$X_i$ и~ошибаются на~всех объектах из~$X'_i$>> также следует из~определения $X_i$ и~$X'_i$.
\end{vkProof}

\subsection{ПЗМ для рандомизированного метода обучения}

В~случае рандомизированного МЭР результатом обучения является подмножество ${A(X) \subseteq A}$.
Таким образом, множество алгоритмов~$A$ порождает
множество подмножеств алгоритмов, получающихся в~результате обучения
\[
    \fA(A) = \bigl\{ A(X) \colon X \in \XXell \bigr\}.
\]

\begin{Hypothesis}
\label{hyp:pzm-rand}
    Пусть множество~$A$ и~выборка~$\XX$ таковы, что
    для~каждого $\alpha \in \fA(A)$
    можно указать пару непересекающихся подмножеств
    ${X_\alpha \subset \XX}$ и~${X'_\alpha\subset \XX}$,
    удовлетворяющую условию
    \begin{equation}
    \label{eq:pzm-rand-mu-representation}
        \bigl[ A(X) \!=\! \alpha \bigr]
        =
        \bigl[  X_\alpha\subseteq  X \bigr]
        \bigl[ X'_\alpha\subseteq \X \bigr]
        \;
        \text{ для всех } X\in \XXell.
    \end{equation}
\end{Hypothesis}

Следующая теорема является непосредственным обобщением теоремы~\ref{th:pzm} для РМЭР.

\begin{Theorem}
\label{th:pzm-rand}
Если справедлива гипотеза~\ref{hyp:pzm-rand},
то~вероятность переобучения РМЭР есть
\begin{equation}
    \label{eq:pzm-rand}
    \Q(A) = \sum_{a \in A} \sum_{\alpha \in \fA(A)} \frac {[a \in \alpha]}{|\alpha|}
        \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        \BHyper{L_\alpha}{m^a_\alpha}{\ell_\alpha}{s^a_\alpha(\eps)},
\end{equation}
где введены следующие обозначения:
\begin{align*}
    L_\alpha &= L - |\Xl_\alpha| - |\Xk_\alpha|;\quad
    \ell_\alpha = \ell - |\Xl_\alpha|;\\
    m^a_\alpha &= n(a, \XX \backslash \Xl_\alpha \backslash \Xl'_\alpha); \\
    s^a_{\alpha}(\eps) &= \tfrac\ell L \bigl(n(a, \XX) - \eps k\bigr) - n(a, X_\alpha).
\end{align*}
\end{Theorem}

\begin{vkProof}
Рассмотрим функционал $\Q(A)$. Введем под~знак суммирования по~$X$ два вспомогательных суммирования:
первое "--- по~всем $\alpha \in \fA(A)$ при условии $\alpha = A(X)$, второе "--- по~всем значениям
$s$ числа ошибок алгоритма~$a$ на~подвыборке $X \backslash X_\alpha$.
Очевидно, значение $\Q(A)$ от~этого не~изменится:
\begin{align}
\label{eq:pzm-rand-proof-eq1}
    & \Q(A)  = \Expect\!\!\sum_{a \in A(X)} \frac {1}{|A(\Xl)|} [\delta (a, \Xl) \geq \eps ] ={} \notag\\
%                & {}= \Expect \sum_{\alpha \in \fA(A)} \sum_{a \in A(X)}
%                      \frac {[\alpha = A(X)]}{|A(\Xl)|} [\delta (a, \Xl) \geq \eps ] \\
                 & {} = \Expect \sum_{\alpha \in \fA(A)} \sum_{a \in \alpha}
                      \frac {[\alpha = A(X)]}{|\alpha|}
                            [\delta (a, \Xl) \geq \eps] ={}\notag\\
                 & {}= \Expect \sum_{\alpha \in \fA(A)} \sum_{a \in \alpha} \sum_{s=0}^{\ell_\alpha}
                      \frac {[\alpha = A(X)]}{|\alpha|}
                            [n(a, \Xl \backslash X_{\alpha}) = s]
                            [\delta (a, \Xl) \geq \eps].
\end{align}
Число ошибок алгоритма~$a$ на~обучающей подвыборке $\Xl$ равно $s + n(a, X_\alpha)$, поэтому
отклонение частот выражается в~виде
\[
    \delta(a, \Xl) = \frac{n(a, \XX) - s - n(a, X_\alpha)}{k} - \frac{s + n(a, X_\alpha)}{\ell}.
\]
Следовательно,
\[
    [\delta(a, \Xl) \geq \eps]
        = \bigl[s \leq \frac{\ell}L (n(a, \XX) - \eps k) - n(a, X_\alpha)\bigr]
        = [s \geq s^a_\alpha(\eps)].
\]

Подставим полученное выражение в~\eqref{eq:pzm-rand-proof-eq1},
затем заменим ${[\alpha = A(X)]}$ правой частью равенства~\eqref{eq:pzm-rand-mu-representation}
и~переставим знак суммирования $\Expect \equiv \frac{1}{\CLl} \sum\limits_{X \in \XXell}$ так,
чтобы он оказался первым справа.
\begin{equation}
    \label{eq:pzm-rand-proof-eq2}
%        &\hspace{-25pt}Q_\eps(A)
 %       ={}\\
  %      {}&\hspace{-30pt}=\!\sum_{\alpha \in \fA(A)}
        Q_\eps(A) =
        \sum_{\alpha \in \fA(A)}
        \sum_{a\in \alpha}
        \sum_{s=0}^{\ell_a}
            \frac 1{|\alpha|}
            \underbrace{
                \Expect
                \bigl[  X_\alpha\subseteq  X \bigr]
                \bigl[ X'_\alpha\subseteq \X \bigr]
                \bigl[ n(a,X{\setminus} X_\alpha) = s \bigr]
            }_{N(\alpha, a)}
            \bigl[ s\leq s^a_\alpha(\eps) \bigr].
    \end{equation}
    Выделенное в~данной формуле выражение $N(\alpha, a)$ есть
    доля разбиений генеральной выборки~$\XX=X\sqcup\X$
    таких, что
    множество объектов~$X_\alpha$ целиком лежит в~$X$,
    множество объектов~$X'_\alpha$ целиком лежит в~$\X$
    и~в~подвыборку $X{\setminus} X_\alpha$ длины~$\ell_\alpha$ попадает ровно~$s$~объектов,
    на~которых алгоритм~$a$ допускает ошибку.

    Для наглядности представим вектор ошибок~$a$ разбитым на~шесть блоков:
    \[
        \vec a = \bigl(\:
            \underbrace{
                X_\alpha;
                \underbrace{
                    \overbrace{1,\ldots,1}^{s}\,;
                    0,\ldots,0
                }_{X\setminus X_\alpha}
            }_{X}\,;
            \underbrace{
                X'_\alpha;
                \underbrace{
                    \overbrace{1,\ldots,1}^{m^a_\alpha-s}\,;
                    0,\ldots,0
                }_{\X\setminus X'_\alpha}
            }_{\X}
        \;\bigr).
    \]
    Число ошибок алгоритма~$a$ на~объектах,
    не~попадающих ни~в~$X_\alpha$, ни~в~$X'_\alpha$, равно~$m^a_\alpha$.
    Существует $\Binom{m^a_\alpha}{s}$ способов выбрать из~них~$s$~объектов,
    которые попадут в~$X{\setminus} X_\alpha$.
    Для каждого из~этих способов имеется ровно $\Binom{L_a-m^a_\alpha}{\ell_\alpha-s}$ способов
    выбрать $\ell_\alpha-s$ объектов, на~которых алгоритм~$a$ не~допускает ошибку,
    и~которые также попадут в~$X{\setminus} X_\alpha$.
    Тем самым однозначно определяется состав выборки $X{\setminus} X_\alpha$,
    а~следовательно, и~состав выборки  $\X{\setminus} X'_\alpha$.
    Таким образом,
    $N(\alpha, a) = {\Binom{m^a_\alpha}{s} \Binom{L_\alpha-m^a_\alpha}{\ell_\alpha-s}} / \CLl$.
    Подставим это выражение в~\eqref{eq:pzm-rand-proof-eq2}
    и~выделим в~нем формулу гипергеометрической функции вероятности:
    \begin{align*}
        \Q(A)
        & =
        \sum_{\alpha \in \fA(A)}
        \sum_{a\in \alpha}
        \frac 1{|\alpha|}
        \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        \sum_{s=s_0}^{\ell_\alpha}
            \bigl[ s\leq s^a_\alpha(\eps) \bigr]
            \frac{\Binom{m^a_\alpha}{s} \Binom{L_\alpha - m^a_\alpha}{\ell_\alpha-s}}
                {\Binom{L_\alpha}{\ell_\alpha}} = {}\\
          & {}=
        \sum_{a \in A}
          \sum_{\alpha \in \fA(A)}
        \frac {[a \in \alpha]}{|\alpha|}
          \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
           H_{L_\alpha}^{\ell_\alpha, m^a_\alpha} \bigl(s^a_\alpha(\eps)\bigr).
    \end{align*}
    Теорема доказана.
\end{vkProof}

\begin{Corollary}
\label{th:pzm-rand-simple}
    Пусть во~множестве~$A$ найдется алгоритм~$a_0$,
    такой что для любого ${a \in A}$ вектор ошибок
    алгоритма~$a_0$ содержится в~векторе ошибок алгоритма~$a$.
    Обозначим через $X_0$ множество объектов, на~которых ошибается алгоритм~$a_0$.
    Пусть система порождающих и~запрещающих множеств такова, что для всех~$\alpha \in \fA(A)$ выполнено
    ${X_0 \cap X_\alpha = \emptyset}$ и~${X_0 \cap X'_\alpha = \emptyset}$.
    Тогда
    \[
        m^a_\alpha = n(a_0, \XX), \quad
        s^a_{\alpha}(\eps) = \tfrac{\ell}{L}\bigl(n(a, \XX) - \eps k\bigr).
    \]
\end{Corollary}
\begin{vkProof}
Зафиксируем обучающую выборку $X \in \XXell$, и~пусть $\alpha = A(X)$.
Докажем, что из~$a \in \alpha$ следует $n(a, X_\alpha) = 0$.
Пусть~$a$ ошибается на~объекте~$x$. Нам необходимо доказать, что $x \notin X_\alpha$.
Допустим обратное, тогда по~определению запрещающих объектов $x \in X_\alpha$ обязан лежать
в~обучении.
Условие $X_0 \cap X_\alpha = \emptyset$ означает, что $a_0$ не~ошибается на~$x$.
Следовательно, алгоритм~$a$ делает как минимум на~одну ошибку больше, чем $a_0$ на~обучении.
Противоречие.

Второе утверждение заключается в~том, что из~$a \in \alpha$ следует
$n(a, \XX) = n(a_0, X_0) + n(a, X'_\alpha)$.
Запишем число ошибок алгоритма~$a$ в~виде $n(a, \XX) = n(a, X_0) + n(a, X \setminus X_0)$.
Из условий теоремы следует, что $n(a, X_0) = n(a_0, X_0)$.
Следовательно, для доказательства достаточно показать, что $n(a, X \setminus X_0) = n(a, X'_\alpha)$.
Отметим, что из~условия $X_0 \cap X'_\alpha$ следует, что $X'_\alpha \subset X \setminus X_0$,
а~значит, $n(a, X \setminus X_0) \geq n(a, X'_\alpha)$. Осталось доказать, что каждая ошибка
$a \in X \setminus X_0$ алгоритма~$a$ принадлежит $X'_\alpha$.
Это следует из~того, что алгоритмы $a_0$ и~$a$ оба лежат в $A(X)$, а значит неразличимы на~обучающей выборке.

Из доказанных выше утверждений следует, что
\begin{align*}
m^a_\alpha &= n(a, \XX \backslash \Xl_\alpha \backslash \Xl'_\alpha) = n(a_0, X_0) = n(a_0, \XX);\\
s^a_{\alpha}(\eps)
    &= \frac{\ell}{L}\bigl(n(a, \XX) - \eps k\bigr) - n(a, X_\alpha)
    = \frac{\ell}{L}\bigl(n(a, \XX) - \eps k\bigr).
    \end{align*}
\end{vkProof}

\begin{Corollary}
\label{th:rand-pzm-sym-algs}
Полученная формула легко объединяется с~теоремой о~разбиении множества алгоритмов на~орбиты:
\begin{equation}
\label{eq:rand-pzm-sym-algs}
\begin{aligned}
    \Q(A) & =     \sum_{\omega \in \Omega(A)}
                        %\sum_{\substack{
                        %        \alpha \in \fA(A) \colon\\
                        %        a_\omega \in \alpha
                        %    }}
                        \sum_{\alpha \in \fA(A)}
                            [a_\omega \in \alpha]
                            \frac {|\omega|}{|\alpha|}
                              \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
                            H_{L_\alpha}^{\ell_\alpha, m^{a_\omega}_\alpha} \bigl(s^{a_\omega}_\alpha(\eps)\bigr).
\end{aligned}
\end{equation}
\end{Corollary}

\begin{vkProof}
Доказательство немедленно следует из~леммы~\ref{th:equalPQa} о~равном вкладе
алгоритмов одной орбиты в~вероятность переобучения.
\end{vkProof}

%\section{*Слабое замыкание множества алгоритмов}
%В данном параграфе приводится один из~возможных вариантов обобщения оценки расслоения-связности на~случай несвязного семейства алгоритмов.
%Для этого нам потребуется несколько новых определений.
%
%\begin{Definition}
%\emph{Диаграммой Хассе} множества алгоритмов~$A$ называется ориентированный граф транзитивной редукции отношения $<$ частичного порядка на~алгоритмах.
%\end{Definition}
%
%\ToDo{Пример: разреженная двумерная монотонная сеть.}
%
%Легко проверить, для разреженной монотонной сети $\ddot A$ ребра диаграммы Хасса $E = \{(a, b) \colon a < b \text { и~} \rho(a, b) = \kappa\}$ соединяют те и~только те пары алгоритмов, что находятся друг от~друга на~хэмминговом расстоянии $\kappa$, и~идут в~сторону увеличения числа ошибок алгоритмов на~полной выборке.
%
%В отличии от~графа расслоения-связности, каждому ребру $(a, b)$ диаграммы Хасса соответствует не~один, а~множество алгоритмов $X_{a b} = \{x \in \XX \colon I(a, x) = 0 \text { и~} I(b, x) = 1\}$. Это позволяет обобщить понятия неполноценности в~терминах диаграммы Хасса.
%
%\begin{Definition}[Неполноценность алгоритма]
%Обозначим через $X_q(a) = \bigcup_{b \leq a}X_{b a}$ множество объектов, соответствующих всевозможным ребрам диаграммы Хассе на~всевозможных путях, ведущих к~вершине~$a$. \emph{Неполноценностью $q(a) = |X_q(a)|$} алгоритма $a \in A$ будем называть мощность множества $X_q(a)$.
%\end{Definition}
%
%Обозначим через $I_a = \{b \in A \colon (a, b) \in E\}$ множество концов ребер, исходящих из~$a$.
%через $I^a = \{b \in A \colon (a, b) \in E\}$ "--- множество алгоритмов, из~которых в~$a$ ведет ребро. Число исходящих из~$a$ ребер обозначим через $u(a) = |I_a|$.
%
%\begin{Lemma}
%\label{lem:randomizedlearn}
%Пусть~$A$ "--- произвольное множество алгоритмов, а
%метод обучения $\mu$ является пессимистической минимизацией эмпирического риска.
%Тогда необходимое условие получения произвольного алгоритма $a \in A$ в~результате обучения
%записывается в~терминах графа Хасса следующим образом:
%\begin{equation}
%    \label{randomizedlearn}
%    \bigl[ \mu X{=}a \bigr]
%    \leq
%    \bigl[ X_u(a)\subseteq  \X \bigr]
%    \prod_{b \in I_a} [X_{a b} \cap \Xl \neq \emptyset]
%    \;
%    \text{ для всех } X\in \XXell.
%\end{equation}
%\end{Lemma}
%\begin{vkProof}
%Это и~последующие доказательства этого параграфа вынесены в~упражнения.
%\end{vkProof}
%
%Это условие означает следующее: для того, чтобы алгоритм~$a$ был выбран методом обучения, необходимо и~достаточно, чтобы множество $X_u(a)$ целиком содержалось в~контроле, а~для каждого $b \in I_a$ хотя бы один объект из~$X_{a b}$ попал в~обучение. В частном случае разреженной монотонной сети~\eqref{randomizedlearn} обращается в~равенство.
%
%Обратим внимание, что в~случае разреженной монотонной сети множества $\{X_{a b} \colon b \in I_a\}$, соответствующие различным $b$, попарно не~пересекаются. Это важное свойство позволяет записать условие~\eqref{randomizedlearn} на~языке порождающих и~запрещающих объектов.
%
%Зафиксируем алгоритм $a \in A$, и~пронумеруем элементы множества $I_a$ произвольным способом: $I_a = \{b_1, \dots, b_{u(a)}\}$. Для каждого $b_i \in I_a$ пронумеруем элементы множества $X_{a b_i} = \{x_{i1}, \dots, x_{i\kappa}\}$ (тоже произвольным способом).
%Возьмем $V_a = \prod\limits_{b \in I_a} X_{a b}$ в~качестве индексного множества,
%фигурирующего в~гипотезе о~порождающих и~запрещающих объектах.
%Положим все $c_{a v} = 1$.
%Элементы $v \in V_a$ будем записывать в~виде вектора чисел:
%$v = (v_1, \dots, v_{u(a)})$,
%где все $v_i = 1, \dots, |X_{b_i}|$.
%Определим систему порождающих и~запрещающих множеств следующим образом:
%\begin{equation}
%\label{eq:weakClosurePzmFormulas}
%\begin{aligned}
%    \bar X_{a v} &= \{x_{ij} \colon i = 1, \dots, u(a), j = 1, \dots, (v_i - 1)\} \cup X_q(a), \\
%    X_{a v} &= \{x_{i j} \colon i = 1, \dots, u(a), j = v_i\}.
%\end{aligned}
%\end{equation}
%
%\begin{Lemma}
%\label{lem13}
%Пусть метод $\mu$ является пессимистической минимизацией эмпирического риска,
%а~множество алгоритмов~$A$ таково, что для каждого $a \in A$
%множества $\{X_{a b} \colon b \in I_a\}$, соответствующие различным $b$, попарно не~пересекаются.
%Тогда определенная выше система порождающих и~запрещающих множеств дает необходимое условие
%получение алгоритмов $a \in A$ в~результате обучения:
%\begin{align*}
%    \bigl[ \mu X{=}a \bigr]
%    & \leq
%    \sum_{v \in V_a}
%    \bigl[  X_{a v}\subseteq  X \bigr]
%    \bigl[ X'_{a v}\subseteq \X \bigr]
%    \;
%    \text{ для всех } X\in \XXell.
%\end{align*}
%\end{Lemma}
%
%Условие о непересекающихся множествах $X_{a b}$ может нарушаться для произвольного множества алгоритмов~$A$.
%Тем не~менее, оказывается что произвольное множество алгоритмов~$A$ можно пополнить до объемлющего множества $B$ таким образом, что
%для $B$ условие о непересекающихся множествах оказывается выполнено,
%а вероятность переобучения пессимистического метода обучения лишь увеличилась: $Q_\eps(A) \leq Q_\eps(B)$.
%
%Для алгоритма~$a$ обозначим через $\XX_{[a]} \subset \XX$ множество ошибок алгоритма~$a$ на~выборке $\XX$.
%Рассмотрим произвольное множество алгоритмов~$A$ и~его диаграмму Хассе $(A, E)$ "--- граф транзитивной редукции отношения $<$,
%определенного на~парах алгоритмов условием <<$a\leq b$ если $\XX_{[a]} \subset \XX_{[b]}$>>.
%Напомним также, что через $I_a = \{b \in A \colon (a, b) \in E\}$ обозначалось множество концов ребер, исходящих из~$a$, через $u(a) = |I_a|$ "--- количество таких ребер, через
%$X_{a b} = \XX_{[b]} \backslash \XX_{[a]}$, где $a < b$ "--- множество объектов, соответствующих ребру $(a, b)$.
%По аналогии с~$I_a$ и~$I^a$ обозначим через $I_a^\infty$ множество алгоритмов $b$, таких что существует путь из~$a$ в~$b$, проходящий по~ребрам графа Хассе; через $I^a_\infty$ "--- множество алгоритмов $b$, таких что существует путь из~$b$ в~$a$. Для любой пары алгоритмов $a, b \in A$ определим алгоритм $a \cap b$ условием $\XX_{[a \cap b]} = \XX_{[a]} \cap \XX_{[b]}$.
%
%Будем говорить, что множество алгоритмов~$A$ является \emph{слабо замкнутым}, если для любого $a \in A$ множества $\{X_{a b} \colon b \in I_a\}$ попарно не~пересекаются. Наша задача "--- дополнить произвольное множество~$A$ алгоритмами до $A^{*}$, такого что $A \subset A^*$, и~$A^*$ "--- слабо замкнуто. Очевидно, что такое множество существует "--- достаточно взять $A^* = \{0, 1\}^L$. Следует отметить, что в~данном случае неверно говорить о наименьшем по~включению множестве. Действительно, легко построить пример двух слабо замкнутых множеств $A_1, A_2$, пересечение которых $A_1 \cap A_2$ уже не~является слабо замкнутым. Именно этим неприятным свойством и~объясняется термин \emph{слабая} замкнутость.
%
%Множество алгоритмов~$A$ назовем \emph{замкнутым}, если для любого $a \in A$, и~для любой пары $b_1, b_2 \in A$, такой что $a < b_1, a < b_2$, выполнено $(b_1 \cap b_2) \in A$. Легко показать, для любой пары замкнутых множеств их пересечение вновь является замкнутым. Это позволяет определить замыкание множества алгоритмов $\bar A$, как наименьшее по~включению замкнутое множество, содержащее~$A$.
%
%Утверждается, что из~замкнутости следует слабая замкнутость. В дальнейшем для произвольного множества~$A$ мы будем определять его слабое замыкание $A^* \subset \bar A$ с~помощью описанной ниже алгоритмической процедуры.
%
%\begin{algorithm}
%\label{weakClosureAlg}
%\caption{Слабое замыкание множества алгоритмов}
%\begin{algorithmic}[1]
%\REQUIRE множество алгоритмов~$A$, множество ребер графа Хассе $E$;
%\ENSURE слабое замыкание $A^*$, новое множество ребер графа Хассе $E^*$;
%
%\STATE Сгенерировать очередь заданий на~обработку:
%
%$Q := \{(a, b_1, b_2) \colon (a, b_1) \in E, (a,b_2) \in E\}$;
%\WHILE{$Q \neq \emptyset$} \label{whileQ}
%    \STATE $(a, b_1, b_2) := $ взять следующее задание из~очереди $Q$;
%    \LINEIF{$(a, b_1) \not\in E$ или $(a, b_2) \not\in E$}{\GOTO{whileQ}}
%    \STATE $c := b_1 \cap b_2$;
%    \LINEIF{$c \in A$}{\GOTO{whileQ}}
%    \STATE $A := A \cup \{c\}$;
%    \STATE Найти множества $I^{a}_\infty, I^{b_1}_\infty, I^{b_2}_\infty, I_{a}^\infty, I_{b_1}^\infty, I_{b_2}^\infty$ для текущей пары $(A, E)$.
%    \STATE $I^c$ := Т-Редукция $((I^{b_1}_\infty \cap I^{b_2}_\infty) \backslash I^{a}_\infty, <, \{a\})$;
%    \STATE $I_c$ := Т-Редукция $(I_{a}^\infty \backslash (I_{b_1}^\infty \cup I_{b_2}^\infty), >, \{b_1, b_2\})$;
%    \STATE $E := E \cup \{(x, c) \colon x \in I^c)\} \cup \{(c, y) \colon y \in I_c)\}$;
%    \FORALL{$x \in I^c$, $y \in I_c$}
%        \IF{$(x, y) \in E$}
%            \STATE $E := E \backslash (x, y)$;
%        \ENDIF
%    \ENDFOR
%    \FORALL{$\{x, y\} \subset I_c$}
%        \STATE $Q = Q \cup \{(c, x, y)\}$;
%    \ENDFOR
%    \FORALL{$x \in I^c$}
%        \FORALL{$y \in I_x$}
%            \IF{$y \neq c$}
%                \STATE $Q = Q \cup \{(x, c, y)\}$;
%            \ENDIF
%        \ENDFOR
%    \ENDFOR
%\ENDWHILE
%\STATE Положить $A^* := A$, $E^* := E$.
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}
%\caption{Т-Редукция($Q, \phi, R$)}
%\begin{algorithmic}[1]
%\REQUIRE очередь кандидатов $Q$, предикат $\phi$ на~парах из~$Q$, начальное приближение $R$;
%\ENSURE множество $\bar R \subset R \cup Q$, транзитивно-замкнутое относительно $\phi$;
%\WHILE{$Q \neq \emptyset$} \label{whileQ2}
%    \STATE $x := $ взять следующее задание из~очереди $Q$;
%    \STATE $D := \emptyset$;
%    \FORALL{$y \in R$}
%        \LINEIF{$\phi(y, x)$}{\GOTO{whileQ2}}
%        \LINEIF{$\phi(x, y)$}{$D := D \cup \{y\}$;}
%    \ENDFOR
%    \STATE $R := (R \backslash D) \cup \{x\}$;
%\ENDWHILE
%\RETURN $R$
%\end{algorithmic}
%\end{algorithm}
%
%В алгоритме построения слабого замыкания (\ref{weakClosureAlg}) на
%каждом шаге в~семейство добавляется алгоритм $b$ c непустым
%$X_q(b)$. Поэтому, согласно лемме~\ref{lem:boundrelation}, при построении
%слабого замыкания семейства, на~каждом шаге вероятность
%переобучения только увеличивается, следовательно $Q_\eps(A^*) \geq Q_\eps(A)$.
%
%Таким образом, предлагается следующая процедура для оценки вероятности переобучения в~несвязных семействах алгоритмах:
%к исходному множеству алгоритмов~$A$ применяется процедура слабого замыкания, и~к получившемуся множеству $A*$
%применяется оценка на~основе порождающих и~запрещающих множеству, выбранных в~соответствии с~\eqref{eq:weakClosurePzmFormulas}.
%
%Рассмотрим алгоритм $a \in A$ и~систему множеств $\{X_{ab} \colon b \in I_a \}$, состоящую из~наборов объектов, соответствующих выходящих из~$a$ ребрам диаграммы Хасса. Пронумеруем элементы множества $I_a$ произвольным способом: $I_a = \{b_1, \dots, b_{u(a)}\}$, и~рассмотрим вектор $w_a = (|X_{a b_i}|)_{i = 1}^{u(a)}$. Напомним, что под модулем вектора $|w_a|$ мы понимаем сумму его координат. Рассмотрим также вектор $1_a = (1, \dots, 1)$, той же размерности что и~$w_a$, но заполненный единицам.
%Рассмотрим функцию $S_a(u)$, определенную для всех $u$ из~$|1_a|, \dots, |w_a|$ выражением $S_a(u) = |\{w \in \ZZ^{u(a)} \colon |w| = u, 1_a \leq w \leq w_a\}|$. Значение $S_a(u)$ соответствует количеству векторов с~целочисленными координатами, ограниченных снизу вектором $1_a$, сверху - $w_a$, и~с суммой координат $u$.
%
%\begin{Theorem}
%\label{theorem5eq}
%Пусть~$A$ "--- произвольное множество алгоритмов, $A^*$ "--- его слабое замыкание.
%Тогда справедлива следующая оценка вероятности переобучения $Q_\eps(A^*)$ и~среднего значения числа ошибок на~контроле $C(A^*)$:
%\begin{align*}
%    Q_\eps(A^*) &\leq
%        \sum_{a \in A}
%        \sum_{u = |1_a|}^{|w_a|}
%            S_a(u)
%            \frac{\Binom{L_a - u}{\ell_a}}{\CLl}
%            \Hyper{L_a - u}{m_a}{\ell_a}{s_a(\eps)}, \\
%    C(A^*) &\leq
%        \sum_{a \in A}
%        \sum_{u = |1_a|}^{|w_a|}
%            S_a(u)
%            \frac{\Binom{L_a - u}{\ell_a}}{\CLl}
%            \left(n(a, \XX) - \frac{\ell_a}{L_a - u}m_a\right), \\
%\end{align*}
%где введены следующие обозначения:
%$L_a = L - q(a)$,
%$\ell_a = \ell - u(a)$,
%$m_a = n(a, \XX) - q(a)$,
%$s_a(\eps) = \frac{\ell}{L} \left(n(a, \XX) - \eps k\right)$,
%$q(a)$ "--- неполноценность алгоритма~$a$, определенная в~терминах диаграммы Хассе.
%\end{Theorem}
%\begin{vkProof}
%Напомним, что согласно лемме~\ref{lem13} множества порождающих и~запрещающих объектов можно выбрать следующим способом:
%\begin{align*}
%    \bar X_{a v} &= \{x_{ij} \colon i = 1, \dots, u(a), j = 1, \dots, (v_i - 1)\} \cup X_q(a), \\
%    X_{a v} &= \{x_{i j} \colon i = 1, \dots, u(a), j = v_i\}.
%\end{align*}
%Следовательно, мощности множеств $\bar X_{a v}$ и~$X_{a v}$ выражаются следующим способом:
%$
%    |\bar X_{a v}| = |v| - u(a) + q(a),
%    |X_{a v}| = u(a).
%$
%Тогда, в~обозначениях теоремы~\ref{th1}, получим:
%\begin{align*}
%    L_{a v} &= L - |v| - q(a) = L_a - |v|, \text{ где } L_a \equiv L - q(a), \\
%    \ell_{a v} &= \ell - u(a) \equiv \ell_a, \\
%    m_{a v} &= n(a, \XX) - q(a) \equiv m_a, \\
%    s_{a v} &= \frac{\ell}L (n(a, \XX) - \eps k) - 0 \equiv s_a,
%\end{align*}
%где выражения $L_a, \ell_a, m_a, s_a$ не~зависят от~$v$.
%Это позволяет записать сумму вида $\sum\limits_{v \in V_a}f(|v|)$ по~декартовому произведению $V_a = \prod\limits_{b \in I_a} X_{a b}$ следующим образом:
%\[
%    \sum\limits_{v \in V_a}f(|v|) = \sum\limits_{u = |u(a)|}^{|w_a|}s_a(u)f(u),
%\]
%где $s_a(u)$ "--- определенный выше коэффициент.
%\end{vkProof}
%
%\begin{figure}[t]
%\begin{center}
%    \includegraphics[width=72mm,height=48mm]{Pictures/Comparison_EchocardRSA.eps}
%    \caption{Сравнение 0.5-квантили различных оценок вероятности переобучения,
%    задача Echo Cardiogram. %Пунктирная кривая
%    %на левом рисунке соответствует семейству, полученному случайным перемешиванием
%    %векторов ошибок в~$A^*_r$.
%    }
%\end{center}
%    \label{fig3}
%\end{figure}
%
%\ToDo{Описать детали эксперимента из~th frei pria3 txt}
%
%\ToDo{убрать лишние кривые с~рисунка, объяснить суть: слабое замыкание незначительно завышает вероятность переобучения, но зато существенно улучшает оценку в~которой не~учитывается верхняя связность. Основной вывод: верхняя связность это важно!}

%\section{*Другие рандомизации}
%
%В данном параграфе рассматриваются другие рандомизации.
%
%\subsection{Рандомизация векторов ошибок}
%
%В~одном из~предыдущих параграфов была получена
%формула для вероятности переобучения сечения шара центральным слоем.
%Такие семейства являются наиболее <<плотными>> множествами алгоритмов, допускающими равное число ошибок.
%Ниже рассматривается другое подмножество слоя, в~котором алгоритмы лишены сходства.
%Показывается, что вероятность переобучения в~данном случае экспоненциально стремиться к~единице
%с ростом числа алгоритмов.
%
%Отметим, что задача построения множества алгоритмов фиксированной мощности с~ограничением
%на минимальное попарное расстояние между алгоритмами хорошо изучена в~теории кодов
%исправляющих ошибки. Однако методы построения кодов исправляющих ошибки накладывают существенные
%ограничения на~параметры задачи: количество объектов в~полной выборке и~минимальные расстояния между алгоритмами.
%Предлагаемый ниже подход позволит обойти задачу порождения несвязного множества алгоритмов заданной мощности.
%
%%\REVIEWNOTE{Убрать слово "расстопыренный" из~предыдущего предложения. Например, заменить все предложение на~"Предлагаемый ниже подход позволит обойти задачу порождения несвязного множества %алгоритмов заданной мощности."}
%
%Рассмотрим \textbf{упорядоченное} множество (т.\,е. вектор) алгоритмов $A = (a_1, \ldots, a_d ) \subset \AA$,\: $d = |A|$,
%и~группу $G = (S_L)^{d}$, состоящую из~$(L!)^{d}$ элементов.
%Группа $G$ действует на~множество~$A$,
%всевозможными способами переставляя ошибки разных алгоритмов независимо друг от~друга.
%
%Для формального определения действия $G$ на~$A$ рассмотрим элемент группы
%$g = (\pi_1, \ldots, \pi_d) \in G$,
%где все $\pi_j \in S_L$.
%Он действует на~множество~$A$ по~правилу
%$g A = ( \pi_1 a_1, \ldots, \pi_d a_d )$,
%где действие элементов группы $S_L$ на~$a \in \AA$
%определено перестановками объектов выборки так же, как и~раньше.
%
%Определим усредненный функционал вероятности переобучения:
%\[
%    \big\langle Q_\eps(A) \big\rangle_G
%        = \frac 1{|G|} \sum_{g \in G} Q_\eps(g A).
%\]
%На~самом деле, $\big\langle Q_\mu(\epsilon, A) \big\rangle_G$ зависит уже не~от
%самого множества алгоритмов~$A$, а~только от~его профиля расслоения.
%Рассмотрим в~качестве примера подмножество~$m$-слоя, состоящее из~$d$ алгоритмов.
%
%\begin{Theorem}
%Пусть~$A$ "--- произвольное множество из~$D$ попарно"=различных алгоритмов,
%каждый из~которых допускает~$m$~ошибок на~полной выборке.
%Тогда
%\begin{equation}
%    \label{eq:scateredSlice}
%    \big\langle Q_\varepsilon(A) \big\rangle_G = 1 - \left( 1 - \Hyper{L}{m}{\ell}{s(\eps)} \right)^D\!\!\!,
%\end{equation}
%где
%$s(\eps) = \tfrac\ell L(m-\eps k)$.
%\end{Theorem}
%\begin{vkProof}
%Переставим знаки суммирования по~$X \in \XXell$ и~по $g \in G$ в~функционале $\big\langle Q_\eps(A) \big\rangle_G$:
%\[
%    \big\langle Q_\varepsilon(A) \big\rangle_G
%        = \frac 1{|G|} \frac1{\CLl}
%                \sum_{\Xl \in \XXell}
%                \sum_{g \in G}\;
%                 \sum \limits_{a\in (gA)(\Xl)}
%                    \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}
%                            {|(gA)(\Xl)|}.
%\]
%
%Заметим, все слагаемые под знаком усреднения $\frac 1{\CLl} \sum \nolimits_{\Xl \in \XXell}$ равны друг другу.
%Поэтому, выбрав произвольного представителя $\Xl \in \XXell$, запишем
%\begin{equation}
%\label{rndSlice1}
%    \big\langle Q_\varepsilon(A) \big\rangle_G
%        = \frac 1{|G|}
%                \sum_{g \in G}\;
%                 \sum \limits_{a\in (gA)(\Xl)}
%                    \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}
%                            {|(gA)(\Xl)|}.
%\end{equation}
%
%Введем обозначение $s(\eps) = \frac \ell L(m - \epsilon k)$ и~докажем, что для любого множества
%$A$, такого что все $a \in A$ имеют~$m$ ошибок на~полной выборке, выполнено:
%\begin{equation}
%\label{oneslice1eq}
%    \sum \limits_{a\in A(\Xl)}
%        \frac {\bigl[ \delta(a, \Xl) \geq \epsilon \bigr]}
%            {|A(\Xl)|} =
%    \left[\min_{a \in A} n(a, \Xl) \leq s(\eps) \right].
%\end{equation}
%Действительно, пусть $m_\ell = \min\limits_{a \in A} n(a, \Xl)$. Тогда все алгоритмы из~
%$A(X)$ имеют по~$m_\ell$ ошибок на~обучении и~по~$m_k = m - m_\ell$ ошибок на~контроле.
%Следовательно, все $a \in A(X)$ имеют одинаковую переобученность
%$\delta(a, X) = \frac{m_k}k - \frac{m_\ell}{\ell}$.
%Отсюда немедленно получим $[\delta(a, X) \geq \eps] = [m_\ell \leq s(\eps)]$.
%
%Подставляя~\eqref{oneslice1eq} в~\eqref{rndSlice1}, получим:
%\[
%\begin{aligned}
%    \big\langle Q_\eps(A) \big\rangle_G
%    &= \frac 1{|G|} \sum_{g \in G} \left[\min_{a \in gA} n(a, \Xl) \leq s(\eps) \right] = \\
%    &= \frac 1{|G|} \sum_{g \in G} \Big[\exists a \in g A, n(a, \Xl) \leq s(\eps) \Big] = \\
%    &= 1 - \frac 1{|G|} \sum_{g \in G} \Big[\forall a \in g A, n(a, \Xl) > s(\eps) \Big] = \\
%   &= 1 - \frac 1{|G|} \sum_{g \in G} \prod_{i = 1}^{D} \Big[n(\pi_i a_i, X) > s(\eps) \Big].
%\end{aligned}
%\]
%Пользуясь тем, что группа~$G$ "--- декартово произведение групп, получим
%\[
%     \big\langle Q_\eps(A) \big\rangle_G
%   = 1 - \prod_{i = 1}^{D} \frac 1{|S_L|} \sum_{\pi_i \in S_L} \Big[n(\pi_i\, a_i, X) > s(\eps) \Big].
%\]
%
%Отметим, что все множители произведения $\prod \nolimits_{i = 1}^{D}$ равны друг другу.
%Зафиксируем произвольный $a \in A$, и~пользуясь условием $n(\pi a, X) = n(a, \pi^{-1}) X$,
%перепишем предыдущее выражение в~виде
%\[
%     \big\langle Q_\eps(A) \big\rangle_G
%   = 1 - \Big(1 -
%   \underbrace{\frac 1{|S_L|} \sum_{\pi' \in S_L}
%   \big[n(a, \pi' X) \leq s(\eps) \big]}_{Q_\eps(a)} \Big)^D\!\!\!,
%\]
%где~$a$ "--- произвольный алгоритм с~$m$ ошибками на~полной выборке.
%
%Нетрудно заметить, что выделенное в~предыдущей формуле выражение
%$Q_\eps(a)$ равно $\BHyper{L}{m}{\ell}{s(\eps)}$ "--- вероятности переобучения
%вырожденного метода обучения, всегда возвращающего алгоритм~$a$.
%\end{vkProof}
%
%\begin{Remark}
%С увеличением числа алгоритмов в~слое
%величина $\left(1 - \BHyper{L}{m}{\ell}{s(\eps)} \right)^D$ стремиться к~нулю,
%а, следовательно, вероятность переобучения стремится к~единице.
%\end{Remark}
%
%\begin{figure}[h]
%    \label{fig:scateredSlice}
%    \begin{centering}
%    \includegraphics[height=80mm]{Pictures/randomSlices.eps}
%    \caption{Рассеянный слой алгоритмов.}
%    \end{centering}
%\end{figure}
%
%На~рисунке приведены результаты численного эксперимента,
%подтверждающего полученную выше формулу~\eqref{eq:scateredSlice}.
%По оси $OX$ откладывается число алгоритмов,
%случайным образом выбранных из~слоя с~$m = 2$ ошибками.
%По оси $OY$ откладывается разность между единицей и~вероятностью переобучения
%для~$\epsilon = 0{.}04$,\: $L=100$,\: $\ell = 60$.
%Гладкая кривая получена вычислением по~формуле~\eqref{eq:scateredSlice},
%тонкая кривая получена для одного случайно сгенерированного множества алгоритмов
%методом Монте-Карло по~50~тыс. разбиений.
%
%%\REVIEWNOTE{И что\dots? Этот раздел имел какую-то цель "--- Ведь так? Надо что-то сказать, что нам этот факт дал "--- ведь не~просто же это была демонстрация <<хитрого>> метода? Я~бы заключил так: мы 1)в~который раз убедились в~необходимости связности для~обучаемости классификаторов "--- бессвязные множества не~просто переобучаются, а~ЭКСПОНЕНЦИАЛЬНО быстро сносят крышу; 2) мы предложили некий новый механизм дополнительной рандомизации "--- в~очередной раз спасающей нам жизнь... Так ведь?}
%%\REVIEWNOTE{Начало следующего параграфа вполне отвечает на~этот вопрос.}
%
%%\begin{task}
%%Обобщить, теоретически или по~крайней мере экспериментально, формулу~\eqref{eq:scateredSlice}
%%на~случай произвольного профиля расслоения.
%%\end{task}
%
%%\subsection{Случайные подмножества алгоритмов}
%%
%%Для дальнейшего уточнения оценки мы будем рассматривать кластер $A_i$ как случайное подмножество некоторого объемлющего множества $B$
%%(например, центрального слоя хэммингова шара или локальной окрестности).
%%
%%\begin{Def}
%%\emph{Средней вероятностью переобучения по~подмножествам фиксированной мощности} назовем следующую величину:
%%\begin{equation}
%%    \label{eq:randomizedSubsetOverfitting}
%%    \bar{Q}_{\eps}(B, d) = \frac{1}{C_{|B|}^d} \sum_{A' \in [B]^d} \Q(A'),
%%\end{equation}
%%где $[B]^d = \{A' \subset B \colon |A'| = d\,\}$ "--- система подмножеств фиксированной мощности.
%%\end{Def}
%%На рис.~\ref{figBallSliceVsLocalVicinity} справа показан пример зависимости средней вероятности переобучения $\bar{Q}_{\eps}(B, d)$ от~параметра $d$ для двух рассматриваемых нами семейств алгоритмов.
%%
%%\begin{Theorem}
%%\label{thTolstR2}
%%Пусть $B$ "--- множество алгоритмов, допускающих равное число ошибок на~полной выборке.
%%Тогда выполнено следующее:
%%\begin{equation}
%%    \label{eq_thTolstR2}
%%    \bar{Q}_{\eps}(B, d) = 1 -
%%        \frac 1{\CLl}
%%            \sum\limits_{\tau \in \Omega(\XXell)}
%%                |\tau|\frac{C^d_{N_\eps(B, X_\tau)}}{C^d_{|B|}},
%%\end{equation}
%%где
%%    $\Omega(\XXell)$ "--- множество орбит действия группы симметрий~$A$ на~$\XXell$,
%%    $X_\tau$ "--- произвольный представитель орбиты $\tau \in \Omega(\XXell)$,
%%    $N_\eps(B, X) = \sum\limits_{a\in B}[n(a,X) > s(\eps)]$ "--- число алгоритмов из~$B$, непереобученных на~разбиении~$X$,\:
%%    $s(\eps)=\frac{\ell}{L}(m - \eps k)$.
%%\end{Theorem}
%%
%%\begin{vkProof}
%%Запишем определение $\bar{Q}_\eps(B, d)$ и~воспользуемся тем,~что $A' \subset B$ вновь является подмножеством слоя:
%%\[
%%\bar{Q}_{\eps}(B, d) =
%%    \frac{1}{C^d_{|B|}}\sum_{A' \in [B]^d}
%%    \frac{1}{C^{\ell}_L}
%%    \sum_{X\in \XXell}
%%    \left[\exists a \in A' \colon n(a, X) \leq s(\eps)\right].
%%\]
%%Переставим местами знаки суммирования и~применим логическое отрицание:
%%\[
%%\bar{Q}_{\eps}(B, d) =
%%    1 -
%%    \frac 1 \CLl
%%    \frac{1}{C^d_{|B|}}
%%    \sum_{X\in \XXell}
%%    \underbrace{\sum_{A' \in [B]^d}
%%    \left[\forall {a \in A'} \colon n(a, X) > s(\eps)\right]}_{F_\eps(B, \, X)}.
%%\]
%%Заметим, что выделенное в~прошлой формуле выражение $F_\eps(B, X)$ соответствует числу способов
%%выбрать из~$B$ подмножество $A'$ мощности $d$ так, чтобы ни один из~алгоритмов в~$A'$ не~был переобученным.
%%Обозначим через $N_\eps(B, X)$ общее число алгоритмов в~$B$, непереобученных на~разбиении~$X$.
%%Тогда $F_\eps(B, X)$ является числом сочетаний из~$N_\eps(B, X)$ по~$d$:
%%\[
%%\bar{Q}_{\eps}(B, d) =
%%    1 - \frac 1{\CLl} \frac 1{C^d_{|B|}}\sum\limits_{X \in \XXell} C^d_{N_\eps(B, X)}.
%%\]
%%По теоремам~\ref{th:sym1} и~\ref{th:sym2} функция $C^d_{N_\eps(B, X)}$, где $N_\eps(B, X) = \sum\limits_{a \in B}[n(a, X) > \eps]$, является симметричной функцией третьего рода,
%%и, следовательно,~\eqref{eq_thTolstR2} факторизуется по~действию группы симметрий на~множестве разбиений:
%%\[
%%\bar{Q}_{\eps}(B, d) =
%%    1 -
%%    \frac{1}{C^{\ell}_L}
%%    \frac{1}{C^d_{|B|}}
%%    \sum_{\tau\in \Omega(\XXell)}
%%    |\tau| C_{N_\eps(B, X_\tau)}^{d}.
%%\]
%%\end{vkProof}
%%
%%При больших значениях параметра $d$ дробь $\frac{C^d_{N_\eps(B, X)}}{C_{|B|}^d}$ приближенно равна $\Big(\frac{N_\eps(B, X_\tau)}{|B|}\Big)^{d}$.
%%Таким образом, вклад разбиения $(X, \bar X)$ в~вероятность переобучения
%%полностью определяется мощностью $d$ рассматриваемых подмножеств и~числом алгоритмов из~$B$, непереобученных на~разбиении $(X, \bar X)$.
%%
%%Покажем,~как вычислять оценку~\eqref{eq_thTolstR2} на~примере локальной окрестности $\hat B_{r,\rho}^{m}$.
%%
%%\begin{Theorem}
%%\label{th:interval-layer-subset}
%%Средняя вероятность переобучения случайного подмножества $A_i \subset \hat B_{r,\rho}^{m}$ фиксированной мощности $d$ дается следующей формулой:
%%\begin{equation}
%%\label{eq:interval-layer-subset}
%%    \bar{Q}_{\eps}(\hat B_{r,\rho}^{m},d) = 1 -
%%        \frac 1{\CLl}
%%            \sum_{i = 0}^{\min(m, \ell)} \sum_{j = 0}^{\min(r, \ell - i)}
%%                C_m^i C_r^j C_{L - m - r}^{\ell - i - j} \frac{C^d_{N_{i, j}}}{C^d_{|\hat B_{r,\rho}^{m}|}},
%%\end{equation}
%%где
%%\[
%%    N_{i, j} = \sum_{t = 0}^{\min(j, \rho)} C_{j}^{t} C_{r - j}^{\rho - t} [i + t > s(\eps)].
%%\]
%%\end{Theorem}
%%
%%\begin{vkProof}
%%Рассмотрим три симметрические группы перестановок $S_m$, $S_r$  и~$S_{L - m - r}$, действующие на~множествах $X_1$, $X_r$ и~$X_0$, соответственно.
%%Группой симметрий множества алгоритмов $\hat B_{r,\rho}^{m}$ является декартово произведение $S_m \times S_r \times S_{L - m - r}$.
%%Орбиты действия $\Sym(\hat B_{r,\rho}^{m})$ на~$\XXell$ индексируются двумя параметрами, $i = |X \cap X_1|$ и~$j = |X \cap X_r|$, где~$X$ "--- обучающая выборка.
%%Мощность орбиты $\tau_{i, j}$ дается, соответственно, выражением $|\tau_{i, j}| = C_m^i C_r^j C_{L - m - r}^{\ell - i - j}$.
%%
%%Разобравшись с~симметриями, необходимо для представителя $X_{i, j} \in \tau_{i, j}$ вычислить величину $N_{i, j} = N(\hat B_{r,\rho}^{m}, X_{i, j})$ "---
%%количество алгоритмов из~$\hat B_{r,\rho}^{m}$, непереобученных на~разбиении $X_{i, j}$.
%%Рассмотрим произвольный алгоритм $a \in \hat B_{r,\rho}^{m}$ и~обозначим через $t$ количество ошибок данного алгоритма на~$X \cap X_r$.
%%Тогда данный алгоритм делает $i + t$ ошибок на~обучении, и, следовательно, условие того, что он не~переобучен, записывается в~виде $i + t > s(\eps)$, где $s(\eps) = \frac{\ell}{L}(m + \rho - \eps k)$.
%%Количество алгоритмов в~$\hat B_{r,\rho}^{m}$ с~данным значением параметра $t$ равно $C_{j}^{t} C_{r - j}^{\rho - t}$.
%%Суммируя по~$t$, получим количество непереобученных алгоритмов:
%%\[
%%    N_{i, j} = \sum_{t = 0}^{\min(j, \rho)} C_{j}^{t} C_{r - j}^{\rho - t} [i + t > s(\eps)].
%%\]
%%Тогда, по~теореме~\ref{thTolstR2}, вероятность переобучения случайного подмножества $\hat B_{r,\rho}^{m}$, состоящего из~$d$ алгоритмов, дается следующей формулой:
%%\begin{equation}
%%    \bar{Q}_{\eps}(\hat B_{r,\rho}^{m},d) = 1 -
%%        \frac 1{\CLl}
%%            \sum_{i = 0}^{\min(m, \ell)} \sum_{j = 0}^{\min(r, \ell - i)}
%%               |\tau_{i,j}| \frac{C^d_{N_{i, j}}}{C^d_{|\hat B_{r,\rho}^{m}|}},
%%\end{equation}
%%где $\tau_{i,j}$ и~$N_{i, j}$ определены выше.
%%\end{vkProof}
%%
%%Теорема~\ref{th:interval-layer-subset} позволяет уточнить верхнюю оценку вероятности переобучения~\eqref{eq:decompToBallSlicePZM}.
%%Для этого будем оценивать $Q_\eps(A_i)$ с~помощью $\bar Q_\eps(B_{r,\rho}^{m})$, где
%%$d$ "--- мощность $A_i$,~$m$ "--- число объектов, на~которых ошибаются все $a \in A_i$,  $r$ "--- число таких объектов, для которых хотя бы два алгоритма из~$A_i$ дают разную классификацию,
%%$\rho$ "--- среднее число ошибок алгоритмов из~$A_i$ на~множестве $X_r$.
%%Отметим, что в~данном случае оценка $\Q(A_i) \leq \bar{Q}_\eps(\hat B_{r,\rho}^{m})$ является эвристикой.
%
%\subsection{Рандомизация целевого вектора}
%
%В работе~\cite{kochedykovDisser} на~основе экспериментальных данных предложена гипотеза о разложении профиля расслоения-связности в~произведение профиля расслоения и~профиля связности.
%В данном параграфе эта гипотеза будет доказана для частного случая средних профилей.
%
%Профиль расслоения $\Delta(A, m)$ множества алгоритмов~$A$ определим как
%зависимость количества алгоритмов в~$m$-слое $|A_m|$ от~номера слоя~$m$:
%\[
%    \Delta(A,m) = |A_m| = |\{a \in A \colon n(a, \XX) = m\}|.
%\]
%
%\begin{Def}
%Профилем $r$-связности множества алгоритмов~$A$ назовем функцию от~параметра $q$, заданную выражением:
%\[
%    \Theta_r(q, A) = \sum_{a \in A} [|B_r(a, A)| = q].
%\]
%Значение $\Theta_r(q, A)$ соответствует числу алгоритмов $a \in A$,
%имеющих ровно $q$ соседей в~шаре $B_r(a, A)$.
%\end{Def}
%
%Рассмотрим задачу классификации выборки $X_L$ на~два класса $Y = \{+1, -1\}$.
%Обозначим целевой класс объекта $x_i \in X_L$ через $y_i \in Y$.
%
%Рассмотрим действие группы $S_2 = \{e, h\}$ на~множестве $Y$,
%при~котором неединичный элемент $h\in S_2$ действует
%сменой метки целевого класса на~противоположную: $+1 \leftrightarrow -1$.
%
%Рассмотрим группу $G = (S_2)^L$ и~ее элемент $g \in G \brop= \{h_1, h_2, \ldots, h_L\}$.
%Элемент $g$ действует на~генеральную выборку $\XX$ с~помощью смены целевых классов у~тех
%объектов $x_i$, для~которых $h_i \neq e$.
%Действие группы $G$ на~$\XX$ естественным образом продолжается до
%действия на~множестве всех алгоритмов $\AA$.
%Действительно, на~каждый алгоритм $a \in \AA$ элемент $g \in G$ действует по~правилу
%\begin{equation}
%\label{eq:inverselabels}
%    I(g a, x_i) =
%    \begin{cases}
%        +I(a, x_i), &\text { при } h_i = e \\
%        -I(a, x_i), &\text { при } h_i \neq e. \\
%    \end{cases}
%\end{equation}
%Это соответствует инверсии вектора ошибок на~тех объектах,
%у~которых $g$ меняет целевой класс на~противоположный.
%
%\begin{Lemma}
%\label{lem_isometr}
%Действие определенной выше группы $G$ является изометрией
%относительно хэммингова расстояния $\rho$ между векторами ошибок: для~всех $a, a'\in \AA$ и~любого~$g \in G$
%\[
%\rho(a, a') = \rho(g a, g a').
%\]
%\end{Lemma}
%
%\begin{vkProof}
%
%Используя определение~\eqref{eq:inverselabels}, убеждаемся, что
%\[|I(g a, x_i) - I(g a', x_i)| = |I(a, x_i) - I(a', x_i)|\]
%при обоих возможных значениях~$g_i$. Тогда
%\begin{align*}
%    \rho(ga, ga') &= \sum_{x_i \in \XX} |I(g a, x_i) - I(g a', x_i)| ={}\\
%    {}&=\sum_{x_i \in \XX} |I(a, x_i) - I(a', x_i)| = \rho(a, a').
%\end{align*}
%
%\end{vkProof}
%
%Из доказанной выше леммы немедленно следует, что для задач бинарной классификации
%профиль $r$-связности инвариантен к~смене меток целевых классов:
%\[
%    \text{для любого } g \in (S_2)^L \text{ выполнено } \Theta_r(q, A) = \Theta_r(q, g A).
%\]
%
%Таким образом, связность является топологическим свойством семейства алгоритмов,
%зависящим лишь от~взаимного расположения классифицируемых объектов в~пространстве признаков,
%но не~зависящим от~меток их~классов.
%Проиллюстрируем предыдущее утверждение на~примере семейства линейных классификаторов.
%
%Рассмотрим задачу классификации точек трехмерного пространства разделяющими плоскостями,
%проходящими через центр координат: $y = \sign(\langle w, x\rangle)$.
%Тогда, с~топологической точки зрения, множеством алгоритмов будет сфера $S^2$.
%Точки сферы соответствуют единичному направляющему вектору разделяющей плоскости.
%Противоположные точки сферы соответствуют одновременной смене классификации всех объектов на~противоположную.
%
%\begin{figure}
%    \begin{centering}
%    \includegraphics[height=100mm]{Pictures/sphereMap.eps}
%    \caption {
%      Точки сферы "--- направляющие векторы линейной разделяющей гиперплоскости,
%      грани графа на~сфере "--- классы эквивалентных алгоритмов (с равными векторами ошибок);
%      пары граней имеющих общую границу соответствуют алгоритмам, различающимся на~одном объекте.
%    }
%    \end{centering}
%\end{figure}
%
%Зафиксируем выборку объектов $X_L$, каждому из~которых приписан целевой класс.
%Это порождает раскраску всей сферы на~области, соответствующие алгоритмам с~равными векторами ошибок.
%Заметим, что алгоритмы различаются на~одном объекте тогда и~только тогда, когда у~них есть общая граница
%на полученной карте. Следовательно, профиль $1$-связности полностью определяется отношением соседства алгоритмов.
%Границы карты, в~свою очередь, задаются условием $\langle w, x\rangle = 0$,
%инвариантным по~отношению к~меткам целевых классов.
%Следовательно, и~профиль $1$"~связности не~зависит от~меток целевых классов.
%
%
%Назовем профилем расслоения"=связности $\Lambda_r(m, q, A)$ семейства~$A$ число алгоритмов
%$a \in A$ с~$m$ ошибками и~$q$ соседями в~шаре радиуса $r$:
%\[
%    \Lambda_r(m, q, A) = \big|\{a \in A \colon |B_r(a, A)| = q\text{ и~} n(a, \XX) = m \}\big|.
%\]
%Известны общие верхние оценки вероятности переобучения~\cite{voron10pria-eng},
%использующие профиль расслоения"=связности при $r=1$.
%Вместе с~тем было известно, что профиль расслоения"=связности обладает рядом интересных свойств.
%В~частности, для семейства линейных классификаторов было экспериментально показано,
%что профиль расслоения"-связности приближенно раскладывается в~произведение двух функций,
%одна из~которых зависит от~числа ошибок алгоритма на~полной выборке, а~вторая "--- только от~числа связей алгоритма.
%
%Ниже мы приводим точную формулировку и~доказательство данной гипотезы.
%Оказывается, точное равенство выполняется после усреднения профиля расслоения"-связности
%по действию группы $G=(S_2)^L$ всевозможных смен целевых классов объектов.
%
%\begin{Lemma}[О наиболее вероятном профиле расслоения]
%\label{th_averageS}
%\[
%    \frac {1}{|G|} \sum_{g \in G} \Delta(m, g A) = \frac{\Binom{L}{m}}{2^L} |A|.
%\]
%\end{Lemma}
%Данное утверждение означает, что биномиальное распределение $P(m) = \frac{\Binom{L}{m}}{2^L}$ задает статистически
%наиболее вероятное распределение числа алгоритмов по~количеству ошибок.
%Необходимо учитывать, что для~реальных задачах данное распределение будет иным.
%Смещение фактического распределения алгоритмов по~числу ошибок
%относительно биномиального можно использовать в~качестве меры
%информативности (<<удачности>>) использования данного семейства~$A$
%для описания свойств конкретной выборки данных $\XX$.
%
%\begin{vkProof}
%
%Переставим знаки суммирования по~$g \in G$ и~по $a' \in A$:
%\[
%    \sum_{g \in G} \Delta(m, g A)
%    = \sum_{g \in G} \sum_{a \in g A} [ n(a, \XX) = m]
%    = \sum_{a' \in A} \sum_{g \in G} [n (g a', \XX) = m].
%\]
%Теперь осталось  воспользоваться тем, что усредненный профиль
% расслоения, записанный для фиксированного алгоритма $a'$,
%задается биномиальным распределением:
%\[
%    \text{для всех } a' \in A_m \text{ выполнено: } \frac 1{|G|}\sum \limits_{g \in G} [n (g\, a', \XX) = m] =
%    \frac {\Binom{L}{m}}{2^L}.
%\]
%
%\end{vkProof}
%
%\begin{Theorem}[О~наиболее вероятном профиле расслоения"=связности]
%\label{th:MostLikeProfile}
%\[
%    \frac {1}{|G|} \sum_{g \in G} \Lambda_r(m, q, g A) =
%    \frac{\Binom{L}{m}}{2^L} \cdot \Theta_r(q, A).
%\]
%\end{Theorem}
%
%Доказательство этой теоремы непосредственно следует из~инвариантности профиля
%$r$"~связности относительно действия группы $G$ и
%леммы~\ref{th_averageS} о~наиболее вероятном профиле расслоения.
%
%\begin{vkProof}
%
%Обозначим оператор усреднения по~действию группы $G$ через $\textbf{E}_G =
%\frac {1}{|G|} \sum_{g \in G}$.
%\[
%\begin{aligned}
%    \textbf{E}_G \Lambda_r&(m, q, g A) ={}\\
%        & {}= \textbf{E}_G \sum_{a' \in A} [|B_r(g a', g A)| = q ][n(g a', \XX) = m ] ={}\\
%        & {}= \sum_{a' \in A} \textbf{E}_G [|B_r(a', A)| = q ][n(g a', \XX) = m ] ={}\\
%        & {}= \sum_{a' \in A} \Big( [|B_r(a', A)| = q ] \cdot \textbf{E}_G [n(g a', \XX) = m ] \Big)={}\\
%        & {}= \frac{\Binom{L}{m}}{2^L} \cdot \Theta_r(q, A). \\
%\end{aligned}
%\]
%
%\end{vkProof}
%
%В~работах~\cite{kochedikov} экспериментально установлено,
%что без усреднения по~действию группы $G$
%аналогичное равенство выполнено приближенно:
%\[
%    \Lambda_r(m, q, A) \approx \frac 1{|A|} \Delta(m, A) \cdot \Theta_r(q, A).
%\]
%
%Приведем экспериментальные результаты по~сравнению
%наиболее вероятного профиля расслоения $\frac{\Binom{L}{m}}{2^L}$
%с~реальными профилями, возникающими при классификации объектов плоскости линейными классификаторами.
%
%На~приведенных ниже рисунках рассматривалось четыре случая,
%отличающихся степенью линейной разделимости объектов выборки.
%Все сгенерированные выборки содержали 24 объекта (по~12 в~каждом классе).
%Строились все возможные варианты линейной классификации моделями вида $y = \sign(\langle w, x\rangle + w_0)$
%и~экспериментально вычислялось значения
%профиля расслоение $\Delta(m, A)$.
%На~графиках с~профилями расслоения по~оси X откладывается количество ошибок,
%по~оси Y "--- доля алгоритмов с~соответствующим уровнем ошибки.
%Более толстая кривая соответствует фактическому профилю расслоения, более тонкая "--- усредненному биномиальному.
%
%\begin{figure}[t]
%    \begin {multicols}{2}
%    \centering
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/data_L24_V2.0.eps}
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/data_L24_V1.0.eps}
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/data_L24_V0.5.eps}
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/data_L24_V0.eps}
%    \hfill
%    \medskip
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/profile_L24_V2.0.eps}
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/profile_L24_V1.0.eps}
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/profile_L24_V0.5.eps}
%    \hfill
%    \includegraphics[width=54mm,height=36mm]{Pictures/profile_L24_v0.eps}
%    \hfill
%    \end {multicols}
%    \caption{Зависимость профиля расслоения от~степени резделимости выборки.}
%    %\caption{В левом столбце показаны четыре различных двумерных выборки из~24~объектов.
%    %Объекты разных классов показаны точками и~звездочками.
%    %В~правом столбце показаны соответствующие профили расслоения для семейства линейных классификаторов.
%    %По~оси~OX отложено количество ошибок,
%    %по~OY "--- доля алгоритмов семейства с~соответствующим количеством ошибок.
%    %Тонкая кривая одинакова на~всех четырех рисунках и~соответствует наиболее вероятному распределению числа ошибок.}
%\end{figure}
%
%Отметим, что смещение профиля расслоения относительно
%биномиального распределения можно использовать в~качестве
%меры информативности множества алгоритмов.
%К сожалению, данный профиль является ненаблюдаемой величиной.
%Обычно поиск лучшего алгоритма обычно является итерационной процедурой,
%в~ходе которой генерируется лишь некоторое подмножество алгоритмов
%из~рассматриваемого семейства.
%Профиль расслоения наблюдаемого подмножества будет, очевидно, смещен
%будет смещен в~сторону алгоритмов с~малым количеством ошибок.
%Возможно, данное смещение удастся оценить, если провести калибровку:
%подать на~вход алгоритму оптимизации шумовую выборку,
%полученную по~исходном данным с~помощью случайной смены меток целевых классов.
%Перспективным применением развитой выше теории представляется
%дальнейшее изучение критериев информативности основанных на~наблюдаемом профиле распределения ошибок.

\section{Основные выводы}

Теоремы~\ref{th:QAorbit} и~\ref{th:QXorbit}, полученные в~данном параграфе,
являются основным инструментом для вывода оценок вероятности переобучения РМЭР
для симметричных семейств алгоритмов.
Оценки ~\eqref{eq:QAorbit} и~\eqref{eq:QXorbit}
являются точными равенствами и, следовательно, неулучшаемы.

Помимо этого,
метод порождающих и~запрещающих множеств (ПЗМ),
предложенный К.\,В.\,Воронцовым,
был обобщен по~двум направлениям:
во-первых, на~случай разложения множества алгоритмов на~кластеры,
во-вторых, на~случай РМЭР.
