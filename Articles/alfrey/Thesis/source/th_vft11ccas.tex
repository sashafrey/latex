\documentclass{ccaspreprint}
\usepackage{multicol}
\usepackage{color}
\RequirePackage[all,poly]{xy}

\newtheorem{hypothesis}{Гипотеза}

% ============================================================================
% Шаманские пляски
% ============================================================================

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\X}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}

\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\sign}{\mathop{\rm sign}\limits}
\renewcommand{\epsilon}{\varepsilon}\newcommand{\eps}{\varepsilon}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\hypergeom}[5]{{#1}_{#2}^{#4,#3}\left(#5\right)}
\newcommand{\Bhypergeom}[5]{{#1}_{#2}^{#4,#3}\bigl(#5\bigr)}
\newcommand{\hyper}[4]{\hypergeom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\hypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\BHyper}[4]{\Bhypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\hypergeom{\bar{H}}{#1}{#2}{#3}{#4}}


\newcommand{\IncludeHalfPicture}[1]{\includegraphics[width=54mm,height=38mm]{#1}}

\def\brop#1{#1\discretionary{}{\hbox{$#1$}}{}}

\def\RR{\mathbb{R}}
\def\DD{\mathbb{D}}
\def\fF{\mathfrak{F}}
\def\fI{\mathfrak{I}}
\def\fM{\mathfrak{M}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\def\afterlabel#1{\renewcommand\labelenumi{\theenumi #1}}


\newcounter{mmroReviewerNote}
\setcounter{mmroReviewerNote}{0}%
\newcommand{\REVIEWNOTE}[1]
{
    \refstepcounter{mmroReviewerNote}%
    \textbf{ REVIEWNOTE \themmroReviewerNote: } #1
}

\begin{document}
% ============================================================================
% Оформление титульной страницы и~аннотации
% ============================================================================
% Заглавие и~автор
\title{Комбинаторные оценки вероятности переобучения: теоретико-групповой подход}
\author{Воронцов~К.\,В., Фрей А.\,И., Толстихин~И.\,О.}

% Номер УДК
\udk{004.852}

% Ответственный редактор
\editor{чл.-корр. РАН К.\,В.~Рудаков}

% Рецензенты: внешний и~внутренний
\reviewer[1]{В.\,В.~Стрижов}
\reviewer[2]{А.\,Г.~Дьяконов}

% Год публикации
\year{2011}

% Ключевые слова через запятую с~маленькой буквы. Точка в~конце не~нужна.
\keywords{%
    теория статистического обучения,
    обобщающая способность,
    вероятность переобучения,
    теория групп
}

% Аннотация
\anno{
    В~рамках комбинаторной теории переобучения
    развивается теоретико"~групповой подход,
    позволяющий получать точные оценки вероятности переобучения
    для симметричных и~рандомизированных семейств алгоритмов.
}

\maketitle

\clearpage
\tableofcontents

% ============================================================================
% Основной текст препринта
% ============================================================================
\section{Проблема переобучения при~восстановлении зависимостей по~эмпирическим данным}

При решении задач
распознавания образов, восстановления регрессии, прогнозирования
всегда возникает проблема выбора по~неполной информации.
Имея лишь конечную обучающую выборку объектов,
требуется из~заданного множества алгоритмов выбрать алгоритм, который
ошибался~бы как можно реже
не~только на~объектах наблюдаемой обучающей выборки,
но~и~на~объектах скрытой контрольной выборки,
которая в~момент выбора алгоритма ещё неизвестна.
Если частота ошибок на~контрольной выборке
оказывается значительно выше, чем на~обучающей,
то~говорят, что произошло
<<переобучение>> (overtraining) или
<<переподгонка>> (overfitting) алгоритма "---
он~слишком хорошо описывает конкретные данные,
но~не~обладает способностью к~обобщению этих данных,
не~восстанавливает порождающую их~зависимость
и~не~пригоден для построения прогнозов.

На~практике обучающая выборка формируется раньше, чем контрольная.
Таким образом, обучающая и~контрольная выборки могут иметь различные статистические свойства.
Низкое качество на~контроле может быть обусловлено не~только
переобучением алгоритма, но~и~нестационарностью данных во~времени.

Процесс формирования обучающей выборки также является важным фактором, влияющим на~переобучение.
В~частности, большое значение имеет представительность (репрезентативность) обучающей выборки.
Данная проблема хорошо известна при проведении социалогических опросов.
Как ограничить круг респондетнов, чтобы тем не~менее представить весь спектр общественного мнения?
Аналогичный вопрос возникает и~при формировании обучающей выборки.

В~дальнейшем выборка данных будет предполагаться репрезентативной и~стационарной.
Главной целью ставится исследование свойств метода обучения как такового.

На~практике переобучение оценивается количественно с~помощью процедуры скользящего контроля (кросс"=валидации).
Фиксируется некоторое множество разбиений исходной выборки на~две подвыборки~--- обучающую и~контрольную.
Для каждого разбиения выполняется настройка алгоритма по~обучающей подвыборке,
затем оценивается его средняя ошибка на~объектах контрольной подвыборки.
\emph{Оценкой скользящего контроля} называется средняя по~всем разбиениям величина ошибки на~контрольных подвыборках.
Аналогичным образом определяется и~\emph{оценка вероятности переобучения}~---
это доля разбиений, при которых
средняя ошибка на~контрольной подвыборке превышает
среднюю ошибку на~обучающей подвыборке более чем на заданную величину~$\eps$.
Главный недостаток данного подхода~--- большая вычислительная сложность,
связанная с~многократной настройкой алгоритма классификации.
Точность оценки скользящего контроля зависит от~стабильности метода обучения
и~от~числа разбиений, по~которым производилось усреднение.
В~комбинаторной теории переобучения рассматривается множество всех возможных разбиений
и~ставится задача получения вычислительно эффективных формул
для оценок скользящего контроля и~вероятности переобучения~\cite{voron09dan,voron10pria}.

Теоретико-групповой подход~\cite{frey09mmro,frey10ioi,frey10pria}
позволяет получать такие формулы для случаев, когда
семейство алгоритмов обладает некоторой симметрией,
а~методом обучения является \emph{рандомизированная минимизация эмпирического риска}.
Рандомизация означает, что если в~семействе существует несколько алгоритмов,
допускающих одинаковое минимальное число ошибок на~обучающей выборке,
то~из них равновероятно выбирается любой.

\subsection{Задача обучения по~прецедентам}

\subsection{Рандомизированная минимизация эмпирического риска}

\subsection {Перестановки объектов}

\subsection{Группа симметрий множества алгоритмов.}

\subsection{Теорема о~порождающих и~запрещающих объектах}

\section{Точные оценки вероятности переобучения}

\subsection{Монотонная цепочка алгоритмов}

\subsection{Унимодальная цепочка алгоритмов}

\subsection{Пучок монотонных цепочек}

\subsection{Многомерная монотонная сеть алгоритмов}

\subsection{Многомерная унимодальная сеть алгоритмов}

\subsection{Разреженные монотонные и унимодальные сети}

\subsection{Один слой хэммингова шара}

\subsection{Хэммингов шар}

\subsection{Нижние слои хэммингова шара}

\section{Рандомизированные семейства алгоритмов}

\subsection{Разреженные подмножества слоя}

\subsection{Разреженные подмножества слоя: \\случайный выбор без~возвращения}

\subsection{Разреженные подмножества семейств, лежащих в~слое}

\section{Профили расслоения и~связности множества алгоритмов}

Поведение функционала $Q_\eps(A)$ существенным образом зависит от~структуры множества алгоритмов $A$.
В~то же время очевидно, что далеко не~все возможные множества алгоритмов $A \subset \AA$ возникают
при решении практических задач обучения по~прецедентам.

Во"~первых, большинство реальных семейств обладают свойством связности:
для каждого алгоритма ${a\in A}$ найдутся другие алгоритмы ${a'\in A}$
такие, что векторы ошибок $\vec a$~и~$\vec a'$
отличаются только на~одном объекте~\cite{sill98phd}.
Связные семейства порождаются, в~частности, методами классификации
с~непрерывной по~параметрам разделяющей поверхностью.
К~ним относятся линейные классификаторы,
машины опорных векторов с~непрерывными ядрами,
нейронные сети с~непрерывными функциями активации,
решающие деревья с~пороговыми условиями ветвления, и~многие другие.
Эксперименты \ref{voron10pria} показали, что с~уменьшением связности семейства
алгоритмов вероятность переобучения существенно возрастает.

Другим свойством, характерным для реальных семейств алгоритмов, является \emph{расслоение}.
Под $m$-слоем алгоритмов $A_m \subset A$ будем понимать подмножество алгоритмов,
допускающих ровно~$m$~ошибок на~полной выборке.
Тогда профиль расслоения $\Delta(A, m)$ множества алгоритмов $A$ определим как
зависимость количества алгоритмов в~$m$-слое $|A_m|$ от~номера слоя $m$:
\[
    \Delta(A,m) = |A_m| = |\{a \in A \colon n(a, \XX) = m\}|.
\]
Эксперименты~\cite{langford00computable,langford02quantitatively} показывают,
что для большинства применяемых на~практике семейств алгоритмов $A$ профиль расслоения $\Delta(A,m)$
имеет форму узкого пика, сконцентрированного в~средних слоях~$m\approx L/2$.
В~то же время, вероятность переобучения в~значительной мере определяется нижними слоями "---
алгоритмами с~наименьшим числом ошибок на~полной выборке.
Алгоритмы высоких слоев имеют ничтожно малую вероятность реализоваться в~результате обучения,
и~потому оказывают незначительный вклад в~вероятность переобучения.

Изучение профиля расслоения и~эффекта связности представляется важным шагом
на~пути к~получению универсальных формул вероятности переобучения.

\subsection{Профиль $r$-связности множества алгоритмов}

В~данном параграфе мы определим профиль $r$-связности множества алгоритмов и~изучим его свойства.
В~частности, для задач классификации на~два класса будет доказана инвариантность профиля $r$-связности
по~отношению к~произвольной смене меток целевых классов объектов.

Напомним, что \emph{расстояние между алгоритмами} $\rho(a, a')$ определялось как
расстояние Хэмминга между их~векторами ошибок на~полной выборке:
\[
    \rho(a, a') = \sum\limits_{x \in \XX} |I(a, x) - I(a', x)|.
\]
Шаром $B_r(a, A)$ радиуса $r$ с центром в алгоритме $a_0$ называлось следующее множество множества алгоритмов $A$:
\[
\{B_r(a, A) = a' \in A \colon \rho(a, a') \leq r\}.
\]

%\REVIEWNOTE{Включи в~определение шара слова о~семействе~$A$.}

Изучение топологической природы множества алгоритмов позволяет
вывести ряд полезных свойств профиля $r$-связности, и, в~частности, среднего числа связей алгоритмов.
Для рассмотренного примера данную величину легко получить используя соотношение
Эйлера между числом вершин, ребер и~граней графа, реализованного на~сфере.

Действительно, пусть точки выборки $X_L$ находятся в~общем положении.
Тогда для~каждой точки выборки $x \in X_L$ рассмотрим семейство разделяющих плоскостей $A_x \subset S^2$,
проходящих через начало координат и~заданную точку.
На~рассмотренной выше сфере алгоритмов данное множество является центральной окружностью,
двойственной к~рассматриваемой точке.
Следовательно, граф границ алгоритмов получается с~помощью сечения сферы центральными окружностями.

Каждая вершина графа получается как пересечение двух центральных окружностей. Следовательно, количество
вершин ${V = L(L-1)}$. Из каждой вершины выходит ровно четыре ребра. Следовательно, количество ребер графа
$E = 2 L (L-1)$. Это позволяет определить количество граней графа:
\[  V - E + S = \varphi(S^2),\]
где $\varphi(S^2) = 2$ "--- Эйлерова характеристика сферы.
Следовательно, количество разделяющих гиперплоскостей в~семействе равно ${S = L(L-1) + 2}$.

Среднее количество связей между
$\frac {1}{|A|}\sum\nolimits_{q = 0}^{|A-1|} q \cdot \Theta_1(q, A)$
алгоритмами определяется отношением $2\, E/ S$,
т.\,е. асимптотически стремиться к~$4$ в~рассматриваемом двумерном случае.
В~работе \ref{kochedikov00candiser} доказывается более общее утверждение о~том,
что для семейства линейных классификаторов данная величина равна
удвоенной размерности пространства признаков. Вопрос об изучении профиля $r$-связности
$\Theta_r(q, A)$ при $r \geq 2$, по~всей видимости, еще не~изучался.

\subsection{Профиль расслоения"=связности множества алгоритмов}

Напомним, что профиль расслоения $\Delta(m, A)$ определялся как число алгоритмов $a \in A$,
допускающих ровно $m$ ошибок на~объектах полной выборки:
\[
    \Delta(m, A) = \sum_{a \in A} [ n(a, \XX) = m].
\]

\subsection{Экспериментальные результаты о~профиле расслоения}

%\clearpage
%\def\url#1.{}

\bibliography{MachLearn}
\bibliographystyle{gost71sv}

%\begin{thebibliography}{1}
%\bibitem{botov09mmro}
%    \BibAuthor{Ботов\;П.\,В.}
%    \BibTitle{Точные оценки вероятности переобучения для монотонных и~унимодальных семейств алгоритмов}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,7--10}.
%\bibitem{voron09dan}
%    \BibAuthor{Воронцов\;К.\,В.}
%    \BibTitle{Точные оценки вероятности переобучения}~//
%    Доклады РАН, 2009. "--- Т.\,429, \No\,1.  "--- С.\,15--18.
%\bibitem{voron09mmro}
%    \BibAuthor{Воронцов\;К.\,В.}
%    \BibTitle{Комбинаторный подход к~проблеме переобучения}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009. "---  \mbox{С.\,18--21}.
%\bibitem{frey09mmro}
%    \BibAuthor{Фрей\;А.\,И.}
%    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов}~//
%    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,66--69}.
%\bibitem{frey10ioi}
%    \BibAuthor{Фрей\;А.\,И.}
%    \BibTitle{Вероятность переобучения плотных и~разреженных многомерных~сеток алгоритмов}~//
%    Интеллектуализация обработки информации "--- М.:~МАКС Пресс, 2010.  "---  \mbox{С.\,87--90}.
%\bibitem{frey10pria}
%    \BibAuthor{Frei\;A.\,I.}
%    \BibTitle{Accurate estimates of the generalization ability for symmetric set of predictors and randomized learning %algorithms}~//
%    Pattern Recognition and Image Analysis. "--- 2010. "--- Vol. 20, no. 3. "--- \mbox{Pp.\,241--250}.
%\bibitem{voron10pria}
%    \BibAuthor{Vorontsov\;K.\,V.}
%    \BibTitle{Exact combinatorial bounds on the probability of overfitting for empirical risk minimization}~//
%    Pattern Recognition and Image Analysis. "--- 2010. "--- Vol. 20, no. 3. "--- \mbox{Pp.\,269--285}.

%\end{thebibliography}

%\begin{thebibliography}{10}
%\bibitem{bishop2006pattern} \BibAuthor{Bishop~C.}
%Pattern Recognition And Machine Learning. Springer.~2006.
%\end{thebibliography}

\end{document}
