\documentclass[12pt]{article}

\usepackage[cp1251]{inputenc}
\usepackage[russian]{babel}
\usepackage{color}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage[noend]{algorithmic}
\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
\def\algorithmicgoto{\textbf{перейти к шагу}}
\def\algorithmicreturn{\textbf{вернуть}}

\newcommand{\LINEIF}[2]{%
    \STATE\algorithmicif\ {#1}\ \algorithmicthen\ {#2} %
}
\newcommand{\GOTO}[1]{%
    \algorithmicgoto\ {\ref{#1};} %
}


\textheight=26cm
\textwidth=17cm
\oddsidemargin=0mm
\topmargin=-20mm
\parindent=24pt
\tolerance=500
%\renewcommand{\baselinestretch}{1.3} %для печати с большим интервалом

\newcommand{\Expect}{\mathsf{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\X}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\def\brop#1{#1\discretionary{}{\hbox{$#1$}}{}} % перенос знака операции на следующую строку
\renewcommand{\em}{\it}
\newcommand{\sign}{\mathop{\rm sign}\limits}

\def\CCfont#1{\ifmmode{\mathsf{#1}}\else\mbox{\rm\textsf{#1}}\fi}
\def\P{\CCfont{P}}

\newcommand{\hypergeom}[5]{{#1}_{#2}^{#4,#3}(#5)}
\newcommand{\hyper}[4]{\hypergeom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\hypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\hypergeom{\bar{H}}{#1}{#2}{#3}{#4}}
\newcommand{\Binom}[2]{C_{#1}^{#2}}
%\newcommand{\Binom}[2]{\binom{#1}{#2}}
\newcommand{\CLl}{\Binom{L}{\ell}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}

\newcommand{\mur}[3]{\mu({#1}, {#2}, {#3})}
\newcommand{\todo}{\textcolor{red}{[ToDo]} }

\theoremstyle{plain}
\newtheorem{Theorem}{Теорема}
\newtheorem{Lemma}[Theorem]{Лемма}
%\newtheorem{State}[Theorem]{Утверждение}
\theoremstyle{definition}
\newtheorem{Def}{Определение}
\newtheorem{Definition}[Def]{Определение}
\newtheorem{Corollary}{Следствие}
\newtheorem{Hypothesis}{Гипотеза}
\newtheorem{Task}{Задача}
\newtheorem{Example}{Пример}

\newcommand{\vkEndProof}{\hfill$\scriptstyle\blacksquare$\par\medskip}
\newenvironment{vkProof}[1][. ]%
    {\par\noindent{\bf Доказательство#1}}%
    {\vkEndProof}

% Проглотить следующий пробел
\makeatletter
\def\gobblespace{\@ifnextchar\ {\hspace{-1ex}}\relax}
\makeatother
% Вставка замечания рецензента
\newcommand\REVIEWERNOTE[1]{{%
    \itshape\bfseries%\color{red}%
    \marginpar{%\raisebox{-1ex}{%\color{red}%
        $\checkmark$%\!_{\themmroReviewerNote}
    }%}%
    \{#1\}
}\gobblespace}

\begin{document}

\begin{center}
{\Large \bf Обобщение оценки расслоения-связности на случай произвольного графа Хассе}

\vspace{0.2cm}

{\bf Фрей Александр, Решетняк Илья.}

\vspace{0.1cm}

\end{center}

%\abstract {
%}

\tableofcontents

\section{Введение}

\todo{Добавить введение.}

Альтернативное название статьи: учёт верхней связности на основе слабого замыкания множества алгоритмов.

\section{Основные обозначения}

Пусть задана генеральная выборка $\XX=\bigl( x_1, \ldots, x_L)$, состоящая из~$L$ объектов.
Произвольный алгоритм классификации, примененный к~данной выборке, порождает бинарный вектор
ошибок $a \equiv \bigl( I(a, x_i) \bigr){}_{i=1}^L$,
где $I(a, x_i) \in \{0, 1\}$ "--- индикатор ошибки алгоритма $a$ на~объекте $x_i$.
В~дальнейшем алгоритмы будут отождествляться с~векторами их~ошибок на~выборке $\XX$.

Обозначим
через $\AA = \{0, 1\}^L$ множество всех возможных векторов ошибок длины~$L$.
Через~$\XXell$ обозначим множество всех разбиений генеральной выборки~$\XX$
на обучающую выборку~$\Xl$ длины~$\ell$ и~контрольную выборку~$\Xk$ длины $k=L-\ell$.
\emph{Число ошибок} алгоритма~$a$ на~выборке $U \subseteq \XX$
обозначим через $n(a, U) = \sum \limits_{x \in U}I(a, x)$.
Величину $\nu(a, U) = n(a, U) / |U|$ будем называть \emph{частотой ошибок}
алгоритма~$a$~на~выборке $U$.
\emph{Уклонение частот} на~разбиении $\XX = \Xl \sqcup \Xk$ определим как разность
частот ошибок на~контроле и~на обучении:
$\delta(a, \Xl) = \nu(a, \Xk) - \nu(a, \Xl)$.

Пусть~$A \subset \AA$ "--- множество алгоритмов с~попарно различными векторами ошибок.
Обозначим через $A(\Xl)$ множество алгоритмов с~минимальным числом ошибок на~обучающей выборке~$\Xl$:
\begin{equation}
\label{eqERM-A(X).sym}
    A(X) = \Argmin_{a\in A} n(a,\Xl).
\end{equation}

Частоту ошибок на~обучающей выборке называют \emph{эмпирическим риском}.
\emph{Минимизация эмпирического риска $\mu$} "--- это метод обучения,
который из~заданного множества $A \subset \AA$ выбирает алгоритм $a \in A$,
допускающий наименьшее число ошибок на~обучающей выборке $\Xl$.
Таким образом, для всех $\Xl \in \XXell$ выполнено $\mu \Xl \in A(\Xl)$.
В дальнейшем будет рассматриваться \emph{пессимистическая} минимизация эмпирического риска,
удовлетворяющая дополнительному условию $\mu \Xl \in A(\XX)$ "--- то есть
среди алгоритмов в~$A(\Xl)$ выбирается алгоритм с~наибольшим числом ошибок на полной выборке.

Говорят, что метод $\mu$ переобучен на~разбиении $\Xl \sqcup \Xk$, если
уклонение частот $\delta(a, \Xl)$ превышает фиксированный порог $\epsilon$.
Переобучение может быть следствием <<неудачного>> разбиения
генеральной выборки на~обучение и~контроль.
Поэтому вводится функционал \emph{вероятности переобучения},
равный доле разбиений выборки, при которых возникает переобучение~\cite{voron09dan, voron09mmro}:
\[
    Q_\eps(A) = \Expect [\delta(\mu \Xl, \Xl) \geq \epsilon],
    \text{ где } \Expect = \frac 1{\CLl} \sum\limits_{\Xl \in \XXell}.
\]
Тут и~далее квадратные скобки "--- нотация Айверсона,
переводящая логическое выражение в~число $0$ или $1$ по~правилам
$[\text{истина}] = 1$, $[\text{ложь}] = 1$.

Функционал $Q_\eps(A)$ уже не~зависит от~выбора разбиения
и~характеризует качество данного метода обучения на~данной генеральной выборке.

\section{Теорема о порождающих и запрещающих объектах}

Первый подход, позволивший получать точные оценки вероятности переобучения
в~рамках слабой вероятностной аксиоматики, основан на выделении порождающих
и~запрещающих объектов \cite{voron10pria}.

\begin{Hypothesis}
\label{hyp1}
    Пусть множество~$A$, выборка~$\XX$ и~детерминированный метод обучения~$\mu$ таковы, что
    для каждого алгоритма $a\in A$
    можно указать
    конечное множество индексов $V_a$, и для каждого индекса $v \in V_a$ можно указать
    порождающее множество $X_{a v} \subset \XX$,
    запрещающее множество $X'_{a v} \subset \XX$ и
    коэффициент $c_{a v} \in \RR$, удовлетворяющие условиям
    \begin{equation}
    \label{eq1muX}
        \bigl[ \mu X{=}a \bigr]
        =
        \sum_{v \in V_a}
        c_{a v}
        \bigl[  X_{a v}\subseteq  X \bigr]
        \bigl[ X'_{a v}\subseteq \X \bigr],
        \quad
        \forall X\in \XXell.
    \end{equation}
\end{Hypothesis}

Введем для каждого алгоритма $a \in A$ и каждого индекса $v \in V_a$ обозначения:
\begin{align*}
    L_{a v} &= L - |X_{a v}| - |X'_{a v}|;
\\
    \ell_{a v} &= \ell - |X_{a v}|;
\\
    m_{a v} &= n(a, \XX) - n(a, X_{a v}) - n(a, X'_{a v});
\\
    s_{a v}(\eps) &= \frac \ell L \left( n(a, \XX) - \eps k \right) - n(a, X_{a v}).
\end{align*}

В условиях гипотезы~\ref{hyp1} справедливы следующие утверждения о вероятностях получения алгоритмов и вероятности переобучения:

\begin{Theorem}
\label{th1}
    Если гипотеза~\ref{hyp1} справедлива,
    то для всех $a \in~A$ вероятность получить в~результате обучения алгоритм~$a$ равна
    \begin{align*}
        P_a &= \P[ \mu X{=}a ] = \sum_{v \in V_a} c_{a v} P_{a v}; \\
        P_{a v} &= \P[ X_{a v} \subset \X ][X'_{a v} \subset \Xk] =
        \frac{C_{L_{a v}}^{\ell_{a v}}}{C_{L}^{\ell}},
    \end{align*}
    а вероятность переобучения~$Q_\eps(A)$ выражается по~формуле
    \begin{equation}
    \label{eq1pzo}
    Q_\eps(A)
    =
    \sum_{a\in A}
    \sum_{v \in V_a}
        c_{a v} P_{a v}
        \Hyper{L_{a v}}{m_{a v}}{\ell_{a v}}{s_{a v}(\eps)},
    \end{equation}
    где
$
    \Hyper{L}{m}{\ell}{z}
    =
    \sum\limits_{s=0}^{\lfloor z \rfloor} \frac{\Binom{m}{s} \Binom{L-m}{\ell-s}}{\CLl}
$ "--- гипергеометрическая функция распределения.

\end{Theorem}

Отметим, что в ряде случаев удается подобрать лишь такие множества порождающих и запрещающих
объектов, для которых \eqref{eq1muX} выполнено лишь в виде неравенства:
\begin{equation}
\label{leq1muX}
    \bigl[ \mu X{=}a \bigr]
    \leq
    \sum_{v \in V_a}
    c_{a v}
    \bigl[  X_{a v}\subseteq  X \bigr]
    \bigl[ X'_{a v}\subseteq \X \bigr],
    \quad
    \forall X\in \XXell.
\end{equation}
Очевидно, что в данном случае выражения \eqref{eq1pzo} будет давать верхнюю оценку для вероятности переобучения $Q_\eps(A)$. В следующем параграфе данный факт будет использован для вывода верхней оценки вероятности переобучения.

\section{Оценка расслоения-связности для связных семейств алгоритмов}

Для пары алгоритмов $a, b \in A$ обозначим через $\rho(a, b)$
хэммингово расстояние между векторами ошибок алгоритмов $a$ и~$b$.
Введем на $A$ естественное отношение порядка: $a \leq b$ тогда и только тогда, когда
$I(a, x) \leq I(b, x)$ для всех $x \in \XX$. Определим $a < b$ есть $a \leq b$ и $a \neq b$.
Если $a < b$ и при этом $\rho(a, b) = 1$, то будем говорить, что $a$ \emph{предшествует} $b$,
и записывать $a \prec b$.

\begin{Definition}
\emph{Графом расслоения-связности} множества алгоритмов $A$ будем называть направленный граф $(A, E)$
с~множеством ребер $E = \{ (a, b) \colon a \prec b \}$.
\end{Definition}

Каждому ребру $a \prec b$ графа расслоения-связности соответствует один и только один объект $x_{a b} \in \XX$, такой что $I(a, x_{a b}) = 0$ и $I(b, x_{a b}) = 1$.

%Введем понятия \emph{верхней связности $u(a)$}, \emph{нижней связности $d(a)$} и
% \emph{неполноценности $q(a)$} алгоритма $a \in A$.

\begin{Definition}[Верхняя связность]
Обозначим через $X_u(a) = \{ x_{a b} \in \XX \colon a \prec b \}$ множество объектов, соответствующих ребрам графа расслоения-связности, \emph{исходящим} из вершины $a$. \emph{Верхней связностью $u(a) = |X_u(a)|$} назовем мощность множества $X_u(a)$.
\end{Definition}

\begin{Definition}[Нижняя связность]
Обозначим через $X_d(a) = \{ x_{b a} \in \XX \colon b \prec a \}$ множество объектов, соответствующих ребрам графа расслоения-связности, \emph{входящим} в вершину $a$. \emph{Нижней связностью $d(a) = |X_d(a)|$} назовем мощность множества $X_d(a)$.
\end{Definition}

Связность $u(a)$ (или $d(a)$) есть число способов изменить алгоритм $a$ так, чтобы он стал делать на одну ошибку больше (или меньше). Связность можно интерпретировать как число степеней свободы семейства $A$ в локальной окрестности алгоритма $a \in A$.

\begin{Definition}[Неполноценность алгоритма]
Обозначим через $X_q(a) = \{x_{c b} \colon c \prec b < a\}$ множество объектов, соответствующих всевозможным ребрам $(c, b)$ на путях, ведущих к вершине $a$. \emph{Неполноценностью $q(a) = |X_q(a)|$} алгоритма $a \in A$ будем называть мощность множества $X_q(a)$.
\end{Definition}

Легко доказать, что если метод $\mu$ является пессимистической минимизацией эмпирического риска, то $X_a = X_u(a)$ и $\bar X_a = X_q(a)$ можно использовать в качестве порождающего и~запрещающего множества:
\begin{equation}
    \bigl[ \mu X{=}a \bigr]
    \leq
    \bigl[ X_u(a)\subseteq  X \bigr]
    \bigl[ X_q(a)\subseteq \X \bigr],
    \quad
    \forall X\in \XXell.
\end{equation}
Следовательно, имеет место следующая верхняя оценка:
\begin{Theorem}[оценка расслоения-связности]
Для произвольной выборки $\XX$, пессимистического метода минимизации эмпирического риска $\mu$ и
произвольного $\eps \in (0, 1)$
\begin{equation}
    \label{eqConSplit}
    Q_\eps(A) = \sum_{a \in A} \frac{\Binom{L-u-q}{\ell - u}}{\CLl}\Hyper{L-u-q}{\ell-u}{m - q}{\tfrac{\ell}{L}(m-\eps k)},
\end{equation}
где $u \equiv u(a)$ "--- верхняя связность, $q \equiv q(a)$ "--- неполноценность, $m = n(a, \XX)$ "--- число ошибок алгоритма $a$ на генеральном множестве объектов.
\end{Theorem}

Благодаря комбинаторному сомножителю $\frac{\Binom{L-u-q}{\ell - u}}{\CLl}$
вклад каждого алгоритма $a$ в оценку $Q_\eps$ экспоненциально убывает
с ростом неполноценности $q$ и связности $u$.

\section{Разреженная монотонная сеть}

В данном параграфе рассматривается \emph{разреженная монотонная сеть}
"--- модельное семейство алгоритмов,
демонстрирующее один из недостатков полученной выше оценки \eqref{eqConSplit}.

Мы начнём с плотной (не-разреженной) многомерной сети алгоритмов.
Данное семейство является моделью параметрического \emph{связного семейства алгоритмов},
предполагающего, что при непрерывном удалении каждой компоненты вектора параметров
от~оптимального значения число ошибок на~полной выборке только увеличивается.
Известно, что рассмотренная выше оценка расслоения-связности \eqref{eqConSplit}
дает точное значение вероятности переобучения для многомерной сети алгоритмов.
Тем не менее, для разреженной монотонной сети, полученной удалением некоторой части алгоритмов из плотной монотонной сети, оценка расслоения-связности вырождается.

Введём целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \ldots, h} |d_j|$,\:
$|\vec d| = |d_1| + \ldots + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из~неравенств строгое.

\begin{Definition}
    \label{eq:monotonicSet}
    Множество алгоритмов
    $A = \bigl\{a_{\vec d}\bigr\}$, где~$\vec d \geq 0$ и~$\|\vec d\|\leq D$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов длины $D$},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j = \{x_j^1, \ldots, x_j^D\} \subset \XX$, для всех $j = 1, \ldots, h$,
    а~так~же множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что:
    \begin{enumerate}
        \item[1)] набор $\bigl\{ U_0, U_1,\{X_j\}_{j=1}^h \bigr\}$ является разбиением множества~$\XX$
              на~непересекающиеся подмножества;
        \item[2)] $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item[3)] $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item[4)] $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Definition}

Обозначим $|U_1| = m$.
Из определения следует, что $n(a_{\vec d}, \XX) = m + |d|$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в~сетке}.
Множество алгоритмов с~равным числом ошибок $t+m \brop= n(a_{\vec d}, \XX)$ называются \emph{$t$-слоем} сетки.

\begin{Example}
    Монотонная двумерная сетка при $m = 0$ и~$L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}



\begin{Definition}
Пусть $\kappa \in \NN$ "--- целочисленный параметр;
${A = \{a_{\vec d}\}}$ "--- $h$"~мерная монотонная сетка длины $\kappa D$;
$m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной монотонной сеткой $\ddot{A}$ разреженности~$\kappa$ и~длины $D$}
будем называть подмножество $A$, заданное условием:
\[
\begin{aligned}
    \ddot{A} =
        %\left\{a_{\vec d} \in A \, \big| \, \forall j = 1, \ldots, h \; \exists d'_j \in \ZZ \colon d_j = \kappa d'_j \right\}.
        \bigl\{a_{\vec d} \in A \, \big| \, \vec d \in \left( \tiny \kappa \ZZ \tiny \right)^h \bigr\}.
\end{aligned}
\]
\end{Definition}

Отметим, что при $\kappa > 1$ граф смежности разреженной монотонной сетки состоит из~изолированных точек.
Следовательно, для всех алгоритмов семейства $u(a) = q(a) = 0$, и оценка расслоения-связности вырождается.

\todo{отразить картинку по вертикали и переделать её векторно}
\begin{Example}
На~рисунке \ref{fig:sparseMonotonicNetExample}
выделено подмножество двумерной монотонной сетки с~параметром $D = 8$,
соответствующее разреженной монотонной сетке с~параметрами $\kappa = 2$, $D = 4$.
\begin{figure}[h]
    \label{fig:sparseMonotonicNetExample}
    \begin{centering}
    \includegraphics[height=64mm]{monot_net_rarefield.eps}
    \caption{Двумерная разреженная монотонная сетка при $\kappa = 2$, $D = 4$.}
    \end{centering}
\end{figure}
\end{Example}

Тем не менее, для разреженной монотонной сети по-прежнему можно ввести систему
порождающих и запрещающих множеств.

\section{Слабое замыкание семейства алгоритмов}

\section{Общая оценка расслоения-связности для разреженных семейств алгоритмов}

\section{Оценка для случая пересекающихся рёбер}
В данном параграфе мы рассмотрим альтернативный подход к учёту
рёбер диаграммы Хассе в оценке расслоения-связности  - прямое
вычисление оценки для случая пересекающихся рёбер.
Обозначим через $X_{u(a)} = \bigcup_{b \in I_a} X_{ab}$ - множество объектов, принадлежащих исходящим из $a$ рёбрам.

Перепишем условие (\ref{randomizedlearn}) в виде, необходимом для применения
теоремы \ref{th1}. Пусть $R(a) = \{r | r \subset X_u(a), \forall b \in I_a, X_{ab} \cap r \neq \emptyset \}$
\begin{equation}
    \label{randomizedlearn}
    \bigl[ \mu X{=}a \bigr]
    \leq \sum_{r \in R(a)}
    \bigl[ X_q(a) \cup (R(a)\backslash r) \subseteq  \X \bigr]
    [r \subseteq X]
\end{equation}
Тогда в обозначениях теоремы \ref{th1}
\begin{align*}
    L_{a r} &= L - |X_u(a)| - q(a);
\\
    \ell_{a r} &= \ell - |r|;
\\
    m_{a r} &= n(a, \XX) - q(a);
\\
    s_{a r}(\eps) &= s_a
\\
    P_{a r} &= \frac{C_{L_{a r}}^{\ell_{a r}}}{C_L^{\ell}}
\end{align*}
Запишем выражение для вклада одного алгоритма в оценку переобучения:
\begin{equation}
\sum_{r \in R(a)}
       P_{a r}
        \Hyper{L_{a r}}{m_{a r}}{\ell_{a r}}{s_{a r}(\eps)}
\end{equation}
Заметим, что параметры   $L_{a r}, \ell_{a r}, m_{a r}, s_{a r}, P_{a r}$
зависят только от мощности множества r, поэтому выражение можно переписать,
сгруппировав слагаемые с одинаковой мощностью $|r|$:
\begin{equation}
\sum_{v = 0}^{|X_u(a)|}
        T(v)
        \frac{C_{L - |X_u(a)| - q(a)}^{\ell - v}}{C_L^{\ell}}
        \Hyper{L - |X_u(a)| - q(a)}{n(a, \XX) - q(a)}{\ell - v}{s_a(\eps)}
\end{equation}
,где $T(v) = \#\{r | r \in R(a), |r| = v\}$.

Задача, вычисления $T(v)$, вообще говоря, NP-трудна(она является обобщением задачи о покрытии множества), но
эксперименты показывают, что число  исходящих рёбер неединичной мощности у алгоритма обычно невелико, поэтому для
вычисления $T(v)$ можно применять практически любой алгоритм.
Мы предлагаем следующий алгоритм:

Пусть $x_1, \dots , x_{|X_u(a)|}$ - пронумерованные произвольным образом объекты из $X_u(a)$.
Пусть $T(v, n, \alpha)$ - число подмножеств мощности $v$ из объектов $x_1, \dots, x_n$, покрывающих множество рёбер
$\alpha \subset I(a)$. Тогда для $T(v, n, \alpha)$ справедлива рекуррентная формула:
\begin{equation}
    T(v, n, \alpha \cup \beta_n) = T(v, n - 1, \alpha \cup \beta_n) + T(v - 1, n - 1, \alpha),
\end{equation}
где $\beta_n = \{b | b \in I_a, x_n \in X_{a b} \}$.

Используя эту рекуррентную формулу можно рассчитать $T(v) \equiv T(v, |X_u(a)|, I(a))$ для всех значений $v$

\section{Численный эксперимент}
В данном параграфе рассматривается вопрос практической применимости полученных выше оценок вероятности переобучения.
Оценка теоремы \eqref{theorem5eq} применима лишь к слабо замкнутому множеству алгоритмов,
поэтому в~первую очередь требуется сравнить вероятности переобучения исходного множества алгоритмов $A$ и его слабого замыкания $A^*$, построенного алгоритмом \ref{weakClosureAlg}.

Отметим, что слабая замкнутость множества алгоритмов необходима лишь для учёта верхней связности.
Положив $u(a) = 0$ и $w_a = 0$ в оценке теоремы \ref{theorem5eq} мы получим новую оценку, которая учитывает лишь неполноценность $q(a)$ каждого алгоритма.
Такая оценка будет менее точной, но в то же время она применима к произвольному множеству алгоритмов.
Следовательно, необходимо сравнить два эффекта: увеличение вероятности переобучения при слабом замыкании множества $A$, и улучшение оценки при учете верхней связности.
Так же представляет интерес сравнение оценки \ref{theorem5eq} и простой оценки, полученной с помощью неравенства Буля:
\begin{align*}
    Q_\eps(A) &\leq
        \sum_{a \in A}
            \Hyper{L}{m}{\ell}{\tfrac{\ell}{L} \left(m - \eps k\right)},
\end{align*}
где $m = n(a, \XX)$ "--- число ошибок алгоритма $a$ на полной выборке.

Для проведения эксперимента было выбрано $8$ задач из репозитория UCI.
К каждой из восьми задач применялось четыре метода поиска логических закономерностей, реализованных в библиотеке Forecsys-LogicPro:
случайный поиск ($RND$),
случайный поиск с адаптацией весов признаков ($RSA$),
генетический алгоритм ($sGA$)
и поиск правил путём отбора признаков алгоритмом $TEMP$.
Каждый метод поиска в процессе своей работы генерирует большое количество логических правил и вычисляет для них информативность.
Следует подчеркнуть, что взаимодействие метода поиска с обучающей выборкой происходит только в момент вычисления информативности.
Поэтому представляется разумным исследовать вероятность переобучения лишь для \emph{наблюдаемой части семейства } "--- то есть для множества логических закономерностей, сгенерированных методом поиска в процессе своей работы. Все прочие логические закономерности предлагается объявить ненаблюдаемыми, и исключить из рассмотрения.

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=72mm,height=48mm]{ErrorProfile_Liverdis.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=72mm,height=48mm]{ErrorProfile_Echocard.eps}
    \hfill
    \end {multicols}
    \caption{Распределение алгоритмов по числу ошибок,
    задача Liver Disorders (слева) и Echo Cardiogram (справа).}
    \label{fig2}
\end{figure}
\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \includegraphics[width=72mm,height=48mm]{Comparison_LiverDisordersRSA.eps}
    \hfill
    \medskip
    \hfill
    \includegraphics[width=72mm,height=48mm]{Comparison_EchocardRSA.eps}
    \hfill
    \end {multicols}
    \caption{Сравнение 0.5-квантили различных оценок вероятности переобучения,
    задача Liver Disorders (слева) и Echo Cardiogram (справа). Пунктирная кривая
    на левом рисунке соответствует семейству, полученному случайным перемешиванием
    векторов ошибок в $A^*_r$.}
    \label{fig3}
\end{figure}

Ошибкой логических закономерностей считалось как покрытие объекта противоположного класса, так и непокрытие объекта своего класса.
На~рис.\,\ref{fig2} приведены профили расслоения полученных семейств.
Очевидно, что при минимизации числа ошибок на обучении вероятность выбрать алгоритм в результате обучения быстро падает с ростом числа его ошибок.
Поэтому в данном эксперименте рассматривались подмножества $A_r = \{a \in A \colon n(a, \XX) \leq m_0 {+} r\}$, где $m_0  = \min\limits_{a \in A} n(a, \XX)$.
Значения $r$ перебирались от $0$ до $50$.
Для каждого $A_r$ строилось внутреннее замыкание $A_r^*$ и вычислялась $0.5$-квантиль распределения вероятности переобучения,
построенного методом Монте-Карло (по $10000$ случайным разбиениям выборки на обучения и контроль).
Также вычислялась $0.5$-квантиль распределения, полученного по формулам расслоения связности --- с учетом и без учета верхней связности.

Результаты сравнения приведены на~рис.\,\ref{fig3}. Во-первых, видно что внутреннее замыкание множества алгоритмов может вести себя принципиально по-разному.
Так, для задачи Liver Disorder вероятности переобучения $A_r$ и $A^*_r$ практически совпадают при всех значениях $r$.
Для задачи Echo Cardiogram вероятность переобучения $A^*_r$ оказывается заметно выше.
Оценки вероятности переобучения также ведут себя по-разному.
Для задачи Liver Disorder учёт связности не даёт улучшения по сравнению с оценкой Буля.
Однако на задаче Echo Cardiogram улучшение становится очевидным.

Завышенность оценки \ref{theorem5eq} в задаче Liver Disorders можно объяснить плохим учетом связности (при малых $r$) и расслоения (при больших $r$).
На левом рисунке пунктиром изображена кривая, соответствующая семейству с $A^*_r$ со случайно переставленными ошибками.
Данная процедура разрушает связность между алгоритмами. Видно, что при малых значениях $r$ данная кривая хорошо приближает оценки теоремы \ref{theorem5eq}.
Плохой учет связности возникает из-за того, что в оценке рассматриваются лишь пары алгоритмов с вложенными векторами ошибок.
Это, в частности, не позволяет учитывать связи между алгоритмами с равным числом ошибок.
В то же время, теоретически доказано что эффект связности имеет место и в этом случае.
В работах Ильи Толстихина и Александра Фрея была показана принципиальная разница между поведением вероятности переобучения для двух модельных семейств:
максимально-компактного (центральное сечение шара), и максимально-разреженного (случайные подмножества слоя).

Вместе с тем, при больших значениях $r$ оценка для перемешенного $A^*_r$ стремится к горизонтальной асимптоте, в то время как оценка расслоения-связности продолжает расти.
Это можно объяснить недостаточным учетом эффекта расслоения. В работах Евгения Соколова показано, что для произвольной пары алгоритмов $a, b$ с различным числом ошибок алгоритм $a$ с меньшим числом
всегда уменьшает вероятность алгоритма $b$. Данный эффект наблюдается даже в тех случаях, когда вектор ошибок $a$ не вложен в вектор ошибок $b$.

\todo{Причесать текст и добавить замечание о высокой толерантности обращения оценки к ошибке по сравнению с непосредственной оценкой среднего числа ошибок на контроле.}

%$P(\delta(x, A_n) > \eps) \leq n \Hyper{L}{m}{\ell}{s(\eps)} \simeq n e^{- \lambda \eps}.$
%Приравнивая правую часть к единице, получим $\eps \simeq \frac{\ln n}{\lambda}$.

\section{Заключение}
\todo{Добавить заключение. Краткие выводы:}
\begin{itemize}
  \item Предложен способ учёта верхней связности, основанный на слабом замыкании множества алгоритмов;
  \item Показано, что учёт верхней связности может оказывать существенное влияние на качество оценки;
  \item Показано, что в ряде случаев оценки всё еще остаются завышенными. Проанализированы возможные причины завышенности и предложены дальнейшей способы борьбы с ними.
\end{itemize}

\def\BibAuthor#1{\emph{#1}}
\def\BibTitle#1{#1}
\def\BibUrl#1{{\small\url{#1}}}
\def\BibHttp#1{{\small\url{http://#1}}}
\def\BibFtp#1{{\small\url{ftp://#1}}}
\def\typeBibItem{\small\sloppy}

%\begin{thebibliography}{1}

%\end{thebibliography}


\end{document}
