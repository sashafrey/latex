\documentclass[11pt,a5paper,twoside]{report}
\usepackage{dissertation}
\usepackage{setspace}

%210 x 148
\textheight=173mm
\textwidth=113mm
\oddsidemargin=-5mm
\evensidemargin=-10mm
%\topmargin=-12mm
\singlespacing
%\textheight=179mm
%\textwidth=12cm
%\parindent=15pt
%\oddsidemargin=20mm
%\evensidemargin=20mm
\topmargin=-22mm
%\flushbottom
%\renewcommand{\baselinestretch}{1.15} %для печати с~большим интервалом
%\singlespacing

\fancypagestyle{plain}{%redefining plain pagestyle
    \fancyhf %clear all headers and footers fields
    \fancyhead{}
    \fancyfoot{}
    \renewcommand{\headrulewidth}{0pt}
    \fancyhead[C]{\footnotesize\thepage} %prints the page number on the right side of the header
    \headheight 25pt
    \headsep 5pt
}

\makeatletter
%\renewcommand{\@oddfoot}{\hfil\footnotesize\thepage\hfil}%
%\renewcommand{\@oddhead}{}%
%\renewcommand{\@evenfoot}{\hfil\footnotesize\thepage\hfil}%
%\renewcommand{\@evenhead}{}

\renewcommand\section{\@startsection{section}{1}{\z@}%
    {1ex \@plus 0.5ex}%
    {2.3ex \@plus .2ex}%
    {\normalfont\Large\sffamily\bfseries}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
    {1ex \@plus 0.5ex}%
    {1.5ex \@plus .2ex}%
    {\normalfont\large\sffamily\bfseries}}
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
    {0ex\@plus0.3ex}%
    {-1ex\@plus-.5ex}%
    {\normalfont\normalsize\sffamily\bfseries}}
\makeatother

\newcommand{\TODO}[1]{\textsf{#1}}

\begin{document}
\pagestyle{plain}
\begin{titlepage}
\begin{center}
%    {\small
%    РОССИЙСКАЯ АКАДЕМИЯ НАУК\\
%    ВЫЧИСЛИТЕЛЬНЫЙ ЦЕНТР ИМ. А. А. ДОРОДНИЦЫНА РАН
%    }
    \begin{flushright}\sl
        На правах рукописи

    ~~~\raisebox{0pt}[0pt][0pt]{\raisebox{-45pt}{\hbox{\includegraphics[width=25mm]{Pictures/alfrey_gs.eps}}}}
    \end{flushright}


    \vspace{25mm}
    ФРЕЙ АЛЕКСАНДР ИЛЬИЧ\\[1cm]

    {\large\bfseries
        ТЕОРЕТИКО-ГРУППОВОЙ ПОДХОД \\ В КОМБИНАТОРНОЙ ТЕОРИИ ПЕРЕОБУЧЕНИЯ
    }\\[1cm]

    05.13.17 --- теоретические основы информатики\\[1cm]
    Автореферат диссертации на~соискание ученой степени\\
    кандидата физико-математических наук\\[2cm]

    \vspace{\fill}
    Москва --- 2013
\end{center}
\end{titlepage}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize
{\parskip=1em\parindent=0cm\small
% \fontsize{10.47}{1.2em}\selectfont

Работа выполнена на кафедре <<Интеллектуальные системы>> факультета управления
и прикладной математики Федерального государственного образовательного учреждения
высшего профессионального образования
<<Московский физико–технический институт (государственный университет)>>.

\def\MYruf#1{\hbox to #1{\hrulefill}}
%\bigskip

\textbf{Научный руководитель:}
доктор физико-математических наук, профессор \textbf{Воронцов Константин Вячеславович}.

\textbf{Официальные оппоненты:} \\
доктор физико-математических наук,
\textbf{Хачай Михаил Юрьевич},
Федеральное государственное бюджетное
учреждение науки Институт математики
и механики им.~Н.~Н.~Красовского
Уральского отделения Российской академии наук,
зав.~отделом математического программирования;\\
кандидат технических наук, \textbf{Игнатов Дмитрий Игоревич},
Федеральное государственное автономное
образовательное учреждение высшего
профессионального образования
Национальный исследовательский университет
<<Высшая школа экономики>>, доцент.

\textbf{Ведущая организация:}
Федеральное государственное бюджетное
учреждение науки Институт проблем передачи
информации им.~А.~А.~Харкевича
Российской академии наук.

Защита диссертации состоится 19 декабря 2013~г.
в~14:30 на~заседании диссертационного совета Д\;002.017.02
при~Федеральном государственном бюджетном учреждении
<<Вычислительный центр им.~А.~А.~Дородницына Российской академии наук>>,
расположенном по~адресу: 119333, г.~Москва, ул.~Вавилова, д.~40.\\
С~диссертацией можно ознакомиться в~библиотеке ВЦ~РАН.

Автореферат разослан 11 ноября 2013~г.

\vspace{\fill}

Ученый секретарь диссертационного совета
~~~\raisebox{0pt}[0pt][0pt]{\raisebox{-60pt}{\hbox{\includegraphics[width=35mm]{Pictures/ryazanov.eps}}}}
\\
Д 002.017.02, д.ф.-м.н., профессор \hfillРязанов В.В.}

}
\thispagestyle{empty}
\setcounter{page}{2}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Общая характеристика работы}
Диссертационная работа посвящена проблеме повышения точности комбинаторных оценок вероятности переобучения.

\paragraph{Актуальность темы.}

При решении задач обучения по~прецедентам, восстановления зависимостей по~эмпирическим данным,
классификации, распознавания образов, прогнозирования часто возникает проблема переобучения.
Она состоит в~том, что решающая функция (алгоритм),
построенная по~конечной обучающей выборке,
может допускать ошибки на~объектах контрольной выборки существенно чаще,
чем на~объектах обучающей выборки.
Для контроля переобучения на~этапе построения алгоритма
необходимо иметь оценки вероятности переобучения.
Такие оценки известны в~статистической теории обучения,
однако они либо сильно завышены,
либо имеют слишком узкую область применимости.

\paragraph{Степень разработанности темы.}
%Данный параграф необходим согласно ГОСТу авторефератов и~диссертаций.

Основы статистической теории обучения были заложены
в~работах В. Н. Вапника и~А. Я. Червоненкиса в~конце 60-х годов.
Ими была доказана состоятельность обучения по~прецедентам
и~получены количественные оценки,
связывающие обобщающую способность метода обучения
с~длиной обучающей выборки и~сложностью семейства алгоритмов.
Основной проблемой этих оценок является их завышенность.
Для устранения завышенности предлагалось
    строить оценки, зависящие от~выборки (D.~Haussler, 1992);
    учитывать ширину зазора, разделяющего классы (P.~Bartlett, 1998);
    строить оценки на~основе локальной радемахеровской сложности семейства алгоритмов (V.~Koltchinskii, 1998);
    учитывать априорные распределения на~множестве алгоритмов (L.~Valiant, 1982; D.~McAllester, 1999; J.~Langford, 2005);
а~также ряд других подходов.

Комбинаторная теория переобучения
показала, что для повышения точности оценок
и~сокращения переобучения необходимо одновременно учитывать
эффекты расслоения и~сходства в~семействах алгоритмов (К.~В.~Воронцов, 2010).
Была получена оценка расслоения"=связности,
справедливая для широкого класса семейств,
представимых в~виде связного графа (К.~В.~Воронцов, А.~А.~Ивахненко, И.~М.~Решетняк, 2010).
Для некоторых модельных частных случаев было показано,
что этого достаточно для получения неулучшаемых (точных) оценок.
Таким образом, комбинаторная теория переобучения является новым перспективным подходом.
Данная работа направлена на~ее дальнейшее развитие:
расширение границ применимости,
разработку новых методов вывода оценок обобщающей способности и
повышение точности этих оценок.

\paragraph{Цели и~задачи работы:}
повышение точности комбинаторных оценок вероятности переобучения;
переход от~требования связности к~более слабому требованию сходства алгоритмов;
разработка новых методов получения оценок обобщающей способности,
применимых к~несвязным семействам алгоритмов высокой мощности.
%в рамках комбинаторной теории переобучения.

\paragraph{Научная новизна.}
Впервые получены неулучшаемые оценки вероятности переобучения для рандомизированного метода минимизации эмпирического риска.
Для их получения разработан новый теоретико"=групповой подход,
основанный на~учете симметрий множества алгоритмов.
С~его помощью получены неулучшаемые оценки вероятности переобучения
для девяти модельных семейств алгоритмов.
Получена комбинаторная оценка вероятности переобучения,
основанная на~разложении множества алгоритмов на~непересекающиеся подмножества (кластеры).
Каждый кластер пополняется алгоритмами до объемлющего множества алгоритмов
с~известной точной оценкой вероятности переобучения.
Итоговая оценка учитывает сходство алгоритмов внутри каждого кластера
и~расслоение алгоритмов по~числу ошибок между разными кластерами.
Данная оценка применима к~широкому классу семейств,
в~том числе и~к~семействам,
не~обладающим свойством связности.

\paragraph{Теоретическая и~практическая значимость.}
Данная работа вносит существенный вклад в~развитие комбинаторной теории переобучения
и~расширяет границы ее применимости на~несвязные семейства алгоритмов высокой мощности.

\paragraph{Методы исследования.}
Для получения оценок вероятности переобучения
использована слабая (перестановочная) вероятностная аксиоматика,
комбинаторная теория переобучения,
элементы комбинаторки, теории групп, теории вероятностей и~теории графов.
Для проверки точности комбинаторных оценок
проведены вычислительные эксперименты на~модельных данных и~задачах из~репозитория UCI.

\paragraph{Положения, выносимые на~защиту.}
\begin{enumerate}
\item
    Теоретико"=групповой метод орбит,
    позволяющий выводить оценки вероятности переобучения
    для симметричных семейств алгоритмов
    и~рандомизированного метода минимизации эмпирического риска.
\item
    Точные оценки вероятности переобучения рандомизированного метода минимизации эмпирического риска
    для модельных семейств: монотонной и~унимодальной сетей, слоя хэммингова шара и~ряда других.
\item
    Общая оценка вероятности переобучения, основанная на~разложении и~покрытии множества алгоритмов.
\item
    Экспериментальное подтверждение того, что новая оценка в~некоторых случаях менее завышена
    по~сравнению с~другими комбинаторными оценками вероятности переобучения.
\end{enumerate}

\paragraph{Степень достоверности и~апробация работы.}
Достоверность результатов подтверждена математическими доказательствами,
экспериментальной проверкой полученных оценок переобучения на~реальных задачах классификации;
публикациями результатов исследования в~рецензируемых научных изданиях, в~том числе рекомендованных ВАК РФ.
Результаты работы докладывались, обсуждались и~получили одобрение специалистов на~следующих научных конференциях~и семинарах:

\begin{itemize}
\item
    Всероссийская конференция <<Математические методы распознавания образов>>
    ММРО-14, 2009~г.~\cite{frey09mmro};
\item
    52"~я научная конференция МФТИ,
    2009~г.~\cite{frey09mipt};
\item
    Международная конференция <<Интеллектуализация обработки информации>>
    ИОИ-8, 2010~г.~\cite{frey10iip};
\item
    Всероссийская конференция <<Математические методы распознавания образов>>
    ММРО-15, 2011~г.~\cite{frey11mmro};
\item
    Международная конференция <<25th European Conference on Operational Research>>, 2012~г.
\item
    Международная конференция <<Интеллектуализация обработки информации>>
    ИОИ-9, 2012~г.~\cite{frey12iip};
\item
    Научные семинары отдела Интеллектуальных систем Вычислительного центра РАН и~кафедры
    <<Интеллектуальные системы>> МФТИ, 2010 -- 2013 г.г.
\end{itemize}

\paragraph{Публикации по~теме диссертации}
в~изданиях из~списка ВАК РФ "--- одна~\cite{frey10pria}.
Другие публикации по~теме диссертации:
\cite{frey09mmro,frey09mipt,frey10iip,frey11mmro,frey12iip,voron12jmlda-eng,frey12jmlda}.
%Отдельные результаты включались в~отчеты по~проектам X, Y, Z.
%Всего публикаций по~теме диссертации "---~$X$, в~том числе в~изданиях из~Списка, рекомендованного ВАК РФ "--- одна~\cite{frey10pria}.

\paragraph{Структура и~объем работы.}
Работа состоит из
введения,
пяти глав,
заключения,
%списка обозначений,
%предметного указателя,
%списка таблиц~(??~пунктов),
списка использованных источников, включающего 78~наименований.
Общий объем работы составляет 102~страницы.

\section*{Краткое содержание работы по~главам}

% В автореферате сохранена нумерация разделов и~утверждений
%(аксиом, гипотез, определений, лемм, теорем и~их следствий),
%принятая в~тексте работы. Нумерация формул сквозная.

\subsection*{Глава~1. Теория статистического обучения}
\refstepcounter{chapter}

Глава содержит краткий обзор современного состояния
теории статистического обучения (statistical learning theory).
Обсуждается проблема завышенности классических оценок переобучения,
приводятся мотивации перехода от~классических
теоретико"=вероятностных постановок задач к~комбинаторным.

\subsection*{Глава~2. Комбинаторный подход}
\refstepcounter{chapter}

\paragraph{2.1. Основные определения.}
Пусть задана конечная генеральная выборка $\XX=\{ x_1, \ldots, x_L\}$, состоящая из~$L$ объектов.
Пусть~$A$ "--- некоторое множество алгоритмов.
Каждый алгоритм классификации $a \in A$, примененный к~выборке $\XX$, порождает бинарный вектор
ошибок $a \equiv \bigl( I(a, x_i) \bigr){}_{i=1}^L$,
где $I(a, x_i) \in \{0, 1\}$ "--- индикатор ошибки алгоритма~$a$ на~объекте $x_i$.
Для произвольной подвыборки $X \subset \XX$
обозначим через
$n(a,X) = \sum\limits_{x\in X}I(a,x)$
число ошибок, а~через
$\nu(a,X) = n(a,X) / |X|$ "---
частоту ошибок алгоритма~$a$ на~выборке~$X$.

\emph{Методом обучения} называют отображение вида $\mu \colon 2^\XX \rightarrow A$.
Результатом обучения по~выборке $X \subset \XX$ называется алгоритм $a = \mu(X)$,
обозначаемый также $a = \mu X$ или $a = \mu(A, X)$.

Метод обучения~$\mu$ называется \emph{минимизацией эмпирического риска} (МЭР), если
\[
    \mu X \in A(X) = \Argmin_{a\in A} n(a,X),
\]
и~методом \emph{пессимистической минимизации эмпирического риска} (ПМЭР), если
\[
    \mu X \in \Argmax_{a\in A(X)} n(a,\XX).
\]

Обозначим через~$\XXell$ множество всех разбиений генеральной выборки~$\XX$
на~обучающую выборку~$\Xl$ длины~$\ell$ и~контрольную выборку~$\Xk$ длины $k=L-\ell$.
Если для выборки $X \in \XXell$ \emph{уклонение частот} $\delta(a, X) = \nu(a,\X) - \nu(a, X)$
превосходит фиксированный порог $\eps > 0$,
то~говорят, что алгоритм $a = \mu X$ является \emph{переобученным}.
Нашей целью является получение оценок \emph{вероятности переобучения}:
\begin{equation}
    \label{eq:QEps}
    Q_\eps (\mu, \XX)
    =
    \Prob\bigl[
        \delta(\mu X, X) \geq \eps
        %\nu(\mu X,\X) - \nu(\mu X, X) \geq \eps
    \bigr]
    \leq \eta(\eps),
    \text{ где }
    \Prob \equiv \frac{1}{\CLl}\sum_{X \in \XXell},
\end{equation}
где квадратные скобки означают $[\text{истина}] = 1$, $[\text{ложь}] = 0$.

Коль скоро такая оценка получена, можно утверждать, что
$\nu(\mu X,\X) \leq \nu(\mu X,X) + \eps(\eta)$
с~вероятностью $1-\eta$, достаточно близкой к~единице,
где $\eps(\eta)$ "--- функция, обратная к~$\eta(\eps)$.

\paragraph{2.2. Расслоение и~связность.}
%Метод порождающих и~запрещающих множеств,
%предложенный К.\,В.\,Воронцовым,
%основан на~гипотезе, что для любого алгоритма~${a\in A}$
%можно записать необходимое и~достаточное условие того,
%что он~будет выбран методом обучения~$\mu$ по~выборке~$X$.

%%Пусть $\XX= \{x_1,\ldots,x_L\}$
%%множество~$A$ состоит из~алгоритмов~$a$ с~попарно различными
%%\emph{векторами ошибок}
%%$\bigl( I(a,x_i) \bigr){}_{i=1}^L$.
%
%\begin{Hypothesis}
%\label{hyp3}
%    Множество~$A$, выборка~$\XX$ и~метод~$\mu$ таковы, что
%    для каждого алгоритма ${a\in A}$ можно указать два множества:
%    \emph{порождающее}~${X_{a}\subset\XX}$ и
%    \emph{запрещающее}~${X'_{a}\subset\XX}$ такие, что
%    \begin{equation}
%    \label{eq:hyp3}
%        \bigl[ \mu X{=}a \bigr]
%        \leq
%        \bigl[  X_{a}\subseteq  X \bigr]
%        \bigl[ X'_{a}\subseteq \X \bigr].
%    \end{equation}
%\end{Hypothesis}
%
%Введем гипергеметрическое распределение
%$\hyper{L}{m}{\ell}{s} = \frac{C_m^s C_{L-m}^{\ell-s}}{C_L^\ell}$
%и~его функцию распределения
%$\Hyper{L}{m}{\ell}{z} = \sum\limits_{s=0}^{\lfloor z \rfloor} \hyper{L}{m}{\ell}{s}$.
%
%\begin{Theorem}
%    Если гипотеза~\ref{hyp3} верна,
%    то~для любого $\eps\in (0,1)$
%    справедлива верхняя оценка вероятности переобучения
%    \begin{equation}
%    \label{eq:th3}
%        Q_\eps
%        \leq
%        \sum_{a\in A}
%            P_a
%            \Hyper{L_a}{m_a}{\ell_a}{s_a(\eps)};
%        \quad
%        \Prob[\mu X{=}a]
%        \leq
%        P_a = \frac{C_{L_a}^{\ell_a}}{C_{L}^{\ell}},
%    \end{equation}
%    где
%    $L_a = L - |X_a| - |X'_a|$,\;
%    $\ell_a = \ell - |X_a|$,\;
%    $m_a = n(a,\XX {\setminus} X_a {\setminus} X'_a)$,\;
%    $s_a(\eps) = \tfrac\ell L \bigl( n(a,\XX)-\eps k \bigr) - n(a,X_a)$.
%\end{Theorem}
%
%%Величина $P_a$ является верхней оценкой вероятности $\Prob[\mu X{=}a]$
%%получить алгоритм~$a$ в~результате обучения.
%%Сумму $\sum\limits_{a\in A} P_a$
%%можно рассматривать как оценку завышенности неравенства~\eqref{eq:th3}.

Определим естественное отношение порядка на~алгоритмах:
${a\leq b}$ тогда и~только тогда, когда
${I(a,x) \leq I(b,x)}$ для всех ${x\in\XX}$.
Определим метрику на~алгоритмах
как хэммингово расстояние между их~векторами ошибок:
$\rho(a,b) = \sum\limits_{i=1}^L |I(a,x) - I(b,x)|$.
Определим отношение предшествования $a \prec b$ если
$a < b$ и~$\rho(a, b) = 1$.

%\begin{Lemma}
%\label{lemConstrDestr3}
%    Если $\mu$ "--- метод пессимистичной минимизации эмпирического риска,
%    то~\eqref{eq:hyp3} выполнено, если положить
Для каждого $a \in A$ рассмотрим \emph{порождающее} и~\emph{запрещающее} множества:
    \begin{align*}
        X_a &=
        \bigl\{
            x\in \XX \bigm| \exists b\in A\colon I(a,x)<I(b,x),\; a\prec b
        \bigr\},
    \\
        X'_a &=
        \bigl\{
            x\in \XX \bigm| \exists b\in A\colon I(b,x)<I(a,x),\; b\leq a
        \bigr\}.
    \end{align*}
%\end{Lemma}
Величину $u(a)=|X_a|$ называют \emph{верхней связностью},
а~величину $q(a)=|X'_a|$ "--- \emph{неполноценностью} алгоритма~$a$.

\begin{Theorem}[Воронцов, Ивахненко, Решетняк, 2010]
\label{th:SC-bound}
    Если $\mu$ "--- метод пессимистичной минимизации эмпирического риска,
    то~для вероятности переобучения справедлива оценка
    %\begin{equation}
    %\label{eq:split-connect}
    \[
        Q_\eps(\mu,\XX)
        \leq
        \sum_{a\in A}
            \frac{C_{L-u(a)-q(a)}^{\ell-u(a)}}{C_{L}^{\ell}}
            \Hyper{L-u(a)-q(a)}{\:n(a,\XX)-q(a)}{\ell-u(a)}{\tfrac\ell L (n(a,\XX) - \eps k)},
    %\end{equation}
    \]
где $H_{L}^{\ell, m}(s) = \sum\limits_{i=0}^{\lfloor s \rfloor}\frac{\CC_m^i \CC_{L-m}^{\ell-i}}{\CC_L^\ell}$ "--- функция гипергеометрического распределения.
\end{Theorem}

%Если в~этой оценке положить ${u(a)=q(a)=0}$ для всех~$a$,
%то получится оценка, известная в~теории Вапника"=Червоненкиса:
%\begin{equation}
%    \label{eq:VCbound}.
%    Q_\eps (\mu,\XX)
%    \leq
%    \sum_{a\in A}
%        \Hyper{L}{n(a,\XX)}{\ell}{\tfrac\ell L (n(a,\XX) - \eps k)},
%\end{equation}
%частично учитывающая расслоение семейства алгоритмов по~числу ошибок~$n(a,\XX)$,
%и~совсем не~учитывающая связность.

\paragraph{2.3. Постановка задачи.}
Приводятся результаты двух экспериментов,
показывающих, в~каких случаях оценка теоремы~\ref{th:SC-bound} сильно завышена.
Эта оценка не~учитывает сходство между алгоритмами с~равным числом ошибок.
Поэтому первая задача, которая ставится в~данной работе "--- получить оценки,
одновременно учитывающие и~расслоение алгоритмов по~числу ошибок,
и~сходство алгоритмов внутри одного слоя.
Вторая задача "--- разработать удобный математический инструментарий,
позволяющий получать точные и~вычислительно эффективные оценки вероятности переобучения
для семейств, состоящих из~большого числа алгоритмов.

\subsection*{Глава~3. Теоретико"=групповой подход}
\refstepcounter{chapter}

\paragraph{3.1.~Рандомизированный метод обучения и~РМЭР.}
При минимизации эмпирического риска может возникать неоднозначность "---
несколько алгоритмов из~$A(\Xl) \equiv \Argmin_{a\in A} n(a,X)$ могут иметь одинаковое число ошибок на~обучающей выборке.
Следующий метод обучения естественным образом устраняет эту неоднозначность.

\begin{Definition}
Путь $\AA = \{0, 1\}^L$ "--- множество всех бинарных векторов ошибок.
\emph{Рандомизированный метод обучения} произвольному множеству алгоритмов $A\subseteq \AA$
и~произвольной обучающей выборке~$\Xl \in \XXell$ ставит в~соответствие функцию
распределения весов на~множестве~алгоритмов:
\begin{equation}
    \label{eq:randomizedSearchMethodDraft}
    \mu : 2^\AA \times \XXell \rightarrow \{f : \AA \rightarrow [0, 1]\}.
\end{equation}
Естественно полагать, что эта функция нормирована и~может быть интерпретирована как вероятность
получить каждый из~алгоритмов в~результате обучения.
\end{Definition}
Методы обучения вида $\mu \colon 2^\XX \rightarrow A$
далее будем называть \emph{детерминированными}.

\emph{Рандомизированный метод минимизации эмпирического риска (РМЭР)}
выбирает произвольный алгоритм из~множества~$A(X)$ случайно и~равновероятно:
\[
    \mu(A, X)(a) = \frac {[a\in A(X)]}{|A(X)|}.
\]

Для РМЭР определение вероятности переобучения~$Q_\eps(A)$ соответствующим образом модифицируется:
%Наиболее естественный вариант модификации "--- усреднение по~множеству~$A(X)$:
\begin{equation}
\label{QepsRERM.sym}
    Q_\eps(A)
    =
    \Expect
    \frac1{|A(X)|} \sum_{a\in A(X)}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr],
    \text{ где }
    \Expect \equiv \frac{1}{\CLl}\sum_{X \in \XXell}.
\end{equation}
Также возможна запись $Q_\eps(A) = \sum\limits_{a \in A} Q_\eps(a, A)$,
где $Q_\eps(a,  A)$ "--- \emph{вклад алгоритма}~$a \in A$ в~вероятность переобучения:
\[
    Q_\eps(a,A)
    =
    \Expect
    \frac{\bigl[a\in A(X)\bigr]}{|A(X)|}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\]

\paragraph{3.2.~Перестановки объектов.}
Пусть $S_L = \{ \pi \colon \XX \rightarrow \XX \}$ "--- симметрическая группа из~$L!$ элементов,
действующая на~генеральную выборку перестановками объектов.
Действие произвольной перестановки $\pi \in S_L$ на~алгоритм $a \in A$ определено перестановкой координат вектора
ошибок: $(\pi a)(x_i)= a(\pi^{-1}x_i)$.
Для произвольной выборки $X \in \XXell$ и~множества алгоритмов $A \subset \{0, 1\}^L$ действия
$\pi X$ и~$\pi A$ определены следующим образом:
$\pi X = \{ \pi x \colon x \in X \}$, $\pi A = \{ \pi a \colon a \in A\}$.

\begin{Lemma}
\label{lem:piSL}
    Свойства действия произвольной перестановки ${\pi\in S_L}$:

    1) $I(\pi a, \pi x) = I(a,x)$ для любых $a\in A$ и~$x\in\XX$;

    2) $n(\pi a, \XX) = n(a,\XX)$ для любого $a\in A$;

    3) $n(\pi a, \pi X) = n(a, X)$ для любых $a\in A$ и~$X\subseteq\XX$;

    4) $\delta(\pi a, \pi X) = \delta(a, X)$ для любых $a\in A$ и~$X\subseteq\XX$;

    5) $[a \in A(X)] = [\pi a \in (\pi A)(\pi X)]$ для любых $a\in A$ и~$X\subseteq\XX$;

    6) $|A(X)| = |(\pi A)(\pi X)|$ для любых~$A$ и~$X\subseteq\XX$;

    7) $\rho(a, a') = \rho(\pi a, \pi a')$ для любых $a, a' \in A$. %, где
%       $\rho(a, a')$ "--- расстояние Хэмминга векторами ошибок алгоритмов~$a$ и~$a'$:
%\[
%    \rho(a, a') = \sum \limits_{x \in \XX} |I(a, x) - I(a', x)|.
%\]
\end{Lemma}

Для построения функций, инвариантных относительной действия $S_L$,
введем следующую классификацию функций:
\begin{itemize}
  \item Симметричной функцией \emph{первого рода} будем называть $g \colon \AA \times \XXell \rightarrow \RR$, такую, что для всех $\pi \in S_L$ выполнено $g(a, X) = g(\pi a, \pi X)$;
  \item Симметричной функцией \emph{второго рода} будем называть $G \colon 2^\AA \times \XXell \rightarrow 2^\AA$, такую, что для всех $\pi \in S_L$ выполнено $\pi G(A, X) = G(\pi A, \pi X)$;
  \item Симметричной функцией \emph{третьего рода} будем называть $f \colon 2^\AA \times \XXell \rightarrow \RR$, такую, что для всех $\pi \in S_L$ выполнено $f(A, X) = f(\pi A, \pi X)$.
\end{itemize}
Лемма~\ref{lem:piSL} утверждает, что функции $n(a, X)$ и~$\nu(a, X)$ являются симметричными функциями первого рода, а~$A(X)$, как функция~$A$ и~$X$, является симметричной функцией второго рода.
\begin{Theorem}
\label{th:sym1}
    Пусть
        $g_1, g_2, \dots, g_p$ "--- симметричные функции первого рода,
        $f_1, f_2, \dots, f_p$ "--- симметричные функции третьего рода,
        $F \colon \RR^p \rightarrow \RR$ "--- произвольная функция многих переменных.
    Тогда
        $F(g_1, g_2, \dots, g_p)$ "--- вновь симметричная функция первого рода,
        $F(f_1, f_2, \dots, f_p)$ "--- симметричная функция третьего рода.
\end{Theorem}

\begin{Theorem}
\label{th:sym2}
    Пусть $g$ "--- симметричная функция первого рода, $G$ "--- симметричная функция второго рода.
    Тогда
    \[
        f(A, X) \equiv |G(A, X)| \text{ и~} f(A, X) \equiv \sum\limits_{a \in G(A, X)} g(a, X)
    \]
    являются симметричными функциями третьего рода.
\end{Theorem}

\paragraph{3.3.~Группа симметрии множества алгоритмов.}
\begin{Def}
    \emph{Группой симметрий} $\Sym(A)$ множества алгоритмов~$A$
    будем называть его стационарную подгруппу:
    \[
        \Sym(A) = \{\pi \in S_L \colon \pi A = A\}.
    \]
\end{Def}

Пусть далее $G \subseteq \Sym(A)$ "--- произвольная подгруппа группы~$\Sym(A)$.
Для любой перестановки ${\pi\in G}$ и~любого алгоритма ${a\in A}$
алгоритм~$\pi a$ снова лежит в~$A$.
В~таких случаях говорят, что группа~$G$ \emph{действует} на~множестве~$A$.

\emph{Орбитой} алгоритма~$a\in A$ называется множество алгоритмов
${Ga = \bigl\{ \pi a \colon \pi \in G \bigr\}}$.
Множество~$A$ разбивается на~непересекающиеся подмножества "--- орбиты:
\[
    A
    \;=\!\! \bigsqcup_{\omega\in\Omega(A)} \!\!\! \omega
    \;=\!\! \bigsqcup_{\omega\in\Omega(A)} \!\!\! Ga_\omega,
\]
где
$\Omega(A)$ "--- множество всех орбит в~$A$,
$a_\omega$ "--- произвольный представитель орбиты~$\omega$.

\begin{Lemma}
\label{lem:equalPQa}
    Алгоритмы из~одной орбиты имеют
    %равные вероятности реализации и
    равные вклады в~вероятность переобучения:
    \[
        %P(a,A) = P(\pi a,A),
        %\quad
        Q_\eps(a,A) = Q_\eps(\pi a,A) \; \text{ для всех } \pi \in G.
    \]
\end{Lemma}
\begin{Theorem}
\label{th:QAorbit}
    Для любой генеральной выборки $\XX$,
    любого множества алгоритмов~$A$ с~попарно различными векторами ошибок
    и~любого $\eps\in[0,1]$
    справедлива формула разложения вероятности переобучения по~орбитам множества~$A$:
    \begin{equation}
    \label{eq:QAorbit}
        Q_\eps(A)
        =
        \sum_{\omega\in\Omega(A)} \!\! |\omega| \:
            \Expect
            \frac{\bigl[ a_\omega \in A(X) \bigr]}{|A(X)|}
            \bigl[
                \delta(a_\omega,X) \geq \eps
            \bigr],
    \end{equation}
    где
    $\Omega(A)$ "--- множество всех орбит в~$A$,
    $a_\omega$ "--- произвольный представитель орбиты~$\omega$.
\end{Theorem}
По аналогии с~действием группы $G \subset \Sym(A)$ на~множестве алгоритмов
рассматривается действие $G$ на~множестве $\XXell$ всех разбиений выборки на~обучение и~контроль.
\begin{Theorem}[Толстихин, Фрей, 2010]
\label{th:QXorbit}
    В условиях теоремы~\ref{th:QAorbit}
    справедлива формула разложения $Q_\eps(A)$ по~орбитам множества~$\XXell$:
    \begin{equation}
    \label{eq:QXorbit}
        Q_\eps(A)
        =
        \frac1{\CLl}
        \sum_{\tau\in\Omega\XXell}
        \frac{|\tau|}{|A(X_\tau)|}
        \sum_{a\in A(X_\tau)}
        \bigl[\delta(a,X_\tau) \geq \eps \bigr],
    \end{equation}
    где
    $\Omega\XXell$ "--- множество всех орбит в~$\XXell$,
    $X_\tau$ "--- произвольный представитель орбиты~$\tau$.
\end{Theorem}
Теоремы~\ref{th:QAorbit} и~\ref{th:QXorbit}
являются основным инструментом для вывода оценок вероятности переобучения
симметричных семейств алгоритмов.
Оценки ~\eqref{eq:QAorbit} и~\eqref{eq:QXorbit}
являются точными равенствами и, следовательно, неулучшаемы.

\paragraph{3.4.~Покрытия множества алгоритмов.}
\begin{Lemma}
\label{lem:qDecomp}
Пусть множество алгоритмов~$A$ представлено в~виде разбиения $A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$.
Тогда для вероятности переобучения детерминированного метода ПМЭР справедлива верхняя оценка
\begin{equation}
    \label{eq:qDecomp}
    \Q(A) \leq \sum_{i=1}^t \Q(A_t).
\end{equation}
\end{Lemma}
Данная оценка аналогична неравенству Буля, широко используемому в~статистической теории обучения.
Преимущество данной леммы состоит в~том, что вместо суммирования по~всем алгоритмам $a \in A$ суммирование
производится по~подмножествам $A_1, \dots, A_t$,
называемых кластерами,
что позволяет сократить число слагаемых и~сделать оценку вычислительно эффективной.

Для вычисления $\Q(A_i)$ предлагается
дополнить каждое множество $A_i$ до множества $B_i \supset A_i$
с~известной точной оценкой вероятности переобучения
и~применить неравенство\footnotemark
\footnotetext{Толстихин~И.~О. Вероятность переобучения плотных и~разреженных
  семейств~алгоритмов
 //~Межд. конф. Интеллектуализация обработки информации
ИОИ-8.~---
М.:~МАКС Пресс, 2010.~---
С.~83--86.}
\[
    \Q(A_i) \leq \Q(B_i),
\]
справедливое для ПМЭР при условии равенства числа ошибок алгоритмов в~множестве~$B_i$.


%Следующая лемма позволяет вычислить $\Q(A_i)$,
%дополнив множества $A_i$ до множества $B \supset A_i$
%с известной точной оценкой вероятности переобучения.
%\begin{Lemma}[Толстихин, 2010]
%\label{le:LayerGr}
%Пусть $A_i, B$ "--- два множества алгоритмов,
%$A_i \subseteq B$,
%и все алгоритмы из~$B$ допускают равное число ошибок на~полной выборке.
%Пусть метод обучения является детерминированным методом МЭР.
%Тогда для всех $\eps > 0$ выполнено неравенство $\Q(A_i) \leq \Q(B)$.
%\end{Lemma}

\paragraph{3.5.~Теоремы о порождающих и~запрещающих множествах.}
Метод порождающих и~запрещающих множеств (ПЗМ),
предложенный К.\,В.\,Воронцовым,
основан на~гипотезе, что для любого алгоритма~${a\in A}$
можно записать необходимое и~достаточное условие того,
что он~будет выбран методом обучения~$\mu$ по~выборке~$X$.
В данном параграфе метод ПЗМ обобщается по~двум направлениям:
во-первых, на~случай разложения множества алгоритмов на~кластеры,
во-вторых, на~случай РМЭР.

\begin{Hypothesis}
\label{generalizedPZM}
Пусть множество алгоритмов~$A$ представлено в~виде разбиения на~непересекающиеся подмножества
$A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$.
Пусть выборка $\XX$ и~детерминированный метод обучения $\mu$ таковы,
что для каждого $i=1,\dots,t$ можно указать пару непересекающихся подмножеств $X_i \subset \XX$ и~$X'_i \subset \XX$,
удовлетворяющую условию
\[
    [\mu(A, X) \in A_i] \leq [X_i \subset X][X'_i \subset \X] \; \text{ для всех } X \in \XXell.
\]
Пусть, кроме этого, все алгоритмы $a \in A_i$ не~допускают ошибок на~$X_i$ и~ошибаются на~всех объектах из~$X'_i$.
\end{Hypothesis}
Множество $X_i$ будем называть \emph{порождающим}, множество $X'_i$ "--- \emph{запрещающим} для $A_i$.
Гипотеза~\ref{generalizedPZM} означает, что результат обучения может принадлежать $A_i$ только в~том случае, если
в~обучающей выборке~$X$ находятся все порождающие объекты и~ни одного запрещающего.
Все остальные объекты $\YY_i \equiv \XX \backslash X_i \backslash X'_i$ будем называть \emph{нейтральными} для~$A_i$.

Пусть $L_i = L - |X_i| - |X'_i|$, $\ell_i = \ell - |X_i|$, $k_i = k - |X'_i|$.
Обозначим через $Q'_\eps(A_i, \YY_i)$ вероятность переобучения на~множестве нейтральных объектов $\YY_i$:
\[
    Q'_\eps(A_i, \YY_i) = \frac{1}{C_{L_i}^{\ell_i}} \sum_{Y \in [\YY_i]^{\ell_i}}
        [\max_{a \in A_i}\delta(a, Y) \geq \eps],
\]
где $[\YY_i]^{\ell_i}$ "--- множество разбиений $\YY_i$ на~обучающую выборку~$Y$ длины~$\ell_i$ и~контрольную выборку~$\bar Y$ длины $k_i=L_i-\ell_i$.

\begin{Theorem}[ПЗМ для кластеров]
\label{th:generalizedPZM}
Пусть выполнена гипотеза~\ref{generalizedPZM},
а~на разбиение $A = A_1 \sqcup A_2 \sqcup \dots \sqcup A_t$ наложено дополнительное ограничение:
внутри каждого кластера $A_i$ все алгоритмы допускают равное число ошибок $m_i$.
Тогда
\begin{equation}
    \label{generalizedPZMtheorem}
    Q_\eps(A, \XX) \leq \sum_{i = 1}^t P_i \, Q'_{\eps_i}(A_i, \YY_i),
\end{equation}
где $P_i = C_{L_i}^{\ell_i} / \CLl$,\;
$\eps_i = \frac{L_i}{\ell_i k_i} \frac{\ell \, k}{L} \eps + \big(1 - \frac{\ell \, L_i}{L \ell_i}\big) \frac{m_i}{k_i} - \frac{|X'_i|}{k_i}$.
\end{Theorem}

\begin{Lemma}
    \label{lem:generalizedPZM-sets}
    Пусть $\mu$ "--- детерминированный ПМЭР. Тогда множества
    \[
        \begin{split}
            X_i  & = \bigcap\limits_{a \in A_i} \{x \in \XX \colon \exists b \in A \colon a \prec b, I(a, x) < I(b, x)\}, \\
            X'_i & = \bigcap\limits_{a \in A_i} \{x \in \XX \colon \exists b \in A \colon b < a,     I(b, x) < I(a, x)\}
        \end{split}
    \]
    являются, соответственно, порождающим и~запрещающим множествами для кластера $A_i$ в~смысле гипотезы~\ref{generalizedPZM}.
\end{Lemma}
Лемма~\ref{lem:generalizedPZM-sets} позволяет в~явном виде
построить множество порождающих и~запрещающих объектов
и~применить теорему~\ref{th:generalizedPZM} к~детерминированному ПМЭР.
Аналогичный результат для РМЭР представлен гипотезой~\ref{hyp2} и~теоремой~\ref{th2}.

\begin{Hypothesis}
\label{hyp2}
    Обозначим $\fA(A) = \bigl\{ A(X) \colon X \in \XXell \bigr\}$.
    Пусть множество~$A$ и~выборка~$\XX$ таковы, что
    для~каждого $\alpha \in \fA(A)$
    можно указать пару непересекающихся подмножеств
    ${X_\alpha \subset \XX}$ и~${X'_\alpha\subset \XX}$,
    удовлетворяющую условию
    \begin{equation}
    \label{eq2muX}
        \bigl[ A(X) \!=\! \alpha \bigr]
        =
        \bigl[  X_\alpha\subseteq  X \bigr]
        \bigl[ X'_\alpha\subseteq \X \bigr]
        \; \text{ для всех } X\in \XXell.
    \end{equation}
\end{Hypothesis}

\begin{Theorem}
\label{th2}
Если справедлива гипотеза~\ref{hyp2},
то~вероятность переобучения рандомизированного метода минимизации эмпирического риска есть
\[
    Q_\eps(A) = \sum_{a \in A} \sum_{\alpha \in \fA(A)} \frac {[a \in \alpha]}{|\alpha|}
        \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        \BHyper{L_\alpha}{m^a_\alpha}{\ell_\alpha}{s^a_\alpha(\eps)},
\]
где введены следующие обозначения:
\begin{align*}
    L_\alpha &= L - |\Xl_\alpha| - |\Xk_\alpha|;\quad
    \ell_\alpha = \ell - |\Xl_\alpha|;\\
    m^a_\alpha &= n(a, \XX \backslash \Xl_\alpha \backslash \Xl'_\alpha); \quad
    s^a_{\alpha}(\eps) = \tfrac\ell L \bigl(n(a, \XX) - \eps k\bigr) - n(a, X_\alpha).
\end{align*}
\end{Theorem}

\subsection*{Глава~4. Точные оценки вероятности переобучения для РМЭР}
\refstepcounter{chapter}

В данной главе получены точные оценки вероятности переобучения РМЭР для девяти модельных семейств алгоритмов:
\begin{itemize}[noitemsep]
    \item монотонная цепь;
    \item унимодальная цепь;
    \item пучок монотонных цепей;
    \item монотонная сеть;
    \item унимодальная сеть;
    \item разреженная монотонная сеть;
    \item разреженная унимодальная сеть;
    \item слой хэммингова шара;
    \item слой интервала булева куба.
\end{itemize}
Для вывода оценок используется разработанный выше математический инструментарий:
разложение вероятности переобучения по~орбитам действия группы симметрий
на~множестве алгоритмов или на~множестве разбиений выборки,
а~также теорема о~порождающих и~запрещающих множеств для РМЭР.

Среди перечисленных семейств наибольшего внимания заслуживают следующие два.

\begin{Def}
\emph{Центральным слоем шара} радиуса $r$ называют множество алгоритмов,
заданное следующим условием:
\[
    B_{r}(a_0) = \{a \in \AA \colon n(a, \XX) = n(a_0, \XX) \text{ и~} \rho(a, a_0) \leq r\},
\]
где $a_0$ "--- фиксированный алгоритм,
$\rho(a, a')$ "--- расстояние Хэмминга между векторами ошибок алгоритмов $a, a'$.
\end{Def}
Центральный слой хэммингова шара является множеством из~большого числа попарно близких алгоритмов.
Это позволяет использовать данное множество в~качестве объемлющего множества $B_i$
в~оценках вида $\Q(A_i) \leq \Q(B_i)$.
Следующее множество также обладает указанным свойством, но кроме этого
различает несколько классов объектов:
надежно классифицируемые,
ошибочно классифицируемые,
и~пограничные.
\begin{Def}
\label{def:algsHeap}
Пусть все объекты генеральной выборки $\XX$ разделены на~три непересекающихся множества:
надежно классифицируемые объекты $X_0$,
ошибочно классифицируемые объекты $X_1$
и~пограничные объекты $X_r$.
Пусть $|X_r| = r$ и~$|X_1| = m$, $\rho$ "--- целочисленный параметр, $\rho \leq r$.
\emph{Слоем интервала булева куба} будем называть множество $\hat B_{r,\rho}^{m}$,
удовлетворяющее следующим условиям:
\begin{itemize}
    \item $\hat B_{r,\rho}^{m}$ содержит все алгоритмы, допускающие ровно $\rho$ ошибок на~объектах из~$X_r$,
    \item ни один алгоритм из~$\hat B_{r,\rho}^{m}$ не~ошибается на~объектах из~$X_0$,
    \item все алгоритмы из~$\hat B_{r,\rho}^{m}$ ошибаются на~всех объектах из~$X_1$.
\end{itemize}
\end{Def}

\begin{Theorem}[Толстихин, 2010]
\label{thBallSlice}
Вероятность переобучения ПМЭР для центрального слоя шара $B_r(a_0)$:
\begin{equation}
    Q_\epsilon(B_{r}(a_0))
    =
    H_L^{\ell, m}\bigl(\tfrac \ell L (m - \epsilon k) + \big\lfloor r/2 \big\rfloor\bigr) \cdot [m \geq \eps k].
\end{equation}
\end{Theorem}
В настоящей работе приводится более простое доказательство Теоремы~\ref{thBallSlice}.
\begin{Theorem}
\label{th:interval-layer}
Вероятность переобучения ПМЭР для слоя интервала булева куба $\hat B_{r,\rho}^{m}$:
\begin{equation}
\label{eq:interval-layer}
    Q_{\eps}(\hat B_{r,\rho}^{m}) =
        \frac 1{\CLl}
            \sum_{i = 0}^{\min(m, \ell)} \sum_{j = 0}^{\min(r, \ell - i)}
                C_m^i C_r^j C_{L - m - r}^{\ell - i - j} \Big[\delta(i, j) \geq \eps\Big],
\end{equation}
где $t(i,j) = i + \max(0, \rho - r - j)$
и~$\delta(i, j) = \frac{m + \rho - t(i,j)}{k} - \frac{t(i,j)}{\ell}$.
\end{Theorem}

\subsection*{Глава~5. Вычислительные эксперименты на~реальных данных}
\refstepcounter{chapter}

В~данной главе описываются вычислительные эксперименты
на~реальных данных из~репозитория UCI.
В экспериментах сравнивается завышенность
уже известных комбинаторных оценок вероятности переобучения
и~новой оценки~\eqref{generalizedPZMtheorem},
в~которой вероятность переобучения каждого кластера оценивается сверху с~помощью~\eqref{eq:interval-layer}.
Показывается, что построенная таким образом оценка
в~ряде случаев оказывается менее завышенной по~сравнению
с~другими комбинаторными оценками.
Кроме того, новая оценка эффективно вычислима для семейств с~существенно большим числом алгоритмов,
т.к. сумма~\eqref{generalizedPZMtheorem} содержит меньшее число слагаемых из-за
кластеризации алгоритмов с~близкими векторами ошибок.

%\paragraph{5.1. Эффективное вычисление SC-оценки.}
%\paragraph{5.2. Проблема сэмплирования алгоритмов.}
%\paragraph{5.3. Эксперимент: сравнение с~PAC-Bayes оценками.}
%\paragraph{5.4. Применение комбинаторные оценок к~логическим алгоритмам.}
%\paragraph{5.5. Вычислительный эксперимент для оценки расслоения-связности.}

\section*{Заключение}

Основные результаты диссертационной работы.
\begin{enumerate}
\item
    Предложен теоретико"=групповой метод вывода оценок вероятности переобучения
    для рандомизированного метода минимизации эмпирического риска.
\item
    Доказаны точные оценки вероятности переобучения рандомизированного метода минимизации эмпирического риска
    для девяти модельных семейств, включая монотонные и~унимодальные сети, слой хэммингова шара и~ряд других.
\item
    Получена общая оценка вероятности переобучения, основанная на~разложении и~покрытии множества алгоритмов.
\item
    В~экспериментах на~реальных данных показано,
    что новая оценка вероятности переобучения является более точной
    по~сравнению с~другими оценками вероятности переобучения.
\end{enumerate}
Возможным направлением дальнейших исследований является
повышение точности комбинаторных оценок вероятности переобучения
путем более аккуратного учета эффекта расслоения,
а~также применение полученных в~данной работе оценок
для улучшения обобщающей способности логических алгоритмов классификации.

%\newpage
\renewcommand{\refname}{Публикации по~теме диссертации}
%\def\BibUrl#1.{}
%\bibliographystyle{gost2008}  %% стилевой файл для оформления по~ГОСТу
%\bibliography{MachLearn,bibliography}     %% имя библиографической базы (bib-файла)

\begin{thebibliography}{1}
\def\selectlanguageifdefined#1{
\expandafter\ifx\csname date#1\endcsname\relax
\else\selectlanguage{#1}\fi}
\providecommand*{\href}[2]{{#2}}
\providecommand*{\url}[1]{{#1}}
\providecommand*{\BibUrl}[1]{\url{#1}}
\providecommand{\BibAnnote}[1]{}
\providecommand*{\BibEmph}[1]{#1}
\ProvideTextCommandDefault{\cyrdash}{\hbox to.8em{--\hss--}}

\bibitem{frey09mmro}
\selectlanguageifdefined{russian}
Фрей~А.~И. Точные оценки вероятности переобучения для симметричных семейств
  алгоритмов //~Всеросс. конф. Математические методы распознавания
  образов-14.~---
\newblock М.:~МАКС Пресс, 2009.~---
\newblock {\cyr\CYRS.}~66--69. \BibAnnote{В комбинаторном подходе к проблеме
  переобучения основной задачей является получение вычислительно эффективных
  формул для вероятности переобучения и~вероятности получить каждый из
  имеющихся алгоритмов в~результате обучения. Предлагается подход, который
  позволяет проще выводить такие формулы в~тех случаях, когда множество
  алгоритмов наделено некоторой группой симметрий. Приводятся примеры подобных
  ситуаций. Дается определение рандомизированного метода обучения, для которого
  доказывается общая оценка вероятности переобучения.}

\bibitem{frey09mipt}
\selectlanguageifdefined{russian}
Фрей~А.~И. Точные оценки вероятности переобучения для симметричных семейств
  алгоритмов //~Труды 52-й научной конференции МФТИ <<Современные проблемы
  фундаментальных и прикладных наук>>. Часть II. Управление и прикладная
  математика.~---
\newblock М.:~МФТИ, 2009.~---
\newblock {\cyr\CYRS.}~106--109.

\bibitem{frey10iip}
\selectlanguageifdefined{russian}
Фрей~А.~И. Вероятность переобучения плотных и~разреженных многомерных~сеток
  алгоритмов //~Межд. конф. Интеллектуализация обработки информации ИОИ-8.~---
\newblock М.:~МАКС Пресс, 2010.~---
\newblock {\cyr\CYRS.}~87--90.

\bibitem{frey11mmro}
\selectlanguageifdefined{russian}
Фрей~А.~И. Метод порождающих и запрещающих множеств для рандомизированного
  метода минимизации эмпирического риска //~Всеросс. конф. Математические
  методы распознавания образов-15.~---
\newblock М.:~МАКС Пресс, 2011.~---
\newblock {\cyr\CYRS.}~60--63.

\bibitem{frey12iip}
\selectlanguageifdefined{russian}
Фрей~А.~И., Ивахненко~А.~А., Решетняк~И.~М. Применение комбинаторных оценок
  вероятности переобучения в~простом голосовании пороговых конъюнкций //~Межд.
  конф. Интеллектуализация обработки информации ИОИ-9.~---
\newblock М.:~МАКС Пресс, 2012.~---
\newblock {\cyr\CYRS.}~86--89.

\bibitem{frey12jmlda}
\selectlanguageifdefined{russian}
Фрей~А.~И., Толстихин~И.~О. Комбинаторные оценки вероятности переобучения на
  основе кластеризации и покрытий множества алгоритмов //~Machine learning and
  data analysis.~---
\newblock 2013.~---
\newblock \CYRT. 1(6).~---
\newblock {\cyr\CYRS.}~751--767.

\bibitem{frey10pria}
\selectlanguageifdefined{english}
Frei~A.~I. Accurate estimates of the generalization ability for symmetric set
  of predictors and randomized learning algorithms //~Pattern Recognition and
  Image Analysis.~---
\newblock 2010.~---
\newblock V.~20, no.~3.~---
\newblock P.~241–250. \BibAnnote{The main issue of the combinatorial approach
  to overfitting is to obtain computationally efficient formulas for
  overfitting probabilities. A group-theoretical approach is proposed to
  simplify derivation of such formulas when the set of predictors has a certain
  group of symmetries. Examples of the sets are given. The general estimate of
  overfitting probability is proved for the randomized learning algorithm. It
  is applied to four model sets of predictors~--- a layer of the Boolean cube,
  the Boolean cube, the unimodal chain, and a bundle of monotonic chains.}

\bibitem{voron12jmlda-eng}
\selectlanguageifdefined{english}
Vorontsov~K., Frey~A.~I., Sokolov~E. Computable combinatorial overfitting
  bounds //~Machine learning and data analysis.~---
\newblock 2013.~---
\newblock V. 1(6).~---
\newblock P.~724--733.

\end{thebibliography}

%\begin{thebibliography}{10}
%\def\selectlanguageifdefined#1{
%\expandafter\ifx\csname date#1\endcsname\relax
%\else\language\csname l@#1\endcsname\fi}
%\ifx\undefined\url\def\url#1{{\small #1}}\else\fi
%\ifx\undefined\BibAuthor\def\BibAuthor#1{\emph{#1}}\else\fi
%\ifx\undefined\BibTitle\def\BibTitle#1{#1}\else\fi
%\ifx\undefined\BibVAK\def\BibVAK#1{{\bfseries #1}}\else\fi
%\ifx\undefined\BibUrl\def\BibUrl#1{\url{#1}}\else\fi
%\ifx\undefined\BibAnnote\def\BibAnnote#1{}\else\fi
%\ifx\undefined\BibSection\def\BibSection#1#2#3{}\else\fi
%\end{thebibliography}

%\newpage
\noindent
В работах с~соавторами лично соискателем сделано следующее:
\begin{itemize}[noitemsep]
%Применение комбинаторных оценок вероятности переобучения в~простом голосовании пороговых конъюнкций
%Комбинаторные оценки вероятности переобучения на~основе кластеризации и~покрытий множества алгоритмов
%Computable combinatorial overfitting bounds
    \item проведены эксперименты по~исследованию обобщающей способности
        логических алгоритмов классификации~\cite{frey12iip};
    \item получена оценка вероятности переобучения,
        обобщающая оценку метода порождающих и~запрещающих множеств на~случай произвольного разбиения множества алгоритмов на~кластеры~\cite{frey12jmlda};
    \item проведены эксперименты по~сравнению комбинаторных и~PAC-Bayes оценок переобучения;
        экспериментально исследованы кривые обучения логистической регрессии~\cite{voron12jmlda-eng}.
\end{itemize}

\newpage
\begin{center}

    Фрей Александр Ильич\\[1cm]

    \vspace{32mm}

    {\large\bfseries
        ТЕОРЕТИКО-ГРУППОВОЙ ПОДХОД \\ В КОМБИНАТОРНОЙ ТЕОРИИ ПЕРЕОБУЧЕНИЯ
    }\\[1cm]

\vspace{23mm}

{\small
    АВТОРЕФЕРАТ

Подписано в~печать: 11.11.2013. Формат $60 \times 84 \, \sfrac{1}{16}$.

Печать трафаретная. Объем: 1,0 усл. печ. л.

Тираж – 100 экз. Заказ № 339.

Федеральное государственное автономное
образовательное учреждение
высшего профессионального образования
<<Московский физико-технический институт (государственный университет)>>

Отдел оперативной полиграфии <<Физтех-полиграф>>

141700, Московская обл., г. Долгопрудный, \\ Институтский пер., 9.
}
\end{center}
\thispagestyle{empty}
\end{document}
