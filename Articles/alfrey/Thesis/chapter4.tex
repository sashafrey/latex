\chapter[Точные оценки вероятности переобучения для РМЭР]{Точные оценки вероятности \\ переобучения для РМЭР}
\label{chap:3}

В данной главе изучается ряд модельных семейств алгоритмов,
непосредственно заданных с~помощью бинарной матрицы ошибок.
Эти семейства моделируют идеализированные свойства реальных семейств алгоритмов:
размерность и~разреженность семейства, расслоение алгоритмов по~числу ошибок,
связность и~сходство алгоритмов, и~др.

Изучение модельных множеств интересно не~только в~теории, но также может быть полезно на~практике.
В частности, в~работе П. Ботова~\cite{botov11mmro} модельные множества используются
для аппроксимации реального семейства классификаторов
с~помощью унимодальной несимметричной сети алгоритмов малой высоты и~размерности,
для которой известны точные комбинаторные формулы вероятности переобучения.
Эксперименты на~решающих деревьях и~реальных задачах классификации показывают, что
такой подход повышает обобщающую способность получаемых алгоритмов классификации.

В настоящей работе предлагается использовать модельные семейства
в~качестве объемлющего множества $B$ (см. лемму~\ref{th:LayerGr})
для эффективного вычисления оценки~\eqref{eq:pzm-cluster},
основанной на~разложениии и~покрытии множества алгоритмов.

\section{Монотонные и~унимодальные цепи}
\begin{Def}
\label{def:chain}
\emph{Цепью алгоритмов} называется семейство алгоритмов,
элементы которого можно выстроить в~такую последовательность,
что любые два соседних алгоритма
последовательности различаются лишь на~одном объекте.
\end{Def}

Точные оценки вероятности переобучения ПМЭР для монотонных и~унимодальных
цепей были получены ранее К.~В.~Воронцовым.
Ниже аналогичные результаты будут получены для РМЭР.

\subsection{Монотонная цепь}
Монотонная цепь является фундаментальным объектом в~теории комбинаторного обучения.
Это одно из~простейших модельных семейств, одновременно обладающее свойствами расслоения и~связности.

\begin{Def}
Множество алгоритмов $\{a_0, \ldots, a_D\}$ называется
\emph{монотонной цепью}, если оно является цепью в~смысле определения~\ref{def:chain},
и~кроме этого число ошибок является монотонной функцией номера алгоритма в~цепи:
\[
    n(a_i, \XX) = n(a_0, \XX) + i, \text{ при } i = 0, \ldots, D.
\]
\end{Def}
Монотонная цепь алгоритмов "--- это простейшая модель однопараметрического
\emph{связного семейства алгоритмов}, предполагающая,
что при непрерывном удалении некоторого параметра от~оптимального значения
число ошибок на~полной выборке только увеличивается.

\begin{Example}
    Пусть
   ~$A$ "--- семейство
    \emph{линейных алгоритмов классификации},
    т.\,е. семейство параметрических отображений из~$\XX = \RR^n$ в~$\mathbb{Y} \brop= \{-1,+1\}$ вида
    \[
        a(x,w) = \sign\left( x_1 w_1 + \ldots + x_n w_n \right),
        \quad
        x=(x_1,\ldots,x_n)\in \RR^n.
    \]
    Параметр ${w\in\RR^n}$ задает направляющий вектор гиперплоскости,
    разделяющей пространство~$\RR^n$ на~два полупространства,
    соответствующих классам~$-1$ и~$+1$.
    Пусть функция потерь имеет вид
    $I(a,x)\brop = \bigl[ a(x,w)\neq y(x) \bigr]$,
    где $y(x)$ "--- истинная классификация объекта~$x$.
    Пусть множество объектов~$\XX$ линейно разделимо,
    т.\,е.~существует вектор ${w^{*} \in\RR^n}$,
    при котором алгоритм~$a(x,w^{*})$ не~допускает ошибок на~$\XX$.
    Тогда множество алгоритмов
    \[
        A_\delta
        =
        \bigl\{
            a(x,w^{*}+t\delta) \colon
            t\in[0,+\infty)
        \bigr\}
    \]
    порождает монотонную цепь при любом ${\delta\in\RR^n}$,
    за~исключением, быть может, некоторого конечного множества векторов.
    При~этом~$n(a_0, \XX)=0$ в~силу линейной разделимости.
\end{Example}

Мы воспользуемся теоремой~\ref{th:pzm-rand-simple}
о порождающих и~запрещающих множествах для рандомизированного метода обучения.
Для этого мы вначале установим структуру множества
$\fA(A) = \bigl\{ A(X) \colon X \in \XXell \bigr\}$,
затем построим систему порождающих и~запрещающих множеств для каждого $\alpha \in \fA(A)$
и, наконец, воспользуемся теоремой~\ref{th:pzm-rand-simple}.

\begin{Lemma}
\label{th:monotonic-chain-fA}
Для монотонной цепи длины~$D$ множество $\fA(A)$
состоит из~$D + 1$ элемента: $\fA(A) = \{\alpha_0, \dots, \alpha_D\}$,
причем для всех $i$ выполнено $\alpha_i = \{a_0, \dots, a_i\}$.
\end{Lemma}
\begin{vkProof}
Доказательство следует непосредственно из~определения
РМЭР~\ref{eq:mu-rand-ERM}
и~монотонности числа ошибок в~цепи.
\end{vkProof}

Чтобы выписать структуру порождающих и~запрещающих множеств, необходимо
зафиксировать нумерацию объектов выборки.
Сделаем это так, как показано ниже:
    \[
        \begin{array}{rccccclll}
                        & x_1 & x_2 & x_3 &   & x_D &  & \overbrace{\hphantom{1,\ldots,1}}^{m} &
        \\
            \vec a_0 = ( & 0,  & 0,  & 0,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \vec a_1 = ( & 1,  & 0,  & 0,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \vec a_2 = ( & 1,  & 1,  & 0,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \vec a_3 = ( & 1,  & 1,  & 1,  & \ldots  & 0,  & 0,\ldots,0, &  1,\ldots,1  & );
        \\
            \ldots\;\: &&&& \ldots && \quad\ldots & \quad\ldots
        \\
            \vec a_D = ( & 1,  & 1,  & 1,  & \ldots  & 1,  & 0,\ldots,0, &  1,\ldots,1  & );
        \end{array}
    \]
 При такой нумерации каждый из~алгоритмов~$a_t$,\, $t=1,\ldots,D,$
допускает ошибку на~объектах $x_1,\ldots,x_t$.
Нумерация остальных объектов не~имеет значения,
так как на~этих объектах алгоритмы неразличимы.

\begin{Lemma}
\label{th:monotonic-chain-pzm}
Система порождающих и~запрещающих множеств монотонной цепи устроена следующим образом:
\begin{align*}
    [A(X){=}\alpha_t] &= [x_1,\ldots,x_D \in \X], \text{ при } t = D,\\
    [A(X){=}\alpha_t] &= [x_{t+1} \in X][x_1,\ldots,x_t \in \X], \text{ при } t \leq D.
\end{align*}
\end{Lemma}
\begin{vkProof}
Рассмотрим два случая.

1.\enspace
    Если $t=D$,
    то~$\alpha_t$ совпадает со всем множеством алгоритмов.
    Следовательно,
     $A(X) = \alpha_t$ тогда и~только тогда, когда
    все объекты $\{x_1,\ldots,x_D\}$ будут находиться в~контрольной подвыборке~$\X$.
    В~этом случае
    \[
        [A(X){=}\alpha_t] = [x_1,\ldots,x_D \in \X].
    \]

2.\enspace
    Во~всех остальных случаях
    $A(X) = \alpha_t$ тогда и~только тогда, когда
    все объекты $\{x_1,\ldots,x_t\}$ будут находиться в~контрольной подвыборке~$\X$,
    а~объект $x_{t+1}$ "--- в~обучающей подвыборке~$X$.
    В~этом случае
    \[
        [A(X){=}\alpha_t] = [x_{t+1} \in X][x_1,\ldots,x_t \in \X].
    \]
\end{vkProof}

Отметим, что в~случае монотонной цепи
структура множества $\fA(A)$ оказалась идентичной самому множеству~$A$.
Кроме этого, система порождающих и~запрещающих множеств
рандомизированного МЭР оказалась такой же, как и~у детерминированного МЭР.
Данное совпадение вызвано простой структурой рассматриваемого множества алгоритмов
и~не~имеет места в~общем случае.

\begin{Theorem}
    Для монотонной цепи из~$D + 1$ алгоритмов вероятность переобучения РМЭР равна
    \begin{equation}
        \label{eq:QEps-monotonic-chain}
           Q_\eps(A) =
            %\frac{1}{\CLl}
            \sum_{d=0}^{D} \sum_{t=d}^D
            \frac 1{1 + t}
            \frac{\Binom{L'}{\ell'}}{\CLl}
            H_{L'}^{\ell\,', m}\bigl(s(\epsilon)\bigr),
    \end{equation}
    где
        $L' = L {-} t {-} F$,\:
        $\ell' = \ell {-} F$,\:
        $F = [t{\neq}D]$,\:
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m {+} d {-} \epsilon k) \bigr\rfloor$.
\end{Theorem}

\begin{vkProof}
Воспользовавшись леммами~\ref{th:monotonic-chain-fA}
и~\ref{th:monotonic-chain-pzm},
получим, что для всех $t = 0, \dots, D$ выполнено $|\alpha_t| = t + 1$,
$L_t = L - t - 1$,
$\ell_t = \ell - [t = D]$,
$m^d_t = m + d - d = m$,
$s^d_t(\eps) \brop= \tfrac\ell L(m+d-\eps k)$,
где для упрощения обозначений
вместо двойных индексов
$L_{\alpha_t}$, $\ell_{\alpha_t}$, $m^d_{\alpha_t}$, $s^d_{\alpha_t}(\eps)$
использованы одинарные:~$L_t$, $\ell_t$, $m^d_t$, и~$s^d_t(\eps)$.
Далее заметим, что система порождающих и~запрещающих множеств,
построенных в~лемме~\ref{th:monotonic-chain-pzm},
удовлетворяет условиям следствия~\ref{th:pzm-rand-simple} теоремы~\ref{th:pzm-rand}.
Подставив посчитанные значения $L_t$, $\ell_t$, $m^d_t$, $s^d_t(\eps)$
в~\eqref{eq:pzm-rand}, получим утверждение настоящей теоремы.
\end{vkProof}

В~приведенном доказательстве мы не~рассматривали отдельно случай $D \leq k$ и~$D > k$.
Эти эффекты уже учтены корректно благодаря тому, что мы доопределили нулем биномиальные коэффициенты
в~гипергеометрическом распределении.
Также отметим, что в~случае монотонной цепи группа симметрии оказалась тривиальной,
и~потому не~учитывалась при вычислениях.

\subsection{Унимодальная цепь}
Унимодальная цепь является более реалистичной моделью однопараметрического \emph{связного семейства}
по~сравнению с~монотонной цепью. Если мы имеем лучший алгоритм $a_0$ c оптимальным значением
некоторого вещественного параметра, то~отклонение значения этого параметра как в~б\'ольшую, так и~в~меньшую
сторону приводит к~увеличению числа ошибок.
\begin{Def}
Множество алгоритмов \[\{a_0,\, a_1, \ldots, a_D,\, a'_1, \ldots, a'_D\}\] называется
\emph{унимодальной цепью}, если выполнены два условия:
\begin{itemize}
  \item[1)] левая ветвь $\{a_0, a_1, \ldots, a_D\}$ и
        правая ветвь $\{a_0, a'_1, \ldots, a'_D\}$
        являются монотонными цепями;
  \item[2)] пересечение множества ошибок алгоритмов $a_D$ и~$a'_D$ равно
        множеству ошибок алгоритма $a_0$.
\end{itemize}
\end{Def}
Параметр $D$ будем называть \emph{длиной ветвей} унимодальной цепи.

Рассмотрим унимодальную цепь длины $D$
и~пронумеруем объекты генеральной выборки $\XX$ так, как показано ниже:
\[
    \bordermatrix{
         & a_0 & a_1 & a_2 & \cdots & a_D & a'_1 & a'_2 & \cdots & a'_D \cr
    x_1  & 0 & 1 & 1 & \cdots & 1 & 0 & 0 & \cdots & 0 \cr
    x_2  & 0 & 0 & 1& \cdots  & 1 & 0 & 0& \cdots  & 0 \cr
         & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots& \cdots  & \cdots \cr
    x_D  & 0 & 0 & 0 & \cdots & 1 & 0 & 0& \cdots  & 0 \vspace{-1.5ex}\cr\cline{2-10}
    x'_1 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 & \cdots & 1 \cr
    x'_2 & 0 & 0 & 0 & \cdots & 0 & 0 & 1& \cdots  & 1 \cr
         & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots& \cdots  & \cdots \cr
    x'_D & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 1 \cr
    }
\]
Нумерация остальных объектов не~имеет значения, так как на этих объектах алгоритмы неразличимы.

\begin{Lemma}
\label{th:unimodal-chain-sym}
Группа симметрии унимодальной цепи при $D \geq 1$
содержит в~качестве своей подгруппы группу перестановок $S_2$.
Данная подгруппа действует на~алгоритмы цепи перестановкой левой и~правой ветви.
\end{Lemma}
\begin{vkProof}
Рассмотрим группу $S_2$,
состоящую из~нулевой перестановки
и~из~перестановки, действующей по~правилу
(${x_1{\,\leftrightarrow\,}x'_1}, \ldots, x_D{\,\leftrightarrow\,}x'_D$).
Данная группа удовлетворяет условию леммы.
\end{vkProof}
Из леммы~\ref{th:unimodal-chain-sym} следует,
что алгоритмы разных ветвей с~равным числом ошибок лежат
в~одной орбите действия группы симметрии.
Орбита $\omega_0 = \{a_0\}$ содержит единственный алгоритм.
Для остальных орбит $\omega_d = \{a_d, a'_d\}$
договоримся выбирать алгоритм $a_d$ из~левой ветви в~качестве представителя орбиты.

\begin{Lemma}
\label{th:unimodal-chain-fA}
Для унимодальной цепи длины $D$ выполнено
$\fA(A) = \{\alpha_{t_1, t_2})\}$, где $t_1, t_2 = 0, \dots, D$.
При этом $\alpha_{t_1, t_2} = \{a_0, a_1, \ldots, a_{t_1}, a'_1, \ldots, a'_{t_2}\}$.
\end{Lemma}
\begin{vkProof}
Доказательство следует непосредственно из~определения
РМЭР~\ref{eq:mu-rand-ERM}
и~монотонности числа ошибок в~каждой ветви унимодальной цепи.
\end{vkProof}

Множества порождающих и~запрещающих объектов для унимодальной цепи строятся
по~аналогии с~леммой~\ref{th:monotonic-chain-pzm} для монотонной цепи.
Мы не~будем выписывать их в~явном виде и~непосредственно перейдем
к~доказательству теоремы о вероятности переобучения РМЭМ для унимодальной цепи.

\begin{Theorem}
    \label{th:QEps-unimodal-chain}
    Для унимодальной цепи с~ветвями длины~$D$ вероятность переобучения РМЭР равна
    \begin{equation}
        \label{eq:QEps-unimodal-chain}
           Q_\eps(A) =
            %\frac{1}{\CLl}
            \sum_{d=0}^{D} \sum_{t_1=d}^D \sum_{t_2 = 0}^{D}
            \frac{|\omega_d|}{1 + t_1 + t_2}
            \frac{\Binom{L'}{\ell\,'}}{\CLl}
            H_{L'}^{\ell\,', m}\bigl(s(\epsilon)\bigr),
    \end{equation}
    где \;
          $\omega_d = [d = 0] + 2 \cdot [d > 0]$,\:
        $L' = L - S - F$,\:
          $S = t_1 + t_2$,\:
        $F = [t_1{\neq}D] + [t_2{\neq}D]$,\:
        $\ell' = \ell {-} F$,\:
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m {+} d {-} \epsilon k) \bigr\rfloor$.
\end{Theorem}

\begin{vkProof}
Воспользуемся леммами~\ref{th:unimodal-chain-sym} и~\ref{th:unimodal-chain-fA}.
Рассмотрим произвольное $\alpha \equiv \alpha_{t_1, t_2} \in \fA(A)$.
Легко заметить, что при $t_1, t_2$ строго меньше $D$
множеством порождающих объектов будет
$X'_\alpha = \{ x_{t_1 + 1}, x_{t_2 + 1}\}$.
Условие $t_i = D$ уменьшает количество порождающих объектов в~$X'_\alpha$ на~единицу.
Множество запрещающих объектов $X_\alpha = \{x_1, \ldots, x_{t_1}, x'_1, \ldots, x'_{t_2}\}$.
Введя обозначение $F = [t_1{\neq}D] + [t_2{\neq}D]$, получим
\begin{align*}
L_\alpha &= L - t_1 - t_2 - F,\quad\hspace{10pt} \ell_\alpha = \ell - F;\\
m^a_\alpha &= m + d - d = m,\quad s^a_\alpha(\eps) = \lfloor \frac{\ell}L (m + d - \epsilon k) \rfloor.
\end{align*}
Наконец, мощность множества $\alpha_{t_1, t_2}$
равна $|\alpha_{t_1, t_2}| = \frac 1{1 + t_1 + t_2}$,
а~$[a_d \in \alpha_{t_1, t_2}] \brop= [d \leq t_1]$.

Подставляя эти значения в~общую формулу из~теоремы~\ref{th:pzm-rand} о~порождающих и~запрещающих объектах,
получаем утверждение доказываемой теоремы.
\end{vkProof}

\section{Многомерные семейства алгоритмов}

\subsection{Пучок монотонных цепей}

\begin{Def}
\emph{Пучком из~$h$ монотонных цепей} называется множество алгоритмов,
полученное объединением $h$ монотонных цепей равной длины,
с~общим первым алгоритмом. Как и~в~случае унимодальной цепи, предполагается,
что множества объектов, на~которых ошибаются алгоритмы ветвей, не~пересекаются.
\end{Def}

Связка из~$2h$ монотонных цепей является
моделью $h$"~параметрического семейства алгоритмов,
в~котором разрешено изменять любой из~$h$ параметров при фиксированных остальных,
а~одновременное изменение нескольких параметров не~допускается.
Данное семейство можно также рассматривать как обобщение трех частных случаев,
рассмотренных в~\cite{voron09dan}:
монотонной цепи~($h=1$),
унимодальной цепи~($h=2$)
и~единичной окрестности лучшего алгоритма ($D=1$).

\begin{Lemma}
\label{th:chains-binding-sym}
Группа симметрии связки~из~$h$ монотонных цепей содержит в~качестве подгруппы
симметрическую группу $S_h$, действующую на~ветви связки всевозможными перестановками.
\end{Lemma}
\begin{vkProof}
Достаточно рассмотреть произвольную перестановку $\pi \in S_h$
и~указать перестановку $g \colon \XX \rightarrow \XX$,
действующую на~ветви связки в~соответствии с~$\pi$.
Такая перестановка строится в~явном виде по~аналогии с~леммой~\ref{th:unimodal-chain-sym}.
\end{vkProof}
Из леммы~\ref{th:chains-binding-sym} следует, что алгоритмы разных ветвей с~равным числом ошибок лежат
в~одной орбите действия группы симметрии.
Орбита $\omega_0 = \{a_0\}$ содержит единственный алгоритм.
Для остальных орбит $\omega_d = \{a^1_d, a^2_d, \ldots, a^h_d\}$
договоримся выбирать алгоритм $a^1_d$ из~первой ветви в~качестве представителя орбиты.

\begin{Lemma}
\label{th:chains-binding-fA}
Для связки из~$h$ цепей длины $D$ выполнено
$\fA(A) = \{\alpha_{t_1, \dots, t_h})\}$, где $t_i = 0, \dots, D \text{ для всех } i$.
При этом $\alpha_{t_1, \dots, t_h} = \{a^i_j\}$,
где $i = 1, \ldots, h$, $j = 1, \ldots, t_i$.
\end{Lemma}
\begin{vkProof}
Доказательство следует непосредственно из~определения
РМЭР~\ref{eq:mu-rand-ERM}
и~монотонности числа ошибок в~каждой ветви унимодальной цепи.
\end{vkProof}

Множества порождающих и~запрещающих объектов для связки цепей строятся
по~аналогии с~леммой~\ref{th:monotonic-chain-pzm} для монотонной цепи.
Мы вновь не~будем выписывать их в явном виде и~непосредственно перейдем
к~доказательству теоремы о вероятности переобучения РМЭМ для связки цепей.
Для этого нам понадобится \emph{комбинаторный коэффициент}
$R_{D,h}^d(S,F)$, который зависит от~параметров $S$~и~$F$,
от~числа монотонных цепей~$h$ и~от их~длины~$D$,
а~также от~$d$ "--- минимального значения параметра~$S$.
Коэффициент $R_{D,h}^d(S,F)$ равен числу способов представить число~$S$
в~виде суммы $h$~неотрицательных пронумерованных слагаемых, $S = t_1+ \ldots + t_h$,
каждое из~которых не~превосходит~$D$.
При этом ровно $F$~слагаемых не~должны равняться~$D$,
а~на~первое слагаемое накладывается дополнительное ограничение $t_1 \geq d$.

\begin{Theorem}
    Пусть в~связке из~$h$ монотонных цепей
    лучший алгоритм допускает~$m$~ошибок на~полной выборке,
    длина каждой ветви без учета лучшего алгоритма равна~$D$.
    \mbox{Тогда} при обучении рандомизированным методом
    вероятность переобучения может быть записана в~виде:
    \begin{equation}
    \label{eq:QEps-chains-binding}
               Q_\eps(A) =
                \sum_{d=0}^{D}
                \sum_{S=d}^{h D}
                \sum_{F = 0}^{h}
                \frac{|\omega_d| R_{D, h}^d(S, F)}{1 + S}
                \frac{\Binom{L'}{\ell'}}{\CLl}
                H_{L'}^{\ell', m}\bigl(s(\epsilon)\bigr),
    \end{equation}
    где
        $L' = L {-} S {-} F$,\:
        $\ell' = \ell {-} F$,\:
        $s(\epsilon) = \bigl\lfloor \frac{\ell}L (m + d - \epsilon k) \bigr\rfloor$;\:
        $|\omega_h| = 1$ при~$h = 0$ и~$|\omega_d| = h$ при~${d \geq 1}$.
        %$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения~\cite{voron09mmro}.
\end{Theorem}

\begin{vkProof}
Воспользуемся леммами~\ref{th:chains-binding-sym} и~\ref{th:chains-binding-fA}.
Для $\alpha_{\vec t}$, где все~$t_i$ строго меньше $D$,
множеством порождающих объектов будет
$X'_\alpha = \{ x^1_{t_1 + 1}, \ldots, x^h_{t_h + 1}\}$.
Условие $t_i = D$ уменьшает количество порождающих объектов в~$X'_\alpha$ на~единицу.
Множество запрещающих объектов
$X_\alpha \brop= \{x^1_1, \ldots, x^1_{t_1}, x^2_1, \ldots, x^2_{t_2}, \ldots, x^h_1, \ldots, x^h_{t_h}\}$.

Введем обозначения
${S = \sum\limits_{i=1}^{p}t_i}$,\:
${F = \sum\limits_{i=1}^{p}[t_i \neq D]}$.
Тогда
$L_\alpha \brop= L {-} S {-} F$, $\ell_\alpha = \ell {-} F$,
$m_\alpha^a = n(a_0, \XX)$,\:
$s_\alpha^a = \lfloor \frac{\ell}L (m + d - \epsilon k) \rfloor$.
Наконец,
$|\alpha_{\vec t}| = \frac 1{1 + t_1 + \ldots + t_h}$,
$[a^1_d \in \alpha_{\vec t}] = [d \leq t_1]$.

Подставляя эти значения в~общую формулу из~теоремы~\ref{th:pzm-rand} о порождающих и~запрещающих множествах,
получим следующее выражение для вероятности переобучения:
\[
    Q_\eps(A) =
        \sum_{d=0}^{D} \sum_{t_1=d}^D \sum_{t_2 = 0}^{D} \ldots \sum_{t_h = 0}^{D}
        \frac{|\omega_d|}{1 {+} S} \frac {\Binom{L'}{\ell'}} {\CLl} H_{L'}^{\ell', m}\bigl(s(\epsilon)\bigr).
\]

Теперь от~суммирования по~параметрам $t_i$
можно перейти к~суммированию по~множеству возможных значений $S$~и~$F$:
\[
    Q_\eps(A) =
        %\frac{1}{\CLl}
        \sum_{d=0}^{D} \sum_{S=d}^{h D} \sum_{F = 0}^{h}
         |\omega_h|
        \frac{R_{D,h}^d(S, F)}{1 {+} S}
        \frac {\Binom{L'}{\ell'}}{\CLl}
        H_{L'}^{\ell', m}\bigl(s(\eps)\bigr),
\]
где $R_{D,h}^d(S, F)$ "--- определенный выше комбинаторный коэффициент.
\end{vkProof}

\begin{Corollary}
    Для единичной окрестности из~${h}$ алгоритмов вероятность переобучения равна
    \begin{equation}
    \label{eq:QEps-unit-vicinity}
        Q_\eps(A) =
            \sum_{d=0}^{1}
            \sum_{S=d}^{h}
            \frac{|\omega_d| \Binom{h-d}{S-d}}{1 {+} S}
              \frac {\Binom{L'}{\ell'}}{\CLl}
             H_{L'}^{\ell', m}\bigl(s(\eps)\bigr),
    \end{equation}
    где
    $L' = L {-} h$,\:
    $\ell' = \ell {+} S {-} h$.
\end{Corollary}

\begin{figure}[t]
    \begin {multicols}{2}
%    \centering
    \hfill
        \IncludeHalfPicture{Pictures/frey_monot.eps}
    \hfill
    \caption{\small Зависимость $Q_\varepsilon(A)$ от~$\epsilon$
             для монотонной цепи при $L=100$, $\ell=60$, $D=40$, $m=20$.}
    \label{fig:Monot}
    \medskip
    \hfill
        \IncludeHalfPicture{Pictures/frey_unit.eps}
    \hfill
    \caption{\small Зависимость $Q_\varepsilon(A)$ от~$\epsilon$
             для~единичной окрестности при $L=100$, $\ell=60$, $h=10$, $m=20$.}
    \label{fig:Unit}
    \end {multicols}
\end{figure}

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
        \IncludeHalfPicture{Pictures/frey_p.eps}
    \hfill
    \caption{\small Зависимость $Q_\varepsilon(A)$ от~$h$ для связки
             из~монотонных цепей
             при $L=300$, $\ell=150$, $m=15$, $D = 1, 2, 3, 5, 10$, $\epsilon = 0.05$.}
    \label{fig:pBinding}
    \medskip
    \hfill
        \IncludeHalfPicture{Pictures/frey_D.eps}
    \hfill
    \caption{\small Зависимость $Q_\varepsilon(A)$ от~$D$ для связки
            из~$h = 1, 2, 3, 5, 10$ монотонных цепей
            при $L=300$, $\ell=150$, $m=15$, $\epsilon = 0.05$.}
    \label{fig:dBinding}
    \end {multicols}
\end{figure}

На~рис.\,\ref{fig:Monot}~и~рис.\,\ref{fig:Unit} представлены результаты численных экспериментов,
в~которых сравнивались вероятности переобучения для различных вариантов минимизации эмпирического риска.
Из~четырех кривых на~каждом графике верхняя (жирная) соответствует
пессимистической минимизации эмпирического риска,
нижняя "--- оптимистической.
Две почти сливающиеся кривые между ними соответствуют РМЭР.
\mbox{Одна из~кривых} вычислена по~доказанным формулам,
а~вторая построена методом Монте-Карло по~$10^5$~случайных разбиений,
при равновероятном выборе лучшего алгоритма в~случаях неопределенности.
Различия этих двух кривых находятся в~пределах погрешности метода Монте"=Карло.

На~рис.\,\ref{fig:pBinding}~и~рис.\,\ref{fig:dBinding} представлены зависимости вероятности переобучения
от~числа~$h$ ветвей в~связке и~от их~длины~$D$.
Графики построены для РМЭР.
Рис.\,\ref{fig:dBinding} показывает, что при увеличении длин цепей~$D$ вероятность переобучения
практически перестает расти уже при $D=7$.
Это~связано с~\emph{эффектом расслоения} "--- лишь алгоритмы из~нижних слоев
имеют существенно отличную от~нуля вероятность быть выбранными
методом минимизации эмпирического риска.
Добавление <<слишком плохих>> алгоритмов не~увеличивает вероятность переобучения.
Рис.\,\ref{fig:pBinding} показывает, что
при~увеличении числа цепей ($h$) в~связке вероятность переобучения продолжает расти.
Однако скорость роста сублинейна по~$h$ благодаря \emph{эффекту связности} "---
все алгоритмы находятся на~хэмминговом расстоянии не~более~$D$ от~лучшего алгоритма.

\subsection{Многомерная монотонная сеть алгоритмов}

Введем целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \ldots, h} |d_j|$,\:
$|\vec d| = |d_1| + \ldots + |d_h|$.
На~множестве векторов индексов введем покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из~неравенств строгое.

\begin{Def}
    Множество алгоритмов
    $A = \bigl\{a_{\vec d}\bigr\}$, где~$\vec d \geq 0$ и~$\|\vec d\|\leq D$,
    называется \emph{монотонной $h$"~мерной сетью алгоритмов длины $D$},
    если существуют $h \in \NN$ и~упорядоченные наборы объектов
    $X_j = \{x_j^1, \ldots, x_j^D\} \subset \XX$ для всех $j = 1, \ldots, h$,
    а~также множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что:
    \begin{enumerate}
        \item[1)] набор $\bigl\{ U_0, U_1,\{X_j\}_{j=1}^h \bigr\}$ является разбиением множества~$\XX$
              на~непересекающиеся подмножества;
        \item[2)] $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item[3)] $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item[4)] $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Def}

Монотонная сеть алгоритмов "--- это модель параметрического \emph{связного семейства алгоритмов},
предполагающая, что при непрерывном удалении каждой компоненты вектора параметров
от~оптимального значения число ошибок на~полной выборке только увеличивается.

Обозначим $|U_1| = m$.
Из определения следует, что $n(a_{\vec d}, \XX) = m + |d|$.
Алгоритм $a_{\vec 0}$ является \emph{лучшим в~сети}.
Множество алгоритмов с~равным числом ошибок $t+m \brop= n(a_{\vec d}, \XX)$ называются \emph{$t$-слоем} сети.

\begin{Example}
    Монотонная двумерная сеть при $m = 0$ и~$L = 4$:
     \[
        \bordermatrix{
             & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}

Число алгоритмов в~$h$-мерной монотонной сети с~ветвями длины $D$ равно $(D + 1)^h$.
\emph{Укороченной} $h$-мерной монотонной сетью $\tilde{A} \subset A$
назовем первые $D$ слоев из~$A$.
Таким образом, \[\tilde{A} = \{a_{\vec d} \in A,\, |\vec d| \leq D\}.\]
Число алгоритмов в~$\tilde{A}$ равно $\Binom{D+h}{h}$.

Пример матрицы ошибок монотонной сети приведен на~рис.~\ref{fig:MonotonicSets}.

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \IncludeHalfPicture{Pictures/netpics_M.eps}
    \hfill
    \medskip
    \hfill
    \IncludeHalfPicture{Pictures/netpics_MS.eps}
    \hfill
    \end {multicols}
    \caption{\small Матрица ошибок монотонной сети (слева) и~укороченной монотонной сети (справа)
    при $D=20$, $h=2$, $m=5$, $L=60$.}
    \label{fig:MonotonicSets}
\end{figure}

Впервые монотонные сети произвольной размерности были изучены П. Ботовым в~\cite{botov09mmro,botov11mmro,botov11pria}.
Там же были получены формулы для~вероятности переобучения \emph{пессимистического} метода минимизации
эмпирического риска.

Численные эксперименты показывают, что при разумных сочетаниях параметров
вероятность переобучения РМЭР для укороченной сети $\tilde{A}$ и~для~простой сети~$A$ различаются крайне мало.
Поэтому в~дальнейшем мы ограничимся исследованием неукороченных монотонных сетей.
Для этого класса семейств алгоритмов будут получены явные формулы вероятности переобучения РМЭР.

\begin{figure}[t]
    \centering
    \includegraphics[height=80mm]{Pictures/monot_net_illustration.eps}
    \caption{\small Строение множества~$A(X)$ для двумерной монотонной сети; $h=2$, $D=8$.}
    \label{fig:monotonic-net-pzm-structure}
\end{figure}

\begin{Lemma}
\label{th:monotonic-net-sym}
Группа симметрии монотонной сети
размерности $h$ содержит в~качестве подгруппы группу $S_h$ всевозможных
перестановок множеств $X_1, \ldots, X_h$.
\end{Lemma}

\begin{vkProof}
Все алгоритмы $h$"~мерной монотонной сети длины $D$ индексированы
множеством вектор-индексов $\vec d \in \{0, \ldots, D\}^h$.
Здесь число ошибок алгоритма $a_{\vec d} = m + |\vec d|$.

Рассмотрим алгоритм $a_{\vec d} \in A$ и~произвольную $\pi \in S_h$.
По~данному выше определению действия $\pi$ на~$\XX$ получаем, что
${\pi a_{\vec d} = a_{ \pi \vec d}}$, где действие $\pi$ на~вектор $\vec d$
определяется соответствующей перестановкой его координат.
Множество $\{0, \ldots, D\}^h$ сохраняется при применении к~нему
произвольной перестановки координат $\pi \in S_h$.
Поэтому $\forall \vec d \in \{0, \ldots, D\}^h$ выполнено $\pi \vec d \in \{0, \ldots, D\}^h$.
А следовательно, $a_{\pi \vec d} \in A$.
\end{vkProof}

Пусть $Y_h^D$ "--- множество целочисленных неотрицательных невозрастающих последовательностей
из $h$ элементов, каждый из которых не~превосходит $D$;
пусть $|S_h \vec d|$ "--- число различных слов, состоящих из~символов $d_1, \ldots, d_h$.

\begin{Lemma}
\label{th:monotonic-net-orbits}
Пусть $A = \{ a_{\vec d} \}$, ${\|d\| \leq D}$ "--- монотонная сеть длины $D$ с размерностью $h$.
Тогда множество орбит $A$
под действием $S_h$ индексировано всевозможными векторами $\lambda \in Y_h^D$.
Число алгоритмов в~орбите $\omega_\lambda$, где $\lambda = (\lambda_1, \ldots, \lambda_h)$, равно
числу различных слов длины $h$, состоящих из~символов $\lambda_1, \ldots, \lambda_h$:
$|\omega_\lambda| = |S_h \lambda|$.
\end{Lemma}

\begin{vkProof}
Напомним, что вместо действия $S_h$ на~${A = \{ a_{\vec d} \}}$ можно рассматривать действие
$S_h$ на~вектор индексов $\vec d$,
где новое действие определяется соответствующей перестановкой координат вектора $\vec d$.

Рассмотрим орбиту произвольного алгоритма $a_{\vec d}$.
Возьмем перестановку $\pi \in S_h$, упорядочивающую координаты $\vec d$ в~порядке невозрастания.
Положим $\lambda = \pi \vec d$. Построенная таким образом~$\lambda$ лежит в~множестве $Y_h^D$.
При этом различным $\lambda_1$ и~$\lambda_2$ будут соответствовать
различные орбиты действия группы $S_h$ на~$\{a_{\vec d}\}$.

Взаимно"=однозначное соответствие между словами длины $h$ из~символов $\lambda_1, \ldots, \lambda_h$ и
количеством элементов орбиты $|\omega_\lambda|$ очевидно.
\end{vkProof}

\begin{Lemma}
\label{th:monotonic-net-fA}
Для монотонной сети множество $\fA(A) \equiv \bigl\{ A(X) \colon X \in \XXell \bigr\}$
устроено следующим образом:
\[\fA(A) = \{\alpha_{\vec t} \, \colon \, \vec t \in [0, \dots, D]^h \},\]
где $\alpha_{\vec t} = \{a_{\vec d} \,|\, \vec d \leq \vec t\}$.
\end{Lemma}
\begin{vkProof}
Рассмотрим пример структуры множества~$A(X)$, приведенный на~рис.\,\ref{fig:monotonic-net-pzm-structure}.
Из определения монотонной сети следует, что в~$A(X)$ всегда найдется <<крайний>> алгоритм $a_{\vec t}$,
такой что $A(X) = \{a_{\vec d} \,|\, \vec d \leq \vec t\}$.
И наоборот, для любого $\vec t$ легко построить такую выборку $X_{\vec t}$,
для которой $A(X) = \{a_{\vec d} \,|\, \vec d \leq \vec t\}$.
Следовательно, $\fA(A) = \{\alpha_{\vec t} \, \colon \, \vec t \in [0, \dots, D]^h \}$,
и~множества $\alpha_{\vec t}$ устроены так, как утверждается в~настоящей лемме.
\end{vkProof}

\begin{Lemma}
\label{th:monotonic-net-PZM}
Пусть для произвольного вектора индексов $\vec t \geq \vec 0$
множество $J(\vec t)$ обозначает множество тех индексов $j \in \{1, \dots, h\}$,
для которых $t_j < D$ (строго).
Тогда для $\alpha_{\vec t}$ множество порождающих и~запрещающих множеств устроено следующим образом:
\[
    \X_{\vec t} = \bigcup\limits_{j \in J(\vec t)} x^{t_j + 1}_j,
    \quad
    \X'_{\vec t} = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{t_j} x^i_j.
\]
Построенные таким образом $\X_{\vec t}$ и~$\X'_{\vec t}$
являются порождающим и~запрещающим множеством для $\alpha_{\vec t}$.
\end{Lemma}
\begin{vkProof}
Утверждение леммы следует непосредственно из~строения множеств
$\X_{\vec t}$, $\X'_{\vec t}$
и~определения РМЭР~\eqref{eq:mu-rand-ERM}.
\end{vkProof}

\begin{Theorem}
    \label{th:QEps-monotonic-net-dense-sym}
    С учетом симметрий монотонной сети вероятность переобучения записывается в~виде
    \begin{equation}
    \label{eq:QEps-monotonic-net-dense-sym}
            Q_\epsilon(A) = \sum_{\vec d \in Y_h^D}
                                 \sum_{\substack{\vec t \geq \vec d, \\\|\vec t\| \leq D}}
                                 \frac {|S_h \vec d|} {V( \vec t )}
                                 \frac{\Binom{L'}{\ell'}}{\CLl}
                                 \BHyper{L'}{m}{\ell'}{s(\epsilon)},
    \end{equation}
    где
    $Y_h^D$ "--- множество целочисленных неотрицательных невозрастающих последовательностей
    из $h$ элементов, каждый из которых не~превосходит $D$,
    $|S_h \vec d|$ "--- число различных слов, состоящих из~символов $d_1, \ldots, d_h$.
\end{Theorem}

\begin{vkProof}
Для доказательства формулы~\eqref{eq:QEps-monotonic-net-dense-sym} необходимо воспользоваться теоремой~\ref{th:rand-pzm-sym-algs}
и~леммами~\ref{th:monotonic-net-sym},~\ref{th:monotonic-net-orbits},~\ref{th:monotonic-net-fA}
и~\ref{th:monotonic-net-PZM}.
\end{vkProof}

Расчет по~формуле~\eqref{eq:QEps-monotonic-net-dense-sym} требует $O(h \cdot D^h \cdot \Binom{D+h}{h})$ операций.
Оценим, насколько больше операций потребуется, если не~учитывать симметрии.

\begin{Theorem}
    Без учета симметрий
    вероятность переобучения РМЭР,
    примененного к~монотонной сети $A = \{ a_{\vec d} \}$ размерности $h$, $\|\vec d\| \leq D$,
    дается выражением:
    \begin{equation}
        \label{eq:QEps-monotonic-net-dense}
        Q_\epsilon(A) = \sum_{\substack{\vec d \geq \vec 0, \\\|\vec d\| \leq D}}
                             \sum_{\substack{\vec t \geq \vec 0, \\\|\vec t\| \leq D}}
                             \frac {[\vec t \geq \vec d]} {V( \vec t )}
                             \frac{\Binom{L'}{\ell'}}{\CLl}
                             \BHyper{L'}{m}{\ell'}{s(\epsilon)},
    \end{equation}
    где
    ${V(\vec {t}) = \!\prod\limits_{j=1}^h (t_j \!+\! 1)}$,\:
    ${\ell' = \ell - \!\!\sum\limits_{j = 1}^h [t_j \!\neq\! D]}$,\:
    ${k' = k \!-\! |\vec t|}$,\:
    ${L' = \ell' + k'}$,\:
    ${s(\epsilon) = \tfrac \ell L \bigl[ m + |\vec d| - \epsilon k \bigr]}$.
\end{Theorem}

\begin{vkProof}
Для доказательства формулы~\eqref{eq:QEps-monotonic-net-dense} необходимо воспользоваться теоремой~\ref{th:pzm-rand}
и~леммами~\ref{th:monotonic-net-fA} и~\ref{th:monotonic-net-PZM}.
\end{vkProof}

Вычисление вероятности переобучения по~новой формуле, не~учитывающей симметрии, требует $O(h \cdot D^{2h})$ операций.
Рассмотрим отношение $D^h \slash C_{h+D}^h$, показывающее, во~сколько раз увеличился объем вычислений.
Данная величина максимальна при $D \gg h$. Это соответствует случаю сетей большой длины,
на~которых группа симметрии действует наиболее эффективно.
В~этом случае учет симметрий дает выигрыш в~$h!$ раз,
что в~точности соответствует количеству элементов в~группе симметрий.
В остальных случаях (сети больших размерностей и~малой длины) выигрыш оказывается меньше.

\subsection{Многомерная унимодальная сеть алгоритмов}

Унимодальная сеть является более реалистичной моделью связного параметрического семейства
по~сравнению с~монотонной сетью. Если мы имеем лучший алгоритм~$a_0$ c~оптимальным значением
вектора вещественных параметров, то~отклонение значений компонент этого вектора
как в~б\'ольшую, так~и~в~меньшую
сторону приводит к~увеличению числа ошибок.

\begin{Def}
    Множество алгоритмов
    $A \!=\! \bigl\{a_{\vec d}\bigr\}$, где ${\|\vec d\|\!\leq\! D}$,
    называется \emph{унимодальной $h$"~мерной сетью алгоритмов},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j \brop= \{x_j^1, x_j^2, \ldots, x_j^D\} \subset \XX$,\:
    $Y_j = \{y_j^1, y_j^2, \ldots, y_j^D\} \subset \XX$,
    для всех $j = 1, \ldots, h$,
    а~также множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что выполнены условия:
    \begin{enumerate}
        \item[1)] Набор $\bigl\{ U_0, U_1,\{X_j\}_{j=1}^h, \{Y_j\}_{j=1}^h \bigr\}$
              является разбиением множества $\XX$
              на~непересекающиеся множества;
        \item[2)] $a_d(x_j^i) = [d_j > 0] \bigl[ i \leq |d_j| \bigr]$, где $x_j^i \in  X_j$;
        \item[3)] $a_d(y_j^i) = [d_j < 0] \bigl[ i \leq |d_j| \bigr]$, где $y_j^i \in  Y_j$;
        \item[4)] $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item[5)] $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Def}

Заметим, что данное определение отличается от~определения монотонной сети отсутствием ограничения
$\vec d \geq 0$. Число алгоритмов в~$h$-мерной унимодальной сети с~ветвями длины $D$ составляет
$(2 D + 1)^h$.
\emph{Укороченной} $h$-мерной унимодальной сетью $\tilde{A}$ назовем множество первых $D$ слоев из~$A$:
\[\tilde{A} = \{a_{\vec d} \in A \colon n(a_{\vec d}, \XX) \leq m + D\}.\]

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \IncludeHalfPicture{Pictures/netpics_U.eps}
    \hfill
    \medskip
    \hfill
    \IncludeHalfPicture{Pictures/netpics_US.eps}
    \hfill
    \end {multicols}
    \caption{\small Матрица ошибок унимодальной сети (слева) и~укороченной унимодальной сети (справа)
    при $D=10$, $h=2$, $m=5$, $L=60$.}
    \label{fig:UnimodalSets}
\end{figure}

На рис. ~\ref{fig:UnimodalSets} показаны примеры матрицы ошибок унимодальных сетей.

Формула для вероятности переобучения \emph{пессимистического} метода минимизации эмпирического риска
на~укороченных унимодальных сетях также была получена в~\cite{botov09mmro}.
Ниже рассматриваются неукороченные унимодальные сети и~случай рандомизированного МЭР.

\begin{Lemma}
Группа симметрии унимодальной сети размерности $h$~содержит в~качестве подгруппы группу $\Sym(A) = (S_2)^h \times S_h$.
Группа $S_h$ действует на~множестве пар $\left(X_j, Y_j\right)_{j=1}^h$
всеми возможными перестановками;
$j$"~тая группа $S_2$ переставляет объекты множества $X_j$ и~$Y_j$ местами,
сохраняя относительный порядок объектов.
\end{Lemma}

\begin{vkProof}
Заметим, что все алгоритмы $h$"~мерной унимодальной сети длины $D$
индексированы множеством вектор-индексов $\vec d \brop\in \{-D, \ldots, D\}^h$.
При этом число ошибок алгоритма $a_{\vec d}$ равно $n(a_{\vec d}, \XX) = m + |\vec d|$.

Рассмотрим алгоритм $a_{\vec d} \in A$ и~произвольную
$\pi \brop= (z_1, \ldots, z_h) \times \pi_0 \in \Sym(A)$,
где $z_j \in S_2$, $\pi_0 \in S_h$.
По данному выше определению действия $\pi$ на~$\XX$ получаем, что
$\pi a_{\vec d} = a_{ \pi \vec d}$, где действие $\pi$ на~вектор $\vec d$
определяется перестановкой его координат с~помощью $\pi_0$ и~инверсией знаков
для всех $j$, таких что $z_j \neq id$ "--- транспозиция.
Множество $\{-D, \ldots, D\}^h$ сохраняется при применении к~нему
произвольной перестановки координат $\pi \in (S_2)^h \times S_h$.
Поэтому $\forall \vec d \in \{-D, \ldots, D\}^h$ выполнено $\pi \vec d \in \{-D, \ldots, D\}^h$,
следовательно, $a_{\pi \vec d} \in A$.
\end{vkProof}

\begin{Lemma}
\label{th:unimodal-net-orbits}
Пусть $A = \{ a_{\vec d} \}$, $\|d\| \leq D$ "--- унимодальная сеть длины $D$ и размерности $h$.
Тогда множество орбит $A$
под действием $\Sym(A)$ индексировано всевозможными элементами из $Y_h^D$.
Пусть $\lambda = (\lambda_1, \ldots, \lambda_h) \in Y_h^D$. Обозначим через $|S_h \lambda|$
число различных слов длины $h$, состоящих из~символов $\lambda_1, \ldots, \lambda_h$.
Пусть $|\lambda > 0|$ "--- число строго положительных компонент вектора $\lambda$.

Тогда число алгоритмов в~орбите $\omega_\lambda$ равно $|S_h \lambda| \cdot 2^{|\lambda > 0|}$.
\end{Lemma}

\begin{vkProof}
Доказательство полностью повторяет рассуждения леммы~\ref{th:monotonic-net-orbits}.
Множитель $2^{|\lambda > 0|}$ соответствует возможности сменить знак у~всех
ненулевых компонент вектора $\vec d$.
\end{vkProof}

\begin{figure}[t]
    \centering
    \includegraphics[height=80mm]{Pictures/unimod_net_illustration.eps}
    \caption{\small Строение множества~$A(X)$ для двумерной унимодальной сети.}
    \label{fig:unimodal-net-pzm-structure}
\end{figure}

\begin{Theorem}
Вероятность переобучения РМЭР,
примененного к~унимодальной сети $A = \{ a_{\vec d} \}$ размерности $h$, $\|d\| \leq D$,
дается выражением:
\begin{equation}
\label{eq:QEps-unimodal-net}
\begin{aligned}
    Q_\varepsilon(A) & =
                         \sum_{\vec d \in Y_h^D}
                         \sum_{\substack{\vec  t   \geq \vec d, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \frac {|S_h \vec d\,| \cdot 2^{|\vec d > 0|}}
                            {T(\vec t +  \vec {t'})}
                         \frac{\Binom{L'}{\ell'\!}}{\CLl}
                         H_{L'}^{\ell', m}(s_0),
\end{aligned}
\end{equation}
где
$\ell' = \ell - \sum \nolimits_{j = 1}^h \left( [t_j \neq D] + [t'_j \neq D] \right)$,
$k' = k - |\vec t| - |\vec t'|$,
а~остальные обозначения совпадают с~обозначениями теоремы~\ref{th:QEps-monotonic-net-dense-sym}.
\end{Theorem}

\begin{vkProof}
Напомним, что через $\fA(A) = \bigl\{ A(X) \colon X \in \XXell \bigr\}$
обозначалось множество подмножеств алгоритмов, получающихся в~результате обучения.
Пусть~$A$ "--- унимодальная сеть.
Тогда для произвольной обучающей выборки~$X$ множество~$A(X)$ устроено специфическим образом.
На~рис.\,\ref{fig:unimodal-net-pzm-structure} показано, что в~$A(X)$ всегда найдется такая пара алгоритмов $(a_{\vec t_1},\,a_{\vec t_2})$,
что \[A(X) = {\{a_{\vec d} \,|\, \vec t_1 \leq \vec d \leq \vec t_2\}}.\]
Следовательно, $\fA(A) = \{\alpha_{\vec t, \vec t'} \, \colon \, \vec t, \vec t' \in [0, \dots, D]^h \}$.

Обозначим через $J(\vec t)$ множество тех индексов $j \in \{1, \dots, h\}$, для которых $t_j < D$.
Положим
\[
\begin{aligned}
    X_{\vec t}  = \bigcup\limits_{j \in J(\vec t)} x^{t_j + 1}_j,
    \quad
    X'_{\vec t} = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{t_j} x^i_j ;
    \\
    Y_{\vec t'} = \bigcup\limits_{j \in J(\vec t')} y^{t'_j + 1}_j ,
    \quad
    Y'_{\vec t'} = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{t'_j} y^i_j.
\end{aligned}
\]
Множества $X_{\vec t} \cup Y_{\vec t'}$ и~$X'_{\vec t} \cup Y'_{\vec t'}$
являются, соответственно, порождающим и~запрещающим множествами для $\alpha_{\vec t, \vec t'}$.
Применив теорему о порождающих и~запрещающих множествах, получим формулу~\eqref{eq:QEps-unimodal-net}.
\end{vkProof}

\subsection{Разреженные монотонные и~унимодальные сети}

В~предыдущих параграфах рассматривались семейства алгоритмов, реализующиеся при непрерывном изменении компонент вектора вещественных параметров.
На~практике возможны ситуации, в~которых наблюдаемое семейство будет собственным подмножеством рассмотренных выше монотонных и~унимодальных сетей.
В~данном параграфе рассматриваются только такие подмножества, в~которых наложено ограничение на~минимальное расстояние между ближайшими алгоритмами в~семействе.
Эти случаи соответствуют изменению каждой компоненты вектора вещественных параметров с~постоянным шагом.

\begin{Def}
Пусть $\rho \in \NN$ "--- целочисленный параметр;
${A = \{a_{\vec d}\}}$ "--- $h$"~мерная монотонная сеть длины $\rho D$;
$m \equiv n(a_0, \XX)$.
\emph{Разреженной $h$-мерной монотонной сетью $\ddot{A}$ плотности~$\rho$ и~длины $D$}
будем называть подмножество~$A$, заданное условием:
\[
\begin{aligned}
    \ddot{A} =
        %\left\{a_{\vec d} \in A \, \big| \, \forall j = 1, \ldots, h \; \exists d'_j \in \ZZ \colon d_j = \rho d'_j \right\}.
        \bigl\{a_{\vec d} \in A \, \big| \, \vec d \in \left( \tiny \rho \ZZ \tiny \right)^h \bigr\}.
\end{aligned}
\]
\end{Def}

Отметим, что при $\rho > 1$ граф смежности разреженной монотонной сети состоит из~изолированных точек.

\begin{Example}
На~рис.~\ref{fig:sparseMonotonicNetExample}
выделено подмножество двумерной монотонной сети с~параметром $D = 8$,
соответствующее разреженной монотонной сети с~параметрами $\rho = 2$, $D = 4$.
\begin{figure}[t]
    \begin{centering}
    \includegraphics[height=64mm]{Pictures/monot_net_rarefield.eps}
    \caption{\small Двумерная разреженная монотонная сеть при $\rho = 2$, $D = 4$.}
    \label{fig:sparseMonotonicNetExample}
    \end{centering}
\end{figure}
\end{Example}

Заметим, что для разреженной монотонной сети
не~удается выписать систему порождающих и~запрещающих множеств непосредственно
в~виде~\eqref{eq:pzm-rand-mu-representation}.
При этом структура вывода формулы вероятности переобучения сохраняется прежней "---
вначале используется теорема~\ref{th:QAorbit}
о разложении вероятности переобучения по~орбитам действия группы симметрии,
затем определяется структура множества $\fA(A)$
и, после подсчета числа элементов в~получившихся множествах, выписывается финальная оценка.

\begin{Theorem}
\label{th:QEps-monotonic-net-sparse}
Вероятность переобучения РМЭР,
примененного к~разреженной монотонной сети $\ddot{A}_M\! =\! \{ a_{\vec d} \}$ размерности $h$, ${\|d\| \!\leq\! D}$,
дается выражением:
\begin{equation}
\label{eq:QEps-monotonic-net-sparse}
    Q_\varepsilon(\ddot{A}_M) = \sum_{\lambda \in Y_{*}^{h, D}}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell', m}(s_0),
\end{equation}
где
$Y_h^D$ определено в теореме \ref{th:QEps-monotonic-net-dense-sym},
$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на~$\lambda$,
$T(\vec {t}) = \prod_j (t_j + 1)$,\:
$\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,\:
$k' = k - |\vec t|$, $L' = \ell' + k'$,\:
$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,\:
$H_{L'}^{\ell\,', m}(s)$ "--- функция гипергеометрического распределения.
\end{Theorem}

\begin{vkProof}
Воспользуемся теоремой~\ref{th:QAorbit} о разложении вероятности переобучения по~орбитам множества алгоритмов:
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_{*}^{h, D}} |S_h \lambda|
                         \sum_{\Xl \in \XXell}
                         \frac {[a_\lambda \in A_M(\Xl)]} {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 1. }Зафиксируем $\Xl \in \XXell$.
Обозначим через $t_j$ максимальный индекс из~$\{0, \dots, \rho D\}$, при котором все объекты
$\{x_j^1, \dots, x_j^{t_j}\}$ содержатся в~$\Xk$, а~$x_j^{t_j + 1}$\!\!, при его наличии, лежит в~$\Xl$.
Положим $\vec t = \{t_j\}_{j = 1}^h$.
Тогда условие $a_\lambda \in A_M(X)$ перепишется  как $\vec t \geq \rho \lambda$.

Действительно, заметим, что для всех $a \in A_M$ и~$\Xl \in \XXell$ выполнено $n(a, \Xl) \geq n(a_0, \Xl)$.
Следовательно, алгоритм $a_\lambda$ может быть выбран, только если объекты
$x_j^i$ при всех $j = 1, \dots, h$ и~$i \leq \rho \lambda_j$ лежат в~контроле.
В терминах $\vec t$ это записывается как $\vec t \geq \rho \lambda$.

Обозначим множество разбиений на~обучение и~контроль с~фиксированным значением параметра $\vec t$
через $\XXell_{\vec t}$. Тогда
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_h^D} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|A_M(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

\textbf{Шаг 2. } Пусть $\Xl \in \XXell_{\vec t}$.
Заметим, что алгоритм $a_{\vec d} \in A_M(\Xl)$ тогда и~только тогда, когда $\rho \vec d \leq \vec t$.
Следовательно,
\[
    |A_M(\Xl)| = (\lfloor t_1 / \rho \rfloor \!+\! 1) (\lfloor t_2 / \rho \rfloor \!+\! 1) \dots (\lfloor t_h / \rho \rfloor \!+\! 1).
\]
Обозначим~${T(\vec v) = \prod_j (v_j\! +\! 1)}$. Тогда $|A(\Xl)| = T\bigl(\lfloor \vec t / \rho \rfloor\bigr)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из~$U_1$, лежащих в~обучении.
Тогда $\delta (a_\lambda, \Xl) = \frac{m - s + \rho |\lambda|}{k} - \frac{s}{\ell}$, и~условие
$\delta (a_\lambda, \Xl) \geq \epsilon$ запишется в~виде $s \leq \frac \ell L [m + \rho |\lambda| - \epsilon k] \equiv s_0$.
Множество всех разбиений из~$\XXell_{\vec t}$ с~фиксированным параметром~$s$ обозначим
через $\XXell_{\vec t, s}$. Тогда
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_h^D} |S_h \lambda|
                         \!\!\sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \!\!\frac 1 {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         {\sum_{s = 0}^{s_0} \bigl|\XXell_{\vec t, s}\bigr|}.
\]

\textbf{Шаг 4. } Вычислим мощность множества $\XXell_{\vec t, s}$.

Введем обозначения $\ell' = \ell - \sum_{j = 1}^h [t_j \neq \rho D]$,
$k' = k - |\vec t|$, $L'\brop = \ell' + k'$. Тогда простое комбинаторное вычисление показывает, что
$\bigl|\XXell_{\vec t, s}\bigr| = C_m^s C_{L' - m}^{k' - s}$.
Следовательно,
\[
    Q_\varepsilon(A_M) = \frac1{C_L^\ell} \sum_{\lambda \in Y_h^D} |S_h \lambda|
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac 1 {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         {\sum_{s = 0}^{s_0} C_m^s C_{L' - m}^{k' - s}}.
\]
Напомним, что
$H_{L'}^{\ell\,', m}(z) = \sum \limits_{s=0}^{\lfloor z \rfloor} \frac{C_m^{\,s} C_{L'-m}^{\,\ell' - s}}{C_{L'}^{\,\ell'}}$
"--- функция гипергеометрического распределения~\cite{voron09mmro}. Тогда
\[
    Q_\varepsilon(A_M) = \sum_{\lambda \in Y_h^D}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}}
                         \frac {|S_h \lambda|} {T\bigl(\lfloor \vec t / \rho \rfloor\bigr)}
                         \frac{C_{L'}^{\ell'}}{C_L^\ell}
                         H_{L'}^{\ell\,', m}(s_0).
\]
\end{vkProof}

\begin{Def}
Пусть~$\rho \in \NN$ "--- целочисленный параметр;
$A=\{a_{\vec d}\}$, где $\|d\| \leq \rho D$ "--- $h$"~мерная унимодальная сеть;
$m \brop\equiv n(a_0, \XX)$.
\emph{Разреженной $h$"~мерной унимодальной сетью $\ddot{A}$ плотности~$\rho$}
будем называть следующее подмножество~$A$:
\[
\begin{aligned}
    \ddot{A} =
        \bigl\{a_{\vec d} \in A \, \big| \, \vec d \in \left( \tiny \rho \ZZ \tiny \right)^h \bigr\}.
\end{aligned}
\]
\end{Def}

\begin{Theorem}
Вероятность переобучения РМЭР,
примененного к~разреженной унимодальной сети $\ddot{A} = \{ a_{\vec d} \}$ размерности $h$, ${\|d\| \!\leq \!D}$,
дается выражением:
\begin{equation}
\label{eq:QEps-unimodal-net-sparse}
\begin{aligned}
    Q_\varepsilon( A) & =
                         \sum_{\lambda \in Y_h^D}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\,\,\|\vec {t'}\| \leq \rho D}}
                         \!\!\!\mathbb{S}(\lambda, \vec t, \vec {t'}); \\
                          \mathbb{S}(\lambda, \vec t, \vec {t'})
                          & =
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                            {T\bigl(\lfloor \vec t / \rho \rfloor + \lfloor \vec {t'} / \rho \rfloor\bigr)}
                         \frac{\Binom{L'}{\ell\,'}}{\CLl}
                         H_{L'}^{\ell\,', m}(s_0),
\end{aligned}
\end{equation}
где
%$Y_h^D$ "--- множество невозрастающих последовательностей длины $h$ и~не~превосходящих $D$,
%$|S_h \lambda|$ "--- мощность орбиты действия симметрической группы $S_h$ на~$\lambda$,
%$T(\vec t, \vec {t'}) = \prod_j (t_j + t'_j + 1)$,
$\ell' = \ell - \sum \nolimits_{j = 1}^h \bigl( [t_j \neq \rho D] + [t'_j \neq \rho D] \bigr)$,
$k' = k - |\vec t| - |\vec t'|$,
%$L' = \ell' + k'$,
а~остальные обозначения совпадают с~обозначениями теоремы~\ref{th:QEps-monotonic-net-sparse}.
%$s_0 = \frac \ell L [m + \rho |\lambda| - \epsilon k]$,
%$H_{L'}^{\ell', m}(s)$ "--- функция гипергеометрического распределения~\cite{voron09mmro}.
\end{Theorem}

\begin{vkProof}

\textbf{Шаг 1. }
Выберем в~качестве представителя $a_\lambda$ орбиты $\omega_\lambda$
алгоритм, не~допускающий ошибок на
множестве $Y = \bigcup_{j=1}^h Y_j$.
Этого можно добиться, взяв произвольный  $a_{\vec d} \in \omega_\lambda$ и~поменяв
знаки у~всех $d_j < 0$ c помощью транспозиции $z_j$.

Введя обозначения $\vec t$ и~$\XXell_{\vec t}$ так же,
как и~на~первом шаге вывода формулы для монотонной сети,
получим
\[
    Q_\varepsilon(\ddot{A}) = \frac1{\CLl} \sum_{\lambda \in Y_h^D} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec t \geq \rho \lambda, \\\|\vec t\| \leq \rho D}} \sum_{\Xl \in \XXell_{\vec t}}
                         \frac 1 {|\ddot{A}(\Xl)|} \left[ \delta(a_\lambda, \Xl) \geq \epsilon \right].
\]

%\REVIEWNOTE{Я бы подумал над обозначениями. Громоздкие индексы. См. дальше.}
%\REVIEWNOTE{Саша: Legacy code. Пусть будут как есть, ладно?}

\textbf{Шаг 2. }
Обозначим с~помощью $t'_j$ максимальный индекс из~$\{0, \ldots, \rho D\}$, при~ко\-тором все объекты
$\{y_j^1, \ldots, y_j^{t'_j}\}$ содержатся в~$\Xk$, а~$y_j^{t\,'_j + 1}$, при его наличии, лежит в~$\Xl$.
Положим $\vec {t'} = \{t'_j\}_{j = 1}^h$. Заметим, что вектор $\vec {t'}$ играет для набора $\{Y_j\}$ ту же
роль, что $\vec t$ для $\{X_j\}$. Обозначим через $\XXell_{\vec t, \vec {t'}}$ множество разбиений с
фиксированными параметрами $\vec t$ и~$\vec {t'}$.

Пусть $\Xl \in \XXell_{\vec t, \vec {t'}}$.
Заметим, что $[a_{\vec d} \in \ddot{A}(\Xl)] = [-\vec {t'} \leq \rho \vec d \leq \vec t].$
Следовательно, $|\ddot{A}(\Xl)| = T\bigl(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor\bigr)$.

\textbf{Шаг 3. } Обозначим через $s = |U_1 \cap \Xl|$ число объектов из~$U_1$, лежащих в~обучении.
Пусть $s_0 \equiv \frac \ell L [m + \rho |\lambda| - \epsilon k]$.
Повторяя рассуждения аналогичного шага теоремы \ref{th:QEps-monotonic-net-sparse}, получим
\[
    Q_\varepsilon(\ddot{A}) = \frac1{\CLl} \sum_{\lambda \in Y_h^D} |S_h \lambda| \cdot 2^{|\lambda > 0|}
                         \sum_{\substack{\vec  t   \geq \lambda, \\\|\vec  t  \| \leq D}}
                         \sum_{\substack{\vec {t'} \geq 0,       \\\|\vec {t'}\| \leq D}}
                         \!\!\frac 1 {T(\vec t, \vec {t'})}
                         {\sum_{s = 0}^{s_0} \bigl|\XXell_{\vec t, \vec {t'}, s}\bigr|}.
                \]

\textbf{Шаг 4. } Посчитаем мощность множества $\XXell_{\vec t, \vec {t'}, s}$.

Обозначим $\ell' = \ell - \sum \nolimits_{j = 1}^h \bigl( [t_j \neq \rho D] + [t'_j \neq \rho D] \bigr)$,\:
$k' = k - |\vec t| - |\vec t'|$, $L' = \ell' + k'$. Тогда
$\bigl|\XXell_{\vec t, \vec {t'}, s}\bigr| = \Binom{m}{s} \Binom{L' - m}{k' - s}$.
Воспользовавшись определением функции гипергеометрического распределения, получим:
\[
    Q_\varepsilon(A) = \sum_{\lambda \in Y_h^D}
                         \sum_{\substack{\vec  t   \geq \rho \lambda, \\\|\vec  t  \| \leq \rho D}}
                         \sum_{\substack{\vec {t'} \geq 0,            \\\|\vec {t'}\| \leq \rho D}}
                         \frac {|S_h \lambda| \cdot 2^{|\lambda > 0|}}
                               {T\bigl(\lfloor \vec t / \rho \rfloor + \lfloor \vec t' / \rho \rfloor\bigr)}
                         \frac{\Binom{L'}{\ell'}}{\CLl}
                         H_{L'}^{\ell', m}(s_0).
\]

\end{vkProof}

\begin{figure}[t]
    \begin {multicols}{2}
    \centering
    \hfill
    \IncludeHalfPicture{Pictures/rarefield_monotonic.eps}
    \hfill
    \caption{\small Зависимость $Q_\eps(\ddot{A})$ от~разреженности $\rho$ монотонной сети
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1,2,3,4$.}
    \label{fig:MonotRarefieldNet}
    \medskip
    \hfill
    \IncludeHalfPicture{Pictures/rarefield_unimodal_vs_monotonic.eps}
    \hfill
    \caption{\small Зависимости вероятности переобучения $Q_\eps(\ddot{A})$ для разреженной монотонной
    и~унимодальной сетей от~$\rho$
    при $L=150$, $\ell=90$, $\epsilon = 0.05$, $D=3$, $m=5$, $h=1(2), 2(4)$.}
    \label{fig:UnimodRarefieldNet}
    \end {multicols}
\end{figure}

Приведем результаты численных расчетов,
иллюстрирующих поведение вероятности переобучения
монотонной и~унимодальной разреженных сетей.
Расчеты выполнены с~помощью доказанных выше формул
\eqref{eq:QEps-monotonic-net-sparse},~\eqref{eq:QEps-unimodal-net-sparse}.

На~рис.\,\ref{fig:MonotRarefieldNet} изображена зависимость
вероятности переобучения $h$"~мерной монотонной сети от~разреженности~$\rho$ (при $h\!=\!1, 2, 3, 4$).
При~увеличении размерности вероятность переобучения возрастает.
При~увеличении разреженности $\rho$ вероятность переобучения падает и~вскоре выходит на~константу,
соответствующую вероятности переобучения лучшего алгоритма семейства $a_0$.
Это связано с~тем, что с~уменьшением плотности семейства
возрастает роль \emph{явления расслоения}~\cite{voron09dan, voron09mmro}.

На~рис.\,\ref{fig:UnimodRarefieldNet} приведены результаты сравнения разреженных
$h$"~мерных унимодальных сетей
с~разреженными $2h$-мерными монотонными сетями при~$h=1$ и~$h=2$.
Тонкая серая кривая соответствует вероятности переобучения для~унимодальной сети.
Полученные результаты подтверждают гипотезу~\cite{botov09mmro} о~связи вероятности переобучения для~унимодальных
сетей с~вероятностью переобучения монотонных сетей удвоенной размерности.

\section{Плотные семейства}

Во всех предыдущих параграфах рассматривались модельные семейства алгоритмов,
в~которых число алгоритмов увеличивалось полиномиально по~параметру $h$.
В данном параграфе будут рассмотрены более плотные модельные семейства алгоритмов,
не~обладающие параметром размерности.
В дальнейшем эти семейства будут использованы
в~качестве объемлющих множеств $B$ (см. лемму~\ref{th:LayerGr})
для эффективного вычисления оценки~\eqref{eq:pzm-cluster}.

\subsection{Слой хэммингова шара}
Напомним, что расстояние между алгоритмами $\rho(a, a')$
определялось как расстояние Хэмминга между их~векторами ошибок:
\[\rho(a, a') = \sum \limits_{x \in \XX} |I(a, x) - I(a', x)|.\]
\begin{Def}
\emph{Центральным слоем хэммингова шара} называется множество
\[
    B_r(a_0) = \{a \in \{0, 1\}^L \colon n(a, \XX) = n(a_0, \XX), \text { и~} \rho(a, a_0) \leq r\}.
\]
\end{Def}
Данное множество получается сечением \emph{шара алгоритмов}
$\{a \in \{0, 1\}^L \colon \rho(a, a_0) \leq r\}$
и~\emph{слоя алгоритмов} $A_m = \{a \in \{0, 1\}^L \colon n(a, \XX) = m\}$ при $m = n(a_0, \XX)$.

Центральный слой хэммингова шара алгоритмов не~имеет явного аналога среди реальных семейств алгоритмов.
Однако изучение этого модельного семейства представляет значительный теоретический интерес,
поскольку слой хэммингова шара является максимально связным множеством булевых векторов.
Это делает~его привлекательным примером
для~изучения влияния эффекта сходства на~вероятность переобучения.

Впервые шар алгоритмов и~его центральный слой были изучены в~работе~\cite{tolstikhin10iip}.
В частности, были доказаны леммы о группе симметрий данных множеств и~о структуре орбит.
Кроме этого, с~помощью теоремы~\ref{th:QAXorbit}
были получены точные формулы вероятности переобучения РМЭР для этих семейств.
В настоящей работе приводится более простой вариант доказательства
формулы вероятности переобучения центрального слоя шара,
основанный на~теореме~\ref{th:QXorbit}.

%Количество алгоритмов в~$B_r^m(a_0)$
%дается выражением $\sum \nolimits_{i = 0}^{\lfloor r / 2 \rfloor} \Binom{m}{i} \Binom{L-m}{i}$.
%Данная величина чрезвычайно быстро растет с~увеличением радиуса $r$.
%Мы покажем, что несмотря на~стремительное увеличение числа алгоритмов
%рост вероятности переобучения оказывается весьма незначительным.

Положим без ограничения общности, что алгоритм~$a_0$ ошибается на~первых
$m = n(a_0, \XX)$~объектах генеральной выборки~$\XX$.
В~дальнейшем множество, состоящее из~первых~$m$~объектов $\XX$, мы будем обозначать~$X^m$.
Множество, состоящее из~последних~$L-m$ объектов будем обозначать~$X^{L-m}$.

\begin{Lemma}[Толстихин,~\cite{tolstikhin10iip}]
\label{th:ball-layer-sym}
Группа $S_m \times S_{L-m}$, где $S_m$ и~$S_{L_m}$ "--- симметрические группы перестановок,
действующие на~множествах $X^m$ и~$X^{L-m}$, соответственно, является подгруппой группы
симметрий множества алгоритмов $B_{r}(a_0)$.
\end{Lemma}

%\begin{vkProof}
%    Очевидно, что для~$\pi\in S_m\times S_{L-m}$ справедливы следующие равенства:
%    ${n(a,X^m)=n\bigl(\pi(a),X^m\bigr)}$ и~${n(a,X^{L-m})=n\bigl(\pi(a),X^{L-m}\bigr)}$.
%    Отсюда получаем:
%    \begin{multline*}
%    \rho(a,a_0)=m-n(a,X^m)+n(a,X^{L-m})={}\\
%    {}=m - n\bigl(\pi(a),X^m\bigr) + n\bigl(\pi(a),X^{L-m}\bigr) = \rho\bigl(\pi(a),a_0\bigr).
%    \end{multline*}
%    Таким образом, элементы группы $S_m\times S_{L-m}$ не~меняют расстояния до~центра шара $B_{r_0}(a_0)$.
%
%    Также заметим, что действие элементов симметрической группы~$S_L$ не~меняет числа ошибок алгоритмов.
%    Поскольку симметрическая группа действует на~алгоритмы инъективно (у~каждой перестановки~$\pi$ есть обратная ей), приходим к~выводу, что $B_{r}^m(a_0)\brop=\pi\bigl(B_{r}^m(a_0)\bigr)$.
%\end{vkProof}

\begin{Lemma}[Толстихин,~\cite{tolstikhin10iip}]
\label{th:ball-layer-orbits-size}
Орбиты $\tau \in \Omega(\XXell)$ действия группы $S_m \times S_{L-m}$ на~множестве $\XXell$ индексированы параметром $i = |\Xl \cap X^m| \brop= n(a_0, X^\ell)$ "--- числом ошибок алгоритма $a_0$ на~обучении. Мощность орбиты $\tau_i$ записывается в~виде $|\tau_i| = \CLl h_L^{\ell, m}(i)$.
\end{Lemma}

%\begin{vkProof}
%Первое утверждение леммы непосредственно следует из~строения подгруппы симметрий,
%отмеченного в~\ref{th:ball-layer-sym}.
%
%Мощность орбиты $\tau_i$ определяется числом способов независимо выбрать $i$ объектов из~$X^m$ и
%$\ell - i$ объектов из~$X^{L-m}$. Таким образом $|\tau_i| = \CLl h_L^{\ell, m}(i)$.
%\end{vkProof}

\begin{Theorem}
Вероятность переобучения множества алгоритмов, получаемого сечением
шара алгоритмов центральным~$m$-слоем, дается в~виде
\[
    Q_\mu(\epsilon, A)
    =
    H_L^{\ell, m}\bigl(\frac \ell L (m - \epsilon k) + \big\lfloor r/2 \big\rfloor\bigr) \cdot [m \geq \eps k],
\]
$H_{L}^{\ell, m}(s)$ "--- функция гипергеометрического распределения.
\end{Theorem}

%\REVIEWNOTE{Нужно ли тут условие $r \leq min(m, L - m)$? alfrey: нет, не~нужно!}

\begin{vkProof}
Заметим, что утверждение лемм~\ref{th:ball-layer-sym} и~\ref{th:ball-layer-orbits-size} верно и~для
сечения шара центральной плоскостью.
Поскольку все алгоритмы имеют равное число ошибок на~полной выборке, применим следствие~\ref{th:QXorbit-layer}
из~теоремы о разложении вероятности переобучения по~орбитам разбиений выборки:
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{m}
    h_L^{\ell, m}(i) \left[\min_{a \in A} n(a, X_i) \leq \frac \ell L(m - \epsilon k)\right].
\]

Напомним, что по~определению $i = |\Xl \cap X^m|$. Пусть $r' = \big\lfloor \frac r 2 \big\rfloor$.
Тогда выполнено следующее утверждение:
\[
    \min_{a \in A} n(a, X_i) =
    \begin{cases}
        & 0, \text{ при } i \leq r', \\
        & i - r', \text{ при } i > r'.
    \end{cases}
\]
Следовательно
\[
    Q_\mu(\epsilon, A)
    =
    \sum_{i=0}^{\lfloor s_d(\epsilon)\rfloor + r'}
    h_L^{\ell, m}(i) =
    H_L^{\ell, m}
        \bigl(
            \frac \ell L (m - \epsilon k) + \big\lfloor r/2 \big\rfloor
        \bigr)
    \cdot [m \geq \eps k],
\]
\end{vkProof}

%\section{*Хэммингов шар}
%
%В~прошлом разделе был рассмотрен пример наиболее <<плотного>> множества, наделенного свойством связности, но не~обладающего свойством расслоения.
%Теперь мы изучим совместное влияние сходства и~расслоения на~вероятность переобучения.
%Хэммингов шар "--- наиболее подходящее для~этого модельное семейство.
%Это пример наиболее <<плотного>> из~одновременно связных и~расслоенных семейств алгоритмов.
%
%%\REVIEWNOTE{В применении к~семейству алгоритмов мы используем термины "компактное", "плотное", "связное", "расслоенное", "разреженное".
%%По-моему "компактное" и~"плотное" это синонимы (поправь меня если это не~так). Давай использовать один из~них.
%%Согласен ли ты что "Связное" и~"Разреженное" --- это антонимы? Для меня целью изучения \emph{разреженных} сетей было понять, как
%%уменьшение связности влияет на~вероятность переобучения. Разреженность можно определить численно: напимер в~качестве игрушечного определения
%%можно взять минимальное расстояние между парой алгоритмов: $sparse(A) = \min_{a, a' \in A} \rho(a, a')$.
%%Есть ли у~тебя какое-нибудь определение (пусть даже и~очень игрушечное) для плотности?}
%
%Пусть семейство алгоритмов~$A$ представляет собой хэммингов шар $B_{r_0}(a_0)$, где~$n(a_0,\mathbb{X})=m$.
%Пусть, без ограничения общности, алгоритм~$a_0$ допускает ошибки на~первых~$m$ объектах генеральной выборки $\mathbb{X}$.
%
%Получим точную оценку вероятности переобучения для~хэммингова шара алгоритмов.
%
%Множество,~состоящее из~первых~$m$ объектов генеральной выборки $\mathbb{X}$, будем обозначать $X^m$, а
%множество, состоящее из~последних $L-m$ объектов, будем обозначать $X^{L-m}$.
%
%%\REVIEWNOTE{Предлагаю тебе взять на~себя ответственность ("take an ownership") за предыдущий параграф --- слой хэммингова шара.
%%Очевидно что нужно переместить туда определения $X_m$, $X_{L-m}$, доказательства леммы~\ref{leTolstBallSubGroup}, и~т.д.
%%Нужно как-то аккуратно объяснить что слой шара и~шар --- это разные семейства, но группы симметрии и~строение орбит у~них похожи.
%%Мне нравится что вначале мы рассматриваем слой шара (потому что он проще).}
%
%\begin{Lemma}\label{leTolstBallSubGroup}
%    Группа $S_m\times S_{L-m}$, где $S_m$ и~$S_{L-m}$ "--- симметрические группы перестановок, действующих на~множествах $X^m$ и~$X^{L-m}$ соответственно, является подгруппой группы симметрии семейства алгоритмов~$A$.
%\end{Lemma}
%\begin{vkProof}
%    Очевидно, что для~$\pi\in S_m\times S_{L-m}$ справедливы следующие равенства:
%    ${n(a,X^m)=n\bigl(\pi(a),X^m\bigr)}$ и~${n(a,X^{L-m})=n\bigl(\pi(a),X^{L-m}\bigr)}$.
%    Отсюда получаем:
%    \begin{multline*}
%    \rho(a,a_0)=m-n(a,X^m)+n(a,X^{L-m})={}\\
%    {}=m - n\bigl(\pi(a),X^m\bigr) + n\bigl(\pi(a),X^{L-m}\bigr) = \rho\bigl(\pi(a),a_0\bigr).
%    \end{multline*}
%    Таким образом, элементы группы $S_m\times S_{L-m}$ не~меняют расстояния до~центра шара $B_{r_0}(a_0)$.
%
%    Тогда, если $a\in B_{r_0}(a_0)$, то~и~$\pi(a)\in B_{r_0}(a_0)$.
%    Поскольку действие элементов симметрической группы на~алгоритмы инъективно, приходим к~выводу, что $B_{r_0}(a_0)\brop=\pi\bigl(B_{r_0}(a_0)\bigr)$.
%\end{vkProof}
%
%%\REVIEWNOTE{Поправь нижние индексы у~алгоритма: если всмотреться, $a_0$ (а-ноль) и~$a_0$ (а-буква-"о") выглядят по-разному.}
%
%%\REVIEWNOTE{Термин "инъективность" --- это когда для разные алгоритмов $a_1 \neq a_2$ образы тоже различаются: $\pi(a_1) \neq \pi_(a_2)$. Правильно?
%%Если так --- предлагаю написать почему действие группы инъективно. А именно, потому что у~каждого $\pi$ есть обратный.}
%%\REVIEWNOTE{Мне кажется,~это понятно само собой...}
%
%
%\begin{Lemma}\label{leTolstBallOrbA}
%    Орбитами действия группы $S_m\times S_{L-m}$ на~множестве алгоритмов~$A$ являются пересечения слоев $m-r_0,\ldots,m+r_0$ со~сферами радиусов $0,1,\ldots,r_0$ и~центрами в~алгоритме~$a_0$.
%\end{Lemma}
%\begin{vkProof}
%    Пусть алгоритм $a\in A_p$: $d(a,a_0)=r_1$.
%    Мы установили, что в~этом случае $d\bigl(\pi(a),a_0\bigr)=d(a,a_0)=r_1$.
%    Также мы знаем, что действие перестановки на~алгоритм не~меняет числа его ошибок на~$\mathbb{X}$.
%    Таким образом, $\pi(a)$ также принадлежит пересечению $p$"~го слоя со сферой радиуса $r_1$.
%
%    Осталось показать, что для любых $a_1,a_2 \in A_p$ таких, что $d(a_1,a_0)=d(a_2,a_0)\brop=r_1$, найдется перестановка $\pi\in S_m\times S_{L-m}$, при которой $\pi(a_1)=a_2$.
%    Но это следует из~того, что $n\left(a_1,X^m\right)\brop=n\left(a_2,X^m\right)$ и~$n\left(a_1,X^{L-m}\right)=n\left(a_2,X^{L-m}\right)$.
%    Последний факт легко установить, выразив число ошибок, допускаемых алгоритмами~$a_1$ и~$a_2$ на~множестве~$X^m$, через~$m$ и~$r_1$.
%\end{vkProof}
%
%\begin{figure}[t]
%    \centering
%    \includegraphics[width=88mm]{Pictures/BScheme}
%    \caption{Алгоритмы из~орбит семейства~$A$.}
%    \label{fig4}
%\end{figure}
%
%На~рис.\,\ref{fig4} для~наглядности представлено по~одному алгоритму из~каждой орбиты семейства~$A$.
%Пронумеруем орбиты двумя целочисленными индексами следующим образом: алгоритмы из~орбиты~$\mathop{\rm Orb}(r,n)$,\; $r=0,\ldots, r_0$,\; $n=0,\ldots, r$, принадлежат пересече\-нию сферы радиуса~$r$ с~центром в~$a_0$ со~слоем~$r+m-2n$.
%Обратим внимание на~то, что алгоритм $a_{(r,n)}$ из~орбиты~$\mathop{\rm Orb}(r,n)$
%имеет ${m-n}$ единиц и~$n$ нулей на~множестве~$X^m$ и
%${r-n}$ единиц, ${L-m-r+n}$ нулей на~множестве~$X^{L-m}$.
%Всего в~орбите $\mathop{\rm Orb}(r,n)$ содержится $C^n_m C^{r-n}_{L-m}$ алгоритмов.
%
%В~дальнейшем особую роль будет играть орбита~$\mathrm{Orb}(r_0,r_0)$ "--- в~нее входят алгоритмы шара~$A$, допускающие наименьшее число ошибок на~генеральной выборке~$\mathbb{X}$.
%Вопреки закономерной аналогии с~шаром в~$\mathbb{R}^3$, для~которого пересечение с~касательной состоит из~одной точки, нижний слой хэммингова шара состоит из~$C^{r_0}_m$~алгоритмов.
%%\REVIEWNOTE{Особую роль в~дальнейшем будет играть $\mathop{\rm Orb}(r_0, r_0)$. Предлагаю дать ей некоторое описание --- например,
%%"это множество, состоящее из~алгоритмов шара с~наименьшим количеством ошибок на~полной выборке". Еще было бы интересно узнать, сколько
%%таких алгоритмов "--- возможно, некоторые читатели рисуют себе шар как шар в~$R^3$, у~которого любой "крайний случай" (например, самая низкая горизонтальная
%%касательная плоскость) имеет одну точку пересечения. Нужно разрушать эти иллюзии :)}
%
%\begin{Lemma}
%\label{leTolstBallCr1}
%    Для~любой обучающей выборки~$X$ и~любого алгоритма~$a \in A \setminus \mathop{\rm Orb}(r_0,r_0)$ выполнено:
%    \[
%        a \in A(X) \Leftrightarrow n(a,X)=0.
%    \]
%\end{Lemma}
%
%%\REVIEWNOTE{Не хватает текстовой расшифровки утверждения леммы. Например: алгоритм не~из~$Orb(r_0, r_0)$ может лежать в~$A(X)$ только если он корректен (не допускает ошибок) на~обучении.}
%
%\begin{vkProof}
%    Достаточность очевидна.
%    Докажем необходимость.
%
%    Введем обозначения:
%    $X^{\ell_1} = |X^m\cap X|$,\;
%    $X^{\ell_2} = |X^{L-m} \cap X|$,\;
%    $\ell_1=|X^{\ell_1}|$,\;
%    $\ell_2=|X^{\ell_2}|$,\;
%    $\ell=|X|$,\;
%    $\ell=\ell_1+\ell_2$,\;
%    $X= X^{\ell_1}\cup X^{\ell_2}$.
%
%    Пусть $a\in A(X)$ и~$a$ принадлежит орбите $\mathop{\rm Orb}(r,n)$, отличной от~$\mathop{\rm Orb}(r_0,r_0)$.
%    Докажем, что алгоритм~$a$ не~допускает ошибок на~обучающей выборке~$X$.
%
%    Начнем с~рассмотрения случая $\ell_1\leqslant r_0$.
%    Алгоритмы орбиты $\mathop{\rm Orb}(\ell_1,\ell_1)$ имеют ровно $\ell_1$~нулей на~множестве~$X^m$ и~не допускают ни одной ошибки на~множестве~$X^{L-m}$.
%    Очевидно, существует алгоритм ${a^{\ell_1}\in \mathop{\rm Orb}(\ell_1,\ell_1)}$, такой что $n\left(a^{\ell_1},X\right)=0$.
%
%    В~случае $\ell_1>r_0$ в~орбите $\mathop{\rm Orb}(r_0,r_0)$~существует алгоритм $a^{r_0}$ для которого $n\left(a^{r_0},X^{L-m}\right)=0$ и~$n\left(a^{r_0},X^m\right)=\ell_1-r_0$.
%    Поскольку $n\left(a,X^m\right)\geqslant \ell_1-n\geqslant \ell_1-r_0=n\left(a^{r_0},X^m\right)$, то~мы приходим к~противоречию с~тем, что $a\in A(X)$.
%\end{vkProof}
%Таким образом, алгоритм не~из~$\mathrm{Orb}(r_0, r_0)$ может лежать в~$A(X)$ только если он не~допускает ошибок на~обучающей выборке.
%
%\begin{Corollary}\label{CorTolstBall1}
%    В~ходе доказательства леммы~\ref{leTolstBallCr1} также \mbox{установлено}, что при ${|X\cap X^{m}|>r_0}$ алгоритмы из~орбит, отличных от~$\mathop{\rm Orb}(r_0,r_0)$, не~могут попасть во~множество~$A(X)$.
%\end{Corollary}
%\begin{Lemma}\label{leTolstBallCr2}
%    Для~любого алгоритма~$a \in \mathop{\rm Orb}(r_0,r_0)$ выполнено:
%
%    если выборка $X \in \XXell$ такова, что $|X\cap X^m| \leqslant r_0$, то:
%    \[
%        a \in A(X) \Leftrightarrow n(a,X)=0;
%    \]
%
%    если~$X$ такова, что $|X\cap X^m| > r_0$, то:
%    \[
%        a \in A(X) \Leftrightarrow
%        n\left(a,X\cap X^{m}\right)=|X\cap X^m| - r_0.
%    \]
%\end{Lemma}
%\begin{vkProof}
%    Случай $|X\cap X^m|\leqslant r_0$ повторяет первую часть доказательства леммы~\ref{leTolstBallCr1}.
%    Рассмотрим случай $|X\cap X^m|>r_0$.
%
%    Поскольку в~этом случае ни один алгоритм из~множества~$A$ не~допускает меньше $|X\cap X^m|-r_0$~ошибок на~обучающей выборке~$X$,
%    а~алгоритмы из~$\mathop{\rm Orb}(r_0,r_0)$ не~ошибаются на~объектах множества $X^{L-m}$, то~достаточность очевидна.
%    Необходимость вытекает из~того факта, что существует алгоритм~$a\in \mathop{\rm Orb}(r_0,r_0)$, такой что $n(a,X)=n(a,X^m)=|X\cap X^m|-r_0$.
%\end{vkProof}
%\begin{Lemma}\label{leTolstOrbX}
%    Орбитами действия группы $S_m\times S_{L-m}$ на~множестве разбиений являются множества  $\left\{ X: |X\cap X^{m}|=i_0\right\}$, где $i_0=\max(0,m-k),\ldots,\min(l,m)$ .
%\end{Lemma}
%\begin{vkProof}
%    Утверждение леммы очевидным образом следует из~определения действия группы~$S_m\times S_{L-m}$ на~множество разбиений.
%\end{vkProof}
%%Теорема - точная оценка для шара
%\begin{Theorem}[Точная оценка для хэмингова шара]\label{Ball}
%    Пусть $n(a_0,\mathbb{X})=m$.
%    Рассмотрим шар алгоритмов $B_{r_0}(a_0)$.
%    Тогда при~обучении методом РМЭР а~и~$r\leqslant\min\left(m,L-m\right)$ вероятность переобучения может быть записана в~следующем виде:
%    \[
%        Q_{\varepsilon}(A)
%        =\!\!
%        \sum_{\substack{i=m-k;\\i\geqslant 0}}^{r_0} \!\!\! h^{\ell,m}_L(i) \:
%        \frac{
%                \mathop{\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r}}
%                \limits_{
%                            \mbox{\tiny{$m+r-2n\geqslant\varepsilon k$}}
%                }
%                S(n,r,i)
%            }
%            {\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r} S(n,r,i)}
%        +
%        \sum_{i=r_0+1}^{\lfloor s(\varepsilon)\rfloor}h^{\ell,m}_L(i),
%    \]
%    где
%    $S\left(n,r,i\right)=C^{n-i}_{m-i} C^{r-n}_{k-m+i}$,\:
%    $s(\varepsilon)=\frac{\ell}{L}(m\brop-\varepsilon k) + \frac{r_0k}{L}$,\:
%    $h^{\ell,m}_L(i)\brop=\frac{C^i_m C^{\ell-i}_{L-m}}{C^{\ell}_L}$.
%\end{Theorem}
%
%%\REVIEWNOTE{Запись ограничение $m + r - 2n \geq \eps k$ под двумя суммами выглядит несколько странно. Я догадался, но предлагаю текстом объяснить что это значит.
%%В качестве альтернативы предложу воспользоваться воронцовскими квадратными кавычками (нотацией Айверсона)? Но, возможно, это менее эстетично --- в~твоей записи формулы очень хорошо выровнены.}
%
%%Доказательство теоремы о точной оценке для шара
%\begin{vkProof}
%    Доказательство основано на~применении теоремы~\ref{th:QAXorbit}, которая позволяет представить вероятность переобучения в~виде:
%    \[
%        Q_\eps(A)
%        =
%        \sum_{\omega\in\Omega(A)}
%        \frac{|\omega|}{\CLl}
%        \sum_{\tau\in\Omega\XXell}
%        |\{X\in \tau\colon a_{\omega}\in A(X)\}|
%        \frac{\bigl[\delta(a_{\omega},X_\tau) \geq \eps \bigr]}{|A(X_\tau)|},
%    \]
%    где $\Omega(A)$ "--- орбиты алгоритмов, а~$\Omega\XXell$ "--- орбиты разбиений.
%
%    C~учетом лемм~\ref{leTolstBallSubGroup},~\ref{leTolstBallOrbA} и~\ref{leTolstOrbX}:
%    \[
%    Q_{\varepsilon}(A)=
%        \sum_{r=0}^{r_0}\sum_{n=0}^r
%        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
%        \sum_{\substack{i=m-k;\\i\geqslant0}}^{\min(l,m)}
%        \frac{ S\left(i,r,n\right) \left[\delta\left(a_{(r,n)},X_i\right)\geqslant \varepsilon\right]}{|A(X_i)|},
%    \]
%    где
%    $S\left(i,r,n\right) = \bigl|\{X\colon|X \cap X^m|=i,\;a_{(r,n)}\in A(X)\}\bigr|$,\:
%    $a{(r,n)}$ "--- алгоритмы, представленные на~рис.~\ref{fig4},
%    а~$X_i$ "--- произвольное разбиение, в~обучающую выборку~которого входят ровно $i$~объектов из~$X^m$.
%
%%    \REVIEWNOTE{Предлагаю начать доказательства с~копирования общей формулы из~теоремы~\ref{th:QAXorbit}.}
%
%    Разобьем суммирование по~орбитам разбиений (по $i$) на~два слагаемых: $i \leq r_0$ и~$i > r_0$.
%    \begin{align*}
%    Q_{\varepsilon}&(A)={}\\
%    {}=
%        &\sum_{r=0}^{r_0}\sum_{n=0}^r
%        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
%        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
%        S\left(i,r,n\right)
%        \frac{\left[\delta\left(a_{(r,n)},X_i\right)\geqslant \varepsilon\right]}{|A(X_i)|}+{}\\
%        {}+
%        &\sum_{r=0}^{r_0}\sum_{n=0}^r
%        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
%        \sum_{i=r_0+1}^{\min(l,m)}
%        \!\!\!S\left(i,r,n\right)
%        \frac{\left[\delta\left(a_{(r,n)},X_i\right)\geqslant \varepsilon\right]}{|A(X_i)|}.
%    \end{align*}
%
%%  REVIEWNOTE{Оформить предыдущую формулу так, что бы знаки двойного суммирования из~разных строчек находились на~одной вертикали. Этого можно добиться окружение aligned и~знаками амперсанда $\&$.}
%
%    Из лемм~\ref{leTolstBallCr1} и~\ref{leTolstBallCr2}, а~также из~следствия~\ref{CorTolstBall1} следует, что первое слагаемое соответствует случаю выбора алгоритма, не~допускающего ошибок на~обучающей выборке~$X$.
%    Следствие~\ref{CorTolstBall1} позволяет опустить суммирование по~орбитам во~втором слагаемом, поскольку при $i>r_0$ во~множество~$A(X_i)$ попадают только алгоритмы из~$\mathop{\rm Orb}(r_0,r_0)$, допускающие в~соответствии с~леммой~\ref{leTolstBallCr2} ров\-но~$i-r_0$~ошибок на~$X$.
%    С учетом этого:
%    \begin{multline}
%    \label{BallTemp1}
%    Q_{\varepsilon}(A)={}\\
%        {}=
%        \sum_{r=0}^{r_0}\sum_{n=0}^r
%        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
%        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
%        S\left(i,r,n\right)
%        \frac{\left[r+m-2n \geqslant \varepsilon k\right]}{|A(X_i)|}+{}
%    \\
%        {}+
%        \frac{C^{r_0}_m C^{0}_{L-m}}{C^l_L}
%        \sum_{i=r_0+1}^{\min(l,m)}
%        \!\!\!S\left(i,r,n\right)
%        \frac{\left[i\leqslant s(\varepsilon)\right]}{|A(X_i)|}.
%    \end{multline}
%
%    Вычислим значение $S\left(i,r,n\right)$.
%    В случае $i\leqslant r_0$ это число способов выбрать $i$ из~$n$~объектов множества~$X^m$ и~$l-i$ из~${L-m-r+n}$~объектов множества~$X^{L-m}$, на~которых не~ошибается алгоритм~$a_{(r,n)}$.
%    В случае $i > r_0$ "--- число способов выбрать~$i-r_0$~объектов из~множества $X^m$, на~которых алгоритм~$a_{(r,n)}$~ошибается, и~$l-i$ произвольных объектов из~$X^{L-m}$.
%    Итого:
%    \[
%    S\left(i,r,n\right)=
%        \begin{cases}
%            C^i_n C^{l-i}_{L-m-r+n}, & i\leqslant r_0;
%            \\
%            C^{i-r_0}_{m-r_0} C^{l-i}_{L-m}, & i>r_0.
%        \end{cases}
%    \]
%
%    Найдем значения $|A(X_i)|$.
%    В~случае $i>r_0$ в~$A(X_i)$~попадают те алгоритмы $\mathop{\rm Orb}(r_0,r_0)$, которые ошибаются на~множестве~$X^m$ $i-r_0$~раз.
%    При~$i\leqslant r_0$ из~каждой орбиты во~множество~$A(X_i)$ попадают алгоритмы, не~ошибающиеся ни на~одном объекте множеств~$X^m$ и~$X^{L-m}$.
%    Получаем:
%    \[
%    |A(X_i)|=
%        \begin{cases}
%            \sum\limits_{r=0}^{r_0}\sum\limits_{n=0}^{r}C^{n-i}_{m-i}C^{r-n}_{k-m+i}, & i\leqslant r_0;
%            \\
%            C^{r_0}_i, & i>r_0.
%        \end{cases}
%    \]
%
%    Подстановка полученных результатов в~(\ref{BallTemp1}) дает:
%    \begin{multline}
%    \label{BallTemp2}
%    Q_{\varepsilon}(A)={}\\
%    {}=
%        \mathop{\sum_{r=0}^{r_0}\sum_{n=0}^r}\limits_{r+m-2n \geqslant \varepsilon k}
%        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
%        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
%        \frac{C^i_n C^{l-i}_{L-m-r+n}}{\sum\limits_{r_1=0}^{r_0}\sum\limits_{n_1=0}^{r_1}C^{n_1-i}_{m-i}C^{r_1-n_1}_{k-m+i}}+{}
%    \\
%        {}+
%        \frac{C^{r_0}_m}{C^l_L}
%        \sum_{i=r_0+1}^{\min(l,m)}
%        C^{i-r_0}_{m-r_0} C^{l-i}_{L-m}
%        \frac{\left[i\leqslant s(\varepsilon)\right]}{C^{r_0}_i}.
%    \end{multline}
%
%    Во~втором слагаемом:
%    \[
%        \frac{C^{i-r_0}_{m-r_0} }{C^{r_0}_i}=\frac{C^{m-i}_{m-r_0} }{C^{r_0}_i}=\frac{C^{r_0-(i-m+r_0)}_{i-(i-m+r_0)} }{C^{r_0}_i}=
%        \frac{C^{i-m+r_0}_{r_0}}{C^{i-m+r_0}_i}=\frac{C^{m-i}_{r_0}}{C^{m-r_0}_i}.
%    \]
%
%    Далее, $C^{r_0}_m C^{m-i}_{r_0}=C^i_m C^{r_0-m+i}_i=C^i_m C^{m-r_0}_i$.
%    Подстановка полученных формул во~второе слагаемое~(\ref{BallTemp2}) дает:
%    \begin{multline}
%    \label{BallTemp3}
%        \frac{C^{r_0}_m}{C^l_L}
%        \sum_{i=r_0+1}^{\min(l,m)}
%        C^{i-r_0}_{m-r_0} C^{l-i}_{L-m}
%        \frac{\left[i\leqslant s(\varepsilon)\right]}{C^{r_0}_i}={}
%    \\
%        {}=\sum_{i=r_0+1}^{\min(l,m)}
%        h^{l,m}_L(i)
%        \left[i\leqslant s(\varepsilon)\right]
%        =\sum_{i=r_0+1}^{\lfloor s(\varepsilon)\rfloor}h^{l,m}_L(i).
%    \end{multline}
%
%    В~первом слагаемом $C^n_m C^i_n=C^i_m C^{n-i}_{m-i}$, а
%    \[
%        C^{r-n}_{L-m}C^{l-i}_{L-m-r+n} = C^{L-m-r+n}_{L-m}C^{l-i}_{L-m-r+n} \brop= C^{l-i}_{L-m}C^{r-n}_{k-m+i}.
%    \]
%    Заменив порядок суммирования, получим:
%    \begin{multline}
%    \label{BallTemp4}
%        \mathop{\sum_{r=0}^{r_0}\sum_{n=0}^r}\limits_{r+m-2n \geqslant \varepsilon k}
%        \frac{C^n_m C^{r-n}_{L-m}}{C^l_L}
%        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
%        \frac{C^i_n C^{l-i}_{L-m-r+n}}{\sum\limits_{r_1=0}^{r_0}\sum\limits_{n_1=0}^{r_1}C^{n_1-i}_{m-i}C^{r_1-n_1}_{k-m+i}}={}
%    \\
%        {}=
%        \sum_{\substack{i=m-k;\\i\geqslant0}}^{r_0}
%        h^{l,m}_L(i)
%        \frac
%        {\mathop{\sum_{r=\,0}^{r_0}\sum_{n=\,0}^r}\limits_{\mbox{\tiny{$m+r-2n\geqslant\varepsilon k$}}}C^{n-i}_{m-i}C^{r-n}_{k-m+i}}
%        {\sum_{r=\,0}^{r_0}\sum_{n=\,0}^{r}C^{n-i}_{m-i}C^{r-n}_{k-m+i}}.
%    \end{multline}
%
%    Подстановка (\ref{BallTemp3}) и~(\ref{BallTemp4}) в~(\ref{BallTemp2}) завершает доказательство.
%\end{vkProof}

\subsection{Слой интервала булева куба}

Центральный слой хэммингова шара $B_r(a_0)$
можно использовать в~качестве объемлющего множества
для вычисления вероятности переобучения каждого элемента разложения
$A = A_1 \sqcup \dots \sqcup A_t$ оценки~\ref{eq:pzm-cluster}.
Отметим, что оценка $\Q(A_i) \leq \Q(B_r(a_0))$ является сильно завышенной.
Это связано с тем, что аппроксимация множества $A_i$
с помощью центрального слоя хэммингова шара не позволяет учесть объекты выборки,
лежащие глубоко внутри своего класса и одинаково классифицируемые всеми алгоритмами кластера $A_i$.
Следующее модельное множество исправляет этот недостаток.
%Во-вторых, при оценке не~учитывается мощность кластера $A_i$, которая на~реальных данных оказывается много меньше мощности множества $B_r^m$.

\begin{Def}
\label{def:interval-layer}
Пусть все объекты генеральной выборки $\XX$ разделены на~три непересекающихся множества:
надежно классифицируемые объекты $X_0$,
ошибочно классифицируемые объекты $X_1$
и~пограничные объекты $X_r$.
Пусть $|X_r| = r$ и~$|X_1| = m$, $\rho \in \NN$ "--- параметр, $\rho \leq r$.
Рассмотрим алгоритм $a_0$, допускающий~$m$ ошибок на~$X_1$ и~$\rho$ ошибок на~$X_r$.
\emph{Слоем интервала булева куба} будем называть множество алгоритмов $\hat B_{r,\rho}^{m} \subset \AA$, удовлетворяющее следующим условиям:
\begin{itemize}
    \item $\hat B_{r,\rho}^{m}$ содержит все алгоритмы, допускающие ровно $\rho$ ошибок на~объектах из~$X_r$;
    \item ни один алгоритм из~$\hat B_{r,\rho}^{m}$ не~ошибается на~объектах из~$X_0$;
    \item все алгоритмы из~$\hat B_{r,\rho}^{m}$ ошибаются на~всех объектах из~$X_1$.
\end{itemize}
\end{Def}

\begin{figure}[t]
    \centering
    \includegraphics[width=160mm,height=48mm]{Pictures/ballslice_vs_localvicinity_combined.eps}
    \caption{\small Переобучение центрального слоя шара $B_{2 \rho}^{m}$ (сплошная кривая) и
        слоя интервала $\hat B_{2\rho, \rho}^{m - \rho}$ (пунктирная кривая) при $L=200$, $\ell = 100$, $m=50$.
        Рисунок слева отображает среднее уклонение частот ошибок на~обучении и~контроле в~зависимости от~параметра $\rho$. Рисунок справа отображает зависимость средней вероятности переобучения $\bar Q_\eps(B, d)$ от~параметра $d$ при $\rho = 5$, $\eps = 0.1$.}
    \label{figBallSliceVsLocalVicinity}
\end{figure}


На рис.~\ref{figBallSliceVsLocalVicinity} слева сравниваются вероятности переобучения центрального слоя хэммингова шара и~слоя интервала булева куба.
Видно, что слой интервала дает меньшую оценку вероятности переобучения.
Следовательно, аппроксимация кластера $A_i$ с~помощью слоя интервала булева куба дает более точную оценку вероятности переобучения.


\begin{Theorem}
\label{th:QEps-interval-layer}
Вероятность переобучения РМЭР для $\hat B_{r,\rho}^{m}$ дается следующей формулой:
\begin{equation}
\label{eq:QEps-interval-layer}
    Q_{\eps}(\hat B_{r,\rho}^{m}) =
        \frac 1{\CLl}
            \sum_{i = 0}^{\min(m, \ell)} \sum_{j = 0}^{\min(r, \ell - i)}
                C_m^i C_r^j C_{L - m - r}^{\ell - i - j} \Big[\frac{m + \rho - t(i,j)}{k} - \frac{t(i,j)}{\ell} \geq \eps\Big],
\end{equation}
где $t(i,j) = i + \max(0, \rho - r - j)$.
\end{Theorem}

\begin{vkProof}
Рассмотрим три симметрические группы перестановок $S_m$, $S_r$  и~$S_{L - m - r}$, действующие на~множествах $X_1$, $X_r$ и~$X_0$, соответственно.
Группой симметрий множества алгоритмов $\hat B_{r,\rho}^{m}$ является декартово произведение $S_m \times S_r \times S_{L - m - r}$.
Орбиты действия $\Sym(\hat B_{r,\rho}^{m})$ на~$\XXell$ индексируются двумя параметрами, $i = |X \cap X_1|$ и~$j = |X \cap X_r|$, где~$X$ "--- обучающая выборка.
Мощность орбиты $\tau_{i, j}$ дается, соответственно, выражением $|\tau_{i, j}| = C_m^i C_r^j C_{L - m - r}^{\ell - i - j}$.

Поскольку все алгоритмы множества $B_{r,\rho}^{m}$ допускают равное число ошибок,
то~для каждой выборки $X \in \XXell$ все алгоритмы из~$A(X)$ имеют одинаковое число ошибок и~на обучении, и~на контроле.
Легко подсчитать, что для произвольной выборки $X \in \tau_{i, j}$ и~$a \in A(X)$
выполнено $n(a, X) = i + \max(0, \rho - r - j)$ и~$n(a, \X) = m + \rho - n(a, X)$.
Для доказательства теоремы осталось подставить эти значения и~мощность орбиты $|\tau_{i, j}|$ в~теорему~\ref{th:QXorbit}
о~разложение вероятности переобучения по~орбитам действия группы симметрии на~разбиениях выборки.
\end{vkProof}

\section{Основные выводы}

В данной главе получены точные оценки вероятности переобучения РМЭР для девяти модельных семейств алгоритмов.
При выводе оценок используется разработанный выше математический инструментарий:
разложение вероятности переобучения по~орбитам действия группы симметрий
на~множестве алгоритмов или на~множестве разбиений выборки,
а~также теорема о~порождающих и~запрещающих множествах для РМЭР.
