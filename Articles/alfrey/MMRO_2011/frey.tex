\documentclass[twoside]{article}
\usepackage{mmro15}
%\NOREVIEWERNOTES

\newcommand{\XX}{\mathbb{X}}
\newcommand{\Xl}{X}
\newcommand{\Xk}{\bar X}
\newcommand{\X}{\bar X}
\newcommand{\XXell}{[\XX]^\ell}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\Argmin}{\mathop{\rm Argmin}\limits}
\newcommand{\Argmax}{\mathop{\rm Argmax}\limits}
\newcommand{\Sym}{\mathop{\rm Sym}\limits}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\renewcommand{\epsilon}{\varepsilon}\newcommand{\eps}{\varepsilon}
\newcommand{\hypergeom}[5]{{#1}_{#2}^{#4,#3}\left(#5\right)}
\newcommand{\hyper}[4]{\hypergeom{h}{#1}{#2}{#3}{#4}}
\newcommand{\Hyper}[4]{\hypergeom{H}{#1}{#2}{#3}{#4}}
\newcommand{\HyperR}[4]{\hypergeom{\bar{H}}{#1}{#2}{#3}{#4}}
\newcommand{\Binom}[2]{C_{#1}^{#2}}
%\newcommand{\Binom}[2]{\binom{#1}{#2}}
\newcommand{\CLl}{\Binom{L}{\ell}}

\newcommand{\vkEndProof}{\hfill$\scriptstyle\blacksquare$\par\medskip}
\newenvironment{vkProof}[1][. ]%
    {\par\noindent{\bf Доказательство#1}}%
    {\vkEndProof}

\newcommand{\IncludeHalfPicture}[1]{\includegraphics[width=54mm,height=38mm]{#1}}

\def\brop#1{#1\discretionary{}{\hbox{$#1$}}{}}

\def\RR{\mathbb{R}}
\def\DD{\mathbb{D}}
\def\fF{\mathfrak{F}}
\def\fI{\mathfrak{I}}
\def\fM{\mathfrak{M}}

\begin{document}
\title
    [Метод порождающих и~запрещающих множеств]
    {Метод порождающих и~запрещающих множеств для~рандомизированного метода минимизации эмпирического~риска}
\author
    {Фрей~А.\,И.}
\thanks
    {Работа поддержана РФФИ (проект \No\,08-07-00422) и~программой ОМН~РАН
    <<Алгебраические и~комбинаторные методы математической кибернетики
    и~информационные системы нового поколения>>.}
\email
    {frey@forecsys.ru}
\organization
    {Москва, Московский Физико-Технический Институт (Государственный Университет)}
\abstract
    {В~комбинаторном подходе к~проблеме обобщающей способности развиваются методы,
    позволяющие упрощать вывод точных оценок вероятности переобучения.
    Для~детерминированного метода минимизации эмпирического риска основным инструментом вывода
    подобных оценок является метод порождающих и~запрещающих объектов.
    В~данной работе указанный метод обобщается на случай рандомизированных методов обучения.}
\maketitle

При решении задач машинного обучения требуется
из~заданного множества алгоритмов выбрать алгоритм,
который ошибался~бы как можно реже не~только на~объектах наблюдаемой обучающей выборки,
но~и~на~объектах скрытой контрольной выборки,
которая в~момент выбора алгоритма ещё неизвестна.
Если частота ошибок на~контрольной выборке
оказывается значительно выше, чем на~обучающей,
то~говорят, что произошло переобучения алгоритма "---
он~слишком хорошо описывает конкретные данные,
но~не~обладает способностью к~обобщению этих данных,
не~восстанавливает порождающую их~зависимость
и~не~пригоден для построения прогнозов.

%В~данной работе выборка данных будет предполагаться репрезентативной и~стационарной.
%Главной целью ставится исследование свойств метода обучения как такового.

На~практике склонность метода обучения к~переобучению оценивается с~помощью
процедуры скользящего контроля (кросс-валидации).
Фиксируется некоторое множество разбиений исходной выборки на~две подвыборки "---
обучающую и~контрольную. Для каждого разбиения выполняется настройка алгоритма по~обучающей подвыборке,
затем оценивается его средняя ошибка на~объектах контрольной подвыборки.
Оценкой скользящего контроля называется средняя по~всем разбиениям величина ошибки на~контрольных подвыборках.

Нашей целью будет получение оценок функционала
скользящего контроля без применения процедуры кросс-валидации.

\section{Основные обозначения}

Пусть задана генеральная выборка $\XX=\bigl( x_1, \ldots, x_L)$, состоящая из~$L$ объектов.
Произвольный алгоритм классификации, примененный к~данной выборке, порождает бинарный вектор
ошибок $a \equiv \bigl( I(a, x_i) \bigr){}_{i=1}^L$,
где $I(a, x_i) \in \{0, 1\}$ "--- индикатор ошибки алгоритма $a$ на~объекте $x_i$.
В~дальнейшем алгоритмы будут отождествляться с~векторами их~ошибок на~выборке $\XX$.

Обозначим
через $\AA = \{0, 1\}^L$ множество всех возможных векторов ошибок длины~$L$.
Через~$\XXell$ обозначим множество всех разбиений генеральной выборки~$\XX$
на обучающую выборку~$\Xl$ длины~$\ell$ и~контрольную выборку~$\Xk$ длины $k=L-\ell$.
\emph{Число ошибок} алгоритма~$a$ на~выборке $U \subseteq \XX$
обозначим через $n(a, U) = \sum \limits_{x \in U}I(a, x)$.
Величину $\nu(a, U) = n(a, U) / |U|$ будем называть \emph{частотой ошибок}
алгоритма~$a$~на~выборке $U$.
\emph{Уклонение частот} на~разбиении $\XX = \Xl \sqcup \Xk$ определим как разность
частот ошибок на~контроле и~на обучении:
$\delta(a, \Xl) = \nu(a, \Xk) - \nu(a, \Xl)$.

Пусть~$A \subset \AA$ "--- множество алгоритмов с~попарно различными векторами ошибок.
Обозначим через $A(\Xl)$ множество алгоритмов с~минимальным числом ошибок на~обучающей выборке~$\Xl$:
\begin{equation}
\label{eqERM-A(X).sym}
    A(X) = \Argmin_{a\in A} n(a,\Xl).
\end{equation}

Частоту ошибок на~обучающей выборке называют \emph{эмпирическим риском}.
\emph{Минимизация эмпирического риска $\mu$} "--- это метод обучения,
который из~заданного множества $A \subset \AA$ выбирает алгоритм $a \in A$,
допускающий наименьшее число ошибок на~обучающей выборке $\Xl$.
Таким образом, для всех $\Xl \in \XXell$ выполнено $\mu \Xl \in A(\Xl)$.

Говорят, что метод $\mu$ переобучен на~разбиении $\Xl \sqcup \Xk$, если
уклонение частот $\delta(a, \Xl)$ превышает фиксированный порог $\epsilon$.
Переобучение может быть следствием <<неудачного>> разбиения
генеральной выборки на~обучение и~контроль.
Поэтому вводится функционал \emph{вероятности переобучения},
равный доле разбиений выборки, при которых возникает переобучение~\cite{voron09dan, voron09mmro}:
\[
    Q_\eps(A) = \Expect [\delta(\mu \Xl, \Xl) \geq \epsilon],
    \text{ где } \Expect = \frac 1{\CLl} \sum\limits_{\Xl \in \XXell}.
\]
Тут и~далее квадратные скобки "--- нотация Айверсона,
переводящая логическое выражение в~число $0$ или $1$ по~правилам
$[\text{истина}] = 1$, $[\text{ложь}] = 1$.

Функционал $Q_\eps(A)$ уже не~зависит от~выбора разбиения
и~характеризует качество данного метода обучения на~данной генеральной выборке.

\section{Теорема о порождающих и запрещающих объектах}

Первый подход, позволивший получать точные оценки вероятности переобучения
в~рамках слабой вероятностной аксиоматики, основан на выделении порождающих
и~запрещающих объектов \cite{voron10pria}.

\begin{Hypothesis}
\label{hyp1}
    Пусть множество~$A$, выборка~$\XX$ и~детерминированный метод обучения~$\mu$ таковы, что
    для каждого алгоритма $a\in A$
    можно указать пару непересекающихся подмножеств
    ${X_a \subset \XX}$ и~${X'_a\subset \XX}$,
    %хотя~бы одно из которых не~пусто,
    удовлетворяющую условию
    \begin{equation}
    \label{eq1muX}
        \bigl[ \mu X{=}a \bigr]
        =
        \bigl[  X_a\subseteq  X \bigr]
        \bigl[ X'_a\subseteq \X \bigr],
        \quad
        \forall X\in \XXell.
    \end{equation}
\end{Hypothesis}

Множество~$X_a$ будем называть \emph{порождающим},
множество~$X'_a$ "--- \emph{запрещающим} для алгоритма~$a$.
Гипотеза~\ref{hyp1} означает, что
метод~$\mu$ выбирает алгоритм~$a$
тогда и~только тогда, когда в~обучающей выборке~$X$
находятся все порождающие объекты и~ни одного запрещающего.
Все остальные объекты $\XX {\setminus} X_a {\setminus} X'_a$ будем называть
\emph{нейтральными} для алгоритма~$a$.

Для произвольного $a\in A$ обозначим
через~$L_a$ число нейтральных объектов,
через~$\ell_a$ "--- число нейтральных объектов, попадающих в~обучающую выборку:
\begin{align*}
    L_a &= L - |X_a| - |X'_a|;
\\
    \ell_a &= \ell - |X_a|.
\end{align*}

\begin{Theorem}
\label{th1}
    Если гипотеза~\ref{hyp1} справедлива,
    то~вероятность получить в~результате обучения алгоритм~$a$ равна
    \[
        P_a(A) = \P[ \mu X{=}a ] = \frac{C_{L_a}^{\ell_a}}{C_{L}^{\ell}},
    \]
    а вероятность переобучения~$Q_\eps(A)$ выражается по~формуле полной вероятности:
    \begin{align*}
    Q_\eps(A)
    &=
    \sum_{a\in A}
        P_a
        \Prob\bigl( \delta(a,X) \geq \eps \mid a\bigr)  \\
    &=
    \sum_{a\in A}
        P_a
        \Hyper{L_a}{m_a}{\ell_a}{s_a(\eps)}.
    \end{align*}
\end{Theorem}

Данный результат позволил получить формулы вероятности переобучения
для широкого класса модельных семейств алгоритмов, в~частности
для монотонных и~унимодальных сеток. В~следующем параграфе аналогичная
теорема будет получена для~рандомизированных методов обучения.

\section{Рандомизированный метод минимизации эмпирического риска}

\emph{Рандомизированный метод минимизации эмпирического риска}
выбирает произвольный алгоритм из~множества~$A(X)$ случайно и~равновероятно
\cite{frey09mmro, frey10pria}.

Поскольку в~задаче статистического обучения появляется второй независимый источник случайности,
определение вероятности переобучения~$Q_\eps(A)$ приходится модифицировать.
Наиболее естественный вариант модификации "--- усреднение по~множеству $A(X)$:
\begin{equation}
\label{QepsRERM.sym}
    Q_\eps(A)
    =
    \Expect
    \frac1{|A(X)|} \sum_{a\in A(X)}
    \bigl[
        \delta(a,X) \geq \eps
    \bigr].
\end{equation}

Для детерминированного метода обучения результатом обучения являлся алгоритм $a~\in~A$.
В~случае рандомизированного метода обучения результатом обучения является подмножество $A(X) \subset A$.
Данное обстоятельство позволяет сформулировать следующую гипотезу.

\begin{Hypothesis}
\label{hyp2}
    Пусть $\aleph = \{ A(X) \; \colon \; X \in \XXell \}$ "--- множество всех $A(X)$,
    получающихся в~результате обучения.
    Пусть множество~$A$ и~выборка~$\XX$ таковы, что
    для каждого алгоритма $\alpha \in \aleph$
    можно указать пару непересекающихся подмножеств
    ${X_\alpha \subset \XX}$ и~${X'_\alpha\subset \XX}$,
    %хотя~бы одно из которых не~пусто,
    удовлетворяющую условию
    \begin{equation}
    \label{eq1muX}
        \bigl[ A(X){=} \alpha \bigr]
        =
        \bigl[  X_\alpha\subseteq  X \bigr]
        \bigl[ X'_\alpha\subseteq \X \bigr],
        \quad
        \forall X\in \XXell.
    \end{equation}
\end{Hypothesis}

\begin{Theorem}
\label{th2}
Если гипотеза~\ref{hyp2} справедлива,
то вероятность переобучения~$Q_\eps(A)$ выражается по~следующей формуле:
\[
    Q_\eps(A) = \sum_{a \in A} \sum_{\alpha \in \aleph} \frac {[a \in \alpha]}{|\alpha|}
        \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        H_{L_\alpha}^{\ell_\alpha, m^a_\alpha} (s^a_\alpha(\eps)),
\]
где
$L_\alpha = L - |\Xl_\alpha| - |\Xk_\alpha|$,
$\ell_\alpha = \ell - |\Xl_\alpha|$,
$m^a_\alpha = n(a, \XX \backslash \Xl_\alpha \backslash \Xl'_\alpha)$,
$s^a_{\alpha}(\eps) = \frac{\ell}{L}(n(a, \XX) - \eps k) - n(a, X_\alpha)$,
$\Hyper{L}{m}{\ell}{z} =
	\sum_{s=0}^{\lfloor z \rfloor}
	\frac{\Binom{m}{s} \Binom{L-m}{\ell-s}}{\CLl}$ "---
левый хвост гипергеометрического распределения.
\end{Theorem}

\begin{Lemma}
\label{lem1}
Пусть в~множестве $A$ есть алгоритм $a_0$, такой что для любого $a \in A$ вектор ошибок
алгоритма $a_0$ содержится в~векторе ошибок алгоритма $a$.
Обозначим множество объектов на~которых ошибается $a_0$ через $X_0$.
Пусть система порождающих и~запрещающих множеств такова, что для всех $\alpha$ выполнено
$X_0 \cap X_\alpha = \emptyset$ и~$X_0 \cap X'_\alpha = \emptyset$.
Тогда в~формуле порождающих и~запрещающих объектов можно следующим образом упростить обозначения:
$m^a_\alpha = n(a_0, \XX)$,
$s^a_{\alpha}(\eps) = \frac{\ell}{L}(n(a, \XX) - \eps k)$.
\end{Lemma}

Теорема о~порождающих и~запрещающих объектах легко объединяется
с~теоремой о~разбиении множества алгоритмов на~орбиты \cite{frey09mmro}.
\begin{Lemma}
Пусть $G \subset \Sym A$ "--- подгруппа группы симметрии множества алгоритмов $A$,
$\Omega(A)$ "--- множество всех орбит действия $G$ на~$A$,
$a_\omega$ "--- произвольный представитель орбиты~$\omega \in \Omega$,
а остальные обозначения "--- как в теореме \ref{th2}.
Тогда вероятность переобучения $Q_\epsilon(A)$ можно записать в виде:
\begin{align*}
\label{lem2}
    Q_\eps(A) & =     \sum_{\omega \in \Omega(A)}
                    %\sum_{\substack{
    %        \alpha \in \aleph \colon\\
    %        a_\omega \in \alpha
    %    }}
    \sum_{\alpha \in \aleph}
        [a_\omega \in \alpha]
        \frac {|\omega|}{|\alpha|}
          \frac{\Binom{L_\alpha}{\ell_\alpha}}{\CLl}
        H_{L_\alpha}^{\ell_\alpha, m^{a_\omega}_\alpha} (s^{a_\omega}_\alpha(\eps)).
\end{align*}
\end{Lemma}

Гипотеза~\ref{hyp2} накладывает слишком сильные ограничения
на~выборку~$\XX$ и семейство~$A$.
Рассмотрим естественное обобщение:
предположим, что для каждого алгоритма~$a$ существуют различные варианты
выделения порождающих и~запрещающих множеств.

\begin{Hypothesis}
\label{hyp3}
    Пусть множество~$A$ и выборка~$\XX$ таковы, что
    для каждого алгоритма ${\alpha\in \aleph}$ можно указать
    конечное множество индексов~$V_\alpha$,
    и~для каждого индекса ${v\in V_\alpha}$ можно указать
    \emph{порождающее} множество~${X_{\alpha v}\subset\XX}$,
    \emph{запрещающее} множество~${X'_{\alpha v}\subset\XX}$
    %хотя~бы одно из~которых не~пусто,
    и~коэффициент $c_{\alpha v}\in\RR$,
    для всех $X\in \XXell$ удовлетворяющие условиям
    \begin{equation}
    \label{eq:hyp3}
        \bigl[ A(X) {=}\alpha \bigr]
        =
        \sum_{v\in V_a}
            c_{\alpha v}
            \bigl[  X_{\alpha v}\subseteq  X \bigr]
            \bigl[ X'_{\alpha v}\subseteq \X \bigr].
    \end{equation}
\end{Hypothesis}
При условиях данной гипотезы теорема \ref{th2} о~порождающих и~запрещающих множествах
обобщается полностью аналогично случаю детерминированного метода обучения.

Важно отметить, что гипотеза \ref{hyp3} выполнена для произвольной выборки~$\XX$
и~множества алгоритмов~$A$.
\begin{Theorem}
\label{th3}
    Для любых~$\XX$ и~$A$
    существуют множества $V_\alpha$, $X_{\alpha v}$, $X'_{\alpha v}$,
    при которых справедливо представление~\eqref{eq:hyp3},
    причём $c_{\alpha v}=1$ для всех $\alpha \in A$,\, $v\in V_\alpha$.
\end{Theorem}

\section{Монотонные и унимодальные сети алгоритмов}

Монотонная сетка алгоритмов \cite{botov09mmro} "--- это модель параметрического \emph{связного семейства алгоритмов},
предполагающая, что при непрерывном удалении каждой компоненты вектора параметров
от~оптимального значения число ошибок на~полной выборке только увеличивается.

\begin{Example}
    Монотонная двумерная сетка при $m = 0$ и~$L = 4$:
    \vskip-6ex
     \[
        \bordermatrix{
             % & a_{0,0} & a_{1,0} & a_{2,0} & a_{0,1} & a_{1,1} & a_{2,1} & a_{0,2} & a_{1,2} & a_{2,2} \cr
             & & & & & & & & & \cr
             x_1 & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} & 0 & \textbf{1} & \textbf{1} \cr
             x_2 & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} & 0 & 0 & \textbf{1} \vspace{-2ex}\cr\cline{2-10}
             x_3 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \cr
             x_4 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{1} & \textbf{1} & \textbf{1} \cr
         }
     \]
\end{Example}

Унимодальная сетка алгоритмов \cite{botov09mmro} является более реалистичной моделью связного параметрического семейства,
по~сравнению с~монотонной сеткой. Если мы имеем лучший алгоритм~$a_0$ c~оптимальным значением
вектора вещественных параметров, то~отклонение значений компонент этого вектора
как в~б\'ольшую, так~и~в~меньшую,
сторону приводит к~увеличению числа ошибок.

Формулы вероятности переобучения рандомизированного метода минимизации эмпирического риска
для многомерной монотонной и унимодальной сеток уже были получены ранее \cite{frey10ioi}.
Однако ранее не отмечалось, что данные множества удовлетворяют 
условиям гипотезы \ref{hyp2} о порождающих и запрещающих объектах для рандомизированного
метода минимизации эмпирического риска. Данное обстоятельство позволяет
упростить вывод точных формул вероятности переобучения.

Введём целочисленный вектор индексов $\vec d = (d_1,\ldots,d_h) \in \ZZ^h$.
Обозначим $\|\vec d\| = \max \limits_{j=1, \ldots, h} |d_j|$,
$|\vec d| = |d_1| + \ldots + |d_h|$.
На~множестве векторов индексов введём покомпонентное отношение сравнения:
$\vec d < \vec d'$,
если
$d_j \leq d'_j$,\; $j=1,\ldots, h$, и~хотя~бы одно из~неравенств строгое.

\begin{Definition}
    Множество алгоритмов
    $A = \bigl\{a_{\vec d}\bigr\}$, где~$\vec d \geq 0$ и~$\|\vec d\|\leq D$
    называется \emph{монотонной $h$"~мерной сеткой алгоритмов длины $D$},
    если существует $h \in \NN$ и~упорядоченные наборы объектов
    $X_j = \{x_j^1, \ldots, x_j^D\} \subset \XX$, для всех $j = 1, \ldots, h$,
    а~так~же множества $U_1 \subset \XX$ и~$U_0 \subset \XX$,
    такие что:
    \begin{enumerate}
        \item набор $\Big\{ U_0, U_1,\{X_j\}_{j=1}^h \Big\}$ является разбиением множества $\XX$
              на~непересекающиеся подмножества;
        \item $a_d(x_j^i) = \left[ i \leq d_j \right]$, где $x_j^i \in  X_j$;
        \item $a_d(x_0) = 0$ при всех $x_0 \in U_0$;
        \item $a_d(x_1) = 1$ при всех $x_1 \in U_1$.
    \end{enumerate}
\end{Definition}

Следующая лемма утверждает, что множество алгоритмов $A$ удовлетворяет условиям гипотезы \ref{hyp2}.

\begin{figure}[t]
    \centering
    \includegraphics[height=72mm]{frey_fig1.eps}
    \caption{Строение множества $A(X)$ для двумерной монотонной сетки; $h=2$, $D=8$.}
\end{figure}

\begin{Lemma}
Рассмотрим произвольное $\alpha \in \aleph$.
Согласно определению $\aleph$ это значит, что
найдется такое разбиение $\Xl \in \XX$, для которого $\alpha = A(\Xl)$.

Обозначим $i'(j)$ наименьший номер $i \in 1, \ldots, D$, такой что объект $x^i_j$ попадает в обучающую выборку $\Xl$.
Возможно, что для некоторых $j = 1, \ldots, h$ выражение $i'(j)$ не определено, поскольку все объекты $x^i_j$ оказались в контроле $\Xk$.
Пусть $J \subset 1, \dots, h$ "--- множество индексов $j$, для которых $i'(j)$ определено.

Тогда
$\Xl_\alpha = \bigcup\limits_{j \in J}x^{i'(j)}_j$,
$\Xl'_\alpha  = \bigcup\limits_{j=1}^h \bigcup\limits_{i=1}^{i'_j {-} 1} x^i_j$,
причем построенные таким образом множества $\Xl_\alpha$ и $\Xl'_\alpha$
зависят только от~исходного множества $\alpha$, но не от
выбора представителя $\Xl \in \XX$, такого что $\alpha = A(\Xl)$.
\end{Lemma}

Данная лемма в сочетании с теоремой \ref{th2} позволяет
выписать формулу вероятности переобучения
рандомизированного метода обучения для многомерной монотонной сетки алгоритмов.

\begin{Theorem}
Вероятность переобучения рандомизированного метода минимизации эмпирического риска,
примененного к~монотонной сетке $A = \{ a_{\vec d} \}$ размерности $h$, $\|\vec d\| \leq D$,
дается выражением:
\[
    Q_\epsilon(A) = \sum_{\substack{\vec d \geq \vec 0, \\\|\vec d\| \leq \vec D}}
                         \sum_{\substack{\vec t \geq \vec 0, \\\|\vec t\| \leq \vec D}}
                         \frac {[\vec t \geq \vec d]} {V( \vec t )}
                         \frac{\Binom{L'}{\ell'}}{\CLl}
                         H_{L'}^{\ell', m}(s(\epsilon)),
\]
$V(\vec {t}) = \prod_j (t_j + 1)$,
$\ell' = \ell - \sum_{j = 1}^h [t_j \neq D]$,
$k' = k - |\vec t|$, $L' = \ell' + k'$,
$s(\epsilon) = \frac \ell L [m + |\vec d| - \epsilon k]$.
%$H_{L'}^{\ell', m}(s(\epsilon))$ "--- функция гипергеометрического распределения.
\end{Theorem}

\section{Выводы}

В~данной работе удалось обобщить метод порождающих и~запрещающих объектов
на~случай рандомизированных методов обучения.
Полученный результат по~прежнему позволяет учитывать структуру симметрии
множества алгоритмов (разбиение множества алгоритмов на орбиты действия
группы симметрии).

Как и для случая детерминированных методов обучения, доказано что для любого множества
алгоритмов можно указать систему порождающих и~запрещающих множеств.
К~сожалению, данный результат вновь является типичной теоремой существования:
использованный при её доказательстве способ построения множества порождающих и~запрещающих множеств
требует явного перебора всех разбиений выборки,
что приводит к~вычислительно неэффективным оценкам вероятности переобучения.

Тем не менее, с~помощью предложенного подхода получены эффективные оценки вероятности переобучения
для модельных семейств алгоритмов: монотонных и унимодальных многомерных сеток.

\begin{thebibliography}{1}
\bibitem{voron09dan}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Точные оценки вероятности переобучения}~//
    Доклады РАН, 2009. "--- Т.\,429, \No\,1.  "--- С.\,15--18.
\bibitem{voron09mmro}
    \BibAuthor{Воронцов\;К.\,В.}
    \BibTitle{Комбинаторный подход к~проблеме переобучения}~//
    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009. "---  \mbox{С.\,18--21}.
\bibitem{voron10pria}
    \BibAuthor{Vorontsov\;K.\,V.}
    \BibTitle{Exact combinatorial bounds on the probability of overfitting for empirical risk minimization}~//
    Pattern Recognition and Image Analysis. "--- 2010. "--- Vol. 20, no. 3. "--- \mbox{Pp.\,269--285}.
\bibitem{frey09mmro}
    \BibAuthor{Фрей\;А.\,И.}
    \BibTitle{Точные оценки вероятности переобучения для симметричных семейств алгоритмов}~//
    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,66--69}.
\bibitem{frey10pria}
    \BibAuthor{Frei\;A.\,I.}
    \BibTitle{Accurate estimates of the generalization ability for symmetric set of predictors and randomized learning algorithms}~//
    Pattern Recognition and Image Analysis. "--- 2010. "--- Vol. 20, no. 3. "--- \mbox{Pp.\,241--250}.
\bibitem{botov09mmro}
    \BibAuthor{Ботов\;П.\,В.}
    \BibTitle{Точные оценки вероятности переобучения для монотонных и~унимодальных семейств алгоритмов}~//
    Всеросс. конф. ММРО-14 "--- М.:~МАКС Пресс, 2009.  "---  \mbox{С.\,7--10}.
\bibitem{frey10ioi}
    \BibAuthor{Фрей\;А.\,И.}
    \BibTitle{Вероятность переобучения плотных и~разреженных многомерных~сеток алгоритмов}~//
    Интеллектуализация обработки информации "--- М.:~МАКС Пресс, 2010.  "---  \mbox{С.\,??--??}.
    
\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
