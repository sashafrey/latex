\documentclass[12pt,fleqn]{article}

\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage{color}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
\usepackage[ruled,section]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{multicol}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом

% русские ключевые слова в алгоритмах
\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}
\floatname{algorithm}{Алгоритм}

% нумерация
\numberwithin{equation}{section}

% новые операторы
\newtheorem{Def}{Определение}[section]
\newtheorem{Lem}{Лемма}
\newtheorem{Th}{Теорема}
\def\argmin{\mathop{\rm arg min}\limits}
\def\sign{\mathop{\rm sign}\limits}
\def\deg{\mathop{\rm deg}\nolimits}
\newcommand{\cond}{\mspace{3mu}{|}\mspace{3mu}}

\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 417 ГРУППЫ\\[10mm]
        <<Комбинаторные оценки обобщающей способности \\ и методы их вычисления>>
    }\\[10mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 4 курса 417 группы\\
            \emph{Соколов Евгений Андреевич}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., доцент\\
            \emph{Воронцов Константин Вячеславович}
        }
    \end{flushright}

    \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
        Заведующий кафедрой\newline
        Математических Методов\newline
        Прогнозирования, академик РАН
        &
        ~\newline~\newline
        \hfill\hbox to 0.45\textwidth{\hrulefill~Ю. И. Журавлёв}
    \\[20mm]
        К защите допускаю\newline
        \hbox to 0.4\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2012 г.}
        &
        К защите рекомендую\newline
        \hbox to 0.45\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2012 г.}
    \end{tabular}

    \vspace{\fill}
    Москва, 2012
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage
\section{Введение}

\subsection{Основные определения}

Пусть задано конечное множество $\mathbb{X} = \{ x_1, \dots, x_L \}$, называемое {\it генеральной выборкой},
и целевая функция $y : \mathbb{X} \rightarrow \mathbb{Y}$.
Пусть также задано множество $\mathcal{A}$, элементы которого называют {\it алгоритмами}.
Алгоритм $ a \in \mathcal{A} $ сопоставляет объекту $x \in \mathbb{X}$ некоторое значение из множества $\mathbb{Y}$,
$a : \mathbb{X} \rightarrow \mathbb{Y}$.
Также предполагается, что задана функция $I : \mathcal{A} \times \mathbb{X} \rightarrow \{ 0, 1 \}$, называемая
{\it индикатором ошибки}.
$I(a, x)$ принимает значение $1$, если алгоритм $a$ допускает ошибку на объекте $x$, и значение $0$ в противном случае.

{\it Вектором ошибок } алгоритма $a$ называется бинарный вектор $\vec a = (I(a, x_i))_{i = 1}^{L}$.
Везде далее под <<алгоритмом>> будем понимать не само отображение, а его вектор ошибок.
Также будем считать, что $\mathcal{A}$ --- это множество бинарных векторов.

{\it Метод обучения} --- это функция, строящая по подмножеству полной выборки алгоритм из заданного семейства:
$\mu : 2^{\mathbb{X}} \rightarrow \mathcal{A}$.

Будем рассматривать задачу обучения по прецедентам в следующей постановке.
Пусть на этапе обучения известна {\it обучающая выборка} $ X \subset \mathbb{X} $ длины $\ell$.
По обучающей выборке с помощью заданного метода обучения $\mu$ выбирается алгоритм $\mu X$ из семейства $\mathbb{A}$.
После того, как обучение закончено, становится известной {\it скрытая выборка}
$\bar X = \mathbb{X} \setminus X$, и к ней применяется выбранный алгоритм $\mu X$.
Длину контрольной выборки будем обозначать через $k = L - \ell$.

{\it Числом ошибок} алгоритма $a$ на выборке $X \subset \mathbb{X}$ называют величину
\[
    n(a, X) = \sum_{x \in X} I(a, x)
\]

{\it Долей ошибок} алгоритма $a$ на выборке $X \subset \mathbb{X}$ называется величина
\[
    \nu (a, X) = \frac{n(a, X)}{|X|}
\]

{\it Уклонением частот ошибок} алгоритма $a$ на двух выборках $X$ и $\bar X = \mathbb{X} \setminus X$ называется величина
\[
    \delta (a, X) = \nu (a, \bar X) - \nu (a, X)
\]

Пусть задан некоторый вещественный параметр $\varepsilon \in [0, 1)$, называемый {\it порогом переобучения}.
Говорят, что алгоритм $a$ переобучается на разбиении $(X, \bar X)$, если
\[
    \delta (a, X) \geq \varepsilon
\]

Аналогично, метод $\mu$ переобучается на разбиении $(X, \bar X)$, если
\[
    \delta (\mu X, X) \geq \varepsilon
\]

\begin{Def}
    Вероятностью переобучения метода $\mu$ называется величина
    \[
        Q_{\varepsilon}(\mu, \mathbb{X}) \equiv \Prob [ \delta (\mu X, X) \geq \varepsilon ] =
            \frac{1}{C_L^\ell} \sum_{(X, \bar X)} [ \delta (\mu X, X) \geq \varepsilon ]
    \]
\end{Def}

Определим некоторые комбинаторные величины, которые понадобятся нам в дальнейшем.

{\it Гипергеометрическая функция вероятности}:
\[
    h_L^{\ell, m}(s) = \frac{C_m^s C_{L - m}^{\ell - s}}{C_L^\ell}
\]

{\it Гипергеометрическая функция распределения}:
\[
    H_L^{\ell, m}(s) = \sum_{i = 0}^{\min(s, \ell, m)} h_L^{\ell, m}(i)
\]

Подробное описание этих величин можно найти в (\cite{voron10doct}).

В данной работе большое внимание будет уделено семейству {\it линейных классификаторов}.
Пусть объекты выборки представляют собой точки в некотором евклидовом пространстве:
$\mathbb{X} \subset \mathbb{R}^d$.
Тогда семейство линейных классификаторов $\mathcal{A}_h$~--- это множество всех
гиперплоскостей, разделяющих данную выборку
\[
    \mathcal{A}_h = \{ a_w(x) = \sign (\langle w, x \rangle + w_0) \ | \ w \in \mathbb{R}^d, w_0 \in \mathbb{R} \}
\]
Как было сказано выше, множество алгоритмов мы будем отождествлять с множеством векторов ошибок этих алгоритмов.
Это означает, что множество $\mathcal{A}_h$ мы будем отождествлять с множеством всех бинарных векторов,
соответствующих разделению выборки на две части гиперплоскостью.

\subsection{Представление семейства алгоритмов графом}

Введем на множестве алгоритмов отношение частичного порядка $\prec$:
\[
    a \leq b \ \Leftrightarrow \ (\forall x \in \mathbb{X} \ I(a, x) \leq I(b, x))
\]
Если $a \leq b$ и при этом $\rho(a, b) = 1$ (здесь $\rho$~--- это хэммингово расстояние),
то будем говорить, что $a$ {\it предшествует} $b$ и записывать $a \prec b$.

\begin{Def}
    Графом расслоения-связности семейства алгоритмов $\mathcal{A}$ называется ориентированный граф
    $G = (V, E)$ с множеством вершин $V = \mathcal{A}$ и множеством ребер
    $E = \{ (a, b) \ | \ a \prec b \}$.
\end{Def}

{\it Слоем} графа расслоения-связности называется множество алгоритмов,
допускающих одинаковое число ошибок: $A_m = \{ a \in \mathcal{A}\ |\ n(a, \mathbb{X}) = m \}$.
Граф расслоения-связности является многодольным,
доли соответствуют слоям $A_m$,
ребрами могут соединяться только соседние слои.
В частности, из многодольности графа следует его двудольность.

Если две вершины графа $a$ и $b$ соединены ребром (где $a \prec b$),
то векторы ошибок алгоритмов $a$ и $b$ отличаются
лишь в одном элементе.
Это позволяет поставить в соответствие каждому ребру $(a, b)$
объект $x_{ab} \in \mathbb{X}$ такой, что $I(a, x_{ab}) = 0$ и
$I(b, x_{ab}) = 1$.

{\it Верхней окрестностью} алгоритма $a$ называется множество
$C^+(a) = \{ b \in \mathbb{A} \ | \ (a, b) \in E \}$.
Аналогично, {\it нижней окрестностью} называется множество $C^-(a) = \{ b \in \mathbb{A} \ | \ (b, a) \in E \}$.
Элементы верхней и нижней окрестностей алгоритма $a$ будем называть верхними и нижними соседями соответственно.

Вершина графа расслоения-связности называется {\it истоком}, если у нее нет входящих ребер.

%{\it Нижним поддеревом} алгоритма $a$ будем называть множество $\{ a' \in \mathbb{A} \ | \ a' \prec a \}$.
%Иными словами, это множество, состоящее из всех вершин, лежащих на всех путях, идущих из $a$ по обратным ребрам графа расслоения-связности.

{\it Верхней связностью} $q(a)$ алгоритма $a$ называется число вершин в его верхней окрестности:
\[
    u(a) = |C_+(a)|
\]

{\it Неполноценностью (inferiority)} $r(a)$ алгоритма $a$ называется число объектов $x \in \mathbb{X}$,
на которых $a$ ошибается, при том, что существует алгоритм $b \prec a$, не ошибающийся на $x$:
\[
    q(a) = \# \{ x \in \mathbb{X} \ | \ I(a, x) = 1, \ \exists b \in \mathbb{A}: \ b \prec a, I(b, x) = 0 \}
\]

Введем также обозначение для числа ошибок алгоритма $a$:
\[
    m(a) = n(a, \mathbb{X})
\]

\subsection{Оценка расслоения-связности}
Пусть $\mathbb{X} = \{ x_1, \dots, x_L \}$ --- выборка, $\mathbb{A} = \{ a_1, \dots, a_D \}$ --- некоторое семейство алгоритмов.
Будем считать, что алгоритмы пронумерованы в порядке неубывания числа ошибок на генеральной выборке.
\begin{Def}
    Метод обучения $\mu$ называется пессимистичным методом минимизации эмпирического риска (ПМЭР), если он выбирает алгоритм, допускающий наименьшее число ошибок на обучающей выборке; если таких несколько~--- выбирает из них алгоритм с наибольшим числом ошибок на генеральной выборке; если и таких несколько, то он выбирает из них алгоритм с наибольшим номером.
\end{Def}
Отметим, что так как алгоритмы отсортированы по числу ошибок, то из всех алгоритмов, минимизирующих эмпирический риск, $\mu$ будет просто выбирать алгоритм с наибольшим номером.

С использованием введенных ранее характеристик алгоритмов, основанных на графе расслоения-связности,
была получена следующая оценка вероятности переобучения для ПМЭР:
\begin{Th}[Воронцов, Решетняк, Ивахненко, 2010]\label{sc_bound_classical}
    Для пессиместичного метода минимизации эмпирического риска~$\mu$ и любых~$\mathbb{X}$, $\mathcal{A}$ и $\varepsilon \in (0, 1)$
    \[
        Q_\varepsilon(\mu, \mathbb{X}) \leqslant \sum_{i = 1}^D
            \frac{C_{L - u - q}^{\ell - u}}{C_L^\ell}
            \mathcal{H}_{L - u - q}^{\ell - u,\ m - q}
            \left( \frac{\ell}{L} (m - \varepsilon k) \right)
    \]
\end{Th}
Отметим, что вклад алгоритма $a$ в данную оценку экспоненциально убывает с ростом
$u(a)$ и $q(a)$.

\section{Улучшенная оценка расслоения-связности}
Напомним некоторые определения и предположения, описанные ранее.
Пусть $\mathbb{X} = \{ x_1, \dots, x_L \}$ --- выборка, $\mathbb{A} = \{ a_1, \dots, a_D \}$ --- некоторое семейство алгоритмов.
Мы считаем, что алгоритмы пронумерованы в порядке неубывания числа ошибок на генеральной выборке.
\begin{Def}
    Метод обучения $\mu$ называется пессимистичным методом минимизации эмпирического риска (ПМЭР), если он выбирает алгоритм, допускающий наименьшее число ошибок на обучающей выборке; если таких несколько~--- выбирает из них алгоритм с наибольшим числом ошибок на генеральной выборке; если и таких несколько, то он выбирает из них алгоритм с наибольшим номером.
\end{Def}
Из всех алгоритмов, минимизирующих эмпирический риск, $\mu$ будет просто выбирать алгоритм с наибольшим номером.

Определим для произвольных двух алгоритмов $a_i$ и $a_j$ множества $A_{ij}$ и $B_{ij}$:
\begin{align*}
    A_{ij} = \{ x \in \mathbb{X} \ |\ I(a_i, x) = 0, I(a_j, x) = 1 \} \\
    B_{ij} = \{ x \in \mathbb{X} \ |\ I(a_i, x) = 1, I(a_j, x) = 0 \}
\end{align*}

Это можно изобразить следующим образом:
\begin{align*}
	a_i:& \  (0\dots0\ 0\dots0\ 1\dots1\ 1\dots1) \\
	a_j:& \  (0\dots0\ \underbrace{1\dots1}_{A_{ij}}\ \underbrace{0\dots0}_{B_{ij}}\ 1\dots1)
\end{align*}

\begin{Lem}\label{choise_th}
    Пусть $X$ - обучающая выборка, $\mu$ --- пессимистичный метод минимизации эмпирического риска.
    Тогда:
    \begin{equation}\label{choise_condition}
        \left[ \mu X = a_i \right] = \left( \prod_{j = 1}^{i - 1} \left[ |X \cap B_{ij}| \leq |X \cap A_{ij}| \right] \right) \left( \prod_{j = i + 1}^{D} \left[ |X \cap B_{ij}| < |X \cap A_{ij}| \right] \right)
    \end{equation}
\end{Lem}

\begin{proof}
Заметим, что если $|X \cap B_{ij}| > |X \cap A_{ij}|$, то $a_j$ допускает на обучающей выборке меньше ошибок, чем $a_i$. Значит, в этом случае $a_i$ не может быть выбран методом $\mu$.
Также, согласно определению метода $\mu$, при равном числе ошибок на выборке он выбирает алгоритм с наибольшим номером.

Значит, для того, чтобы алгоритм $a_i$ был выбран ПМЭР $\mu$ на обучающей выборке $X$, необходимо, чтобы для любого $j = 1, \dots, D$ было выполнено:
\begin{itemize}
    \item
        $|X \cap B_{ij}| \leq |X \cap A_{ij}|$, если $j < i$
    \item
        $|X \cap B_{ij}| < |X \cap A_{ij}|$, если $j > i$
\end{itemize}
Значит, верно следующее неравенство:
\[
    \left[ \mu X = a_i \right] \leq \left( \prod_{j = 1}^{i - 1} \left[ |X \cap B_{ij}| \leq |X \cap A_{ij}| \right] \right) \left( \prod_{j = i + 1}^{D} \left[ |X \cap B_{ij}| < |X \cap A_{ij}| \right] \right)
\]

Покажем, что данное неравенство выполнено и в другую сторону.
Действительно, если правая часть равенства (\ref{choise_condition}) равна единице, то алгоритм $a_i$ допускает не больше ошибок на $X$, чем какой-либо другой алгоритм.
Более того, не существует алгоритма с номером $j > i$, допускающего столько же ошибок, сколько и $a_i$.
Так как среди всех алгоритмов, минимизирующих эмпирический риск, метод обучения $\mu$ выбирает алгоритм с наибольшим номером, то выполнено
\[
    \left[ \mu X = a_i \right] \geq \left( \prod_{j = 1}^{i - 1} \left[ |X \cap B_{ij}| \leq |X \cap A_{ij}| \right] \right) \left( \prod_{j = i + 1}^{D} \left[ |X \cap B_{ij}| < |X \cap A_{ij}| \right] \right)
\]

\end{proof}

\begin{Lem}
    Пусть $\mu$ --- метод пессимистичной минимизации эмпирического риска, $a_i$ и $a_s$ --- два произвольных алгоритма из $\mathcal{A}$.
    Тогда:
    \[
        \Prob [\mu X = a_i] [\delta (a_i, X) \geq \varepsilon] \leq \sum_{t = 0}^{\min(|A_{is}|, |B_{is}|)} \frac{C_{|B_{is}|}^t C_{L - u - |B_{is}|}^{l - u - t}}{C_L^l} \mathcal{H}_{L - u - |B_{is}|}^{l - u - t,\ m - |B_{is}|} \left( \frac{l}{L} (m - \varepsilon k) - t \right)
    \]
\end{Lem}

\begin{proof}
    С помощью леммы 1 оценим величину $[\mu X = a_i]$ сверху, оставив только те множители, которые соответствуют $a_s$ и верхней полуокрестности:
    \begin{align*}
        [\mu X = a_i] \leq &\left( [s \leq i] \left[ |B_{is} \cap X| \leq |A_{is} \cap X| \right] + [s > i] \left[ |B_{is} \cap X| < |A_{is} \cap X| \right] \right) \times \\
                    & \times \prod_{j:\ a_j \in C^+(a_i)} \left[ |B_{ij} \cap X| < |A_{ij} \cap X| \right] \leq \\
                    \leq &\left[ |B_{is} \cap X| \leq |A_{is} \cap X| \right] \prod_{j:\ a_j \in C^+(a_i)} \left[ |B_{ij} \cap X| < |A_{ij} \cap X| \right] \leq \\
                    \leq &\left[ |B_{is} \cap X| \leq |A_{is}| \right] \prod_{j:\ a_j \in C^+(a_i)} \left[ |B_{ij} \cap X| < |A_{ij} \cap X| \right] = \\
                    = &\left[ |B_{is} \cap X| \leq |A_{is}| \right] \prod_{j:\ a_j \in C^+(a_i)} \left[ |A_{ij} \cap X| > 0 \right]
    \end{align*}
    Из последнего неравенства следует, что для того, чтобы был выбран алгоритм $a_i$, необходимо, чтобы в обучающую выборку попали все объекты из $\bigcup_{a_j \in C^+(a_i)} A_{ij}$, и чтобы из $B_{is}$ в $X$ попало не более $|A_{is}|$ объектов.
    Исходя из этих соображений, получаем следующую оценку:
    \begin{align*}
        \Prob [\mu X = a_i] [\delta (a_i, X) \geq \varepsilon] \leq \sum_{t = 0}^{\min(|A_{is}|, |B_{is}|)} \frac{C_{|B_{is}|}^t C_{L - u - |B_{is}|}^{l - u - t}}{C_L^l} \mathcal{H}_{L - u - |B_{is}|}^{l - u - t,\ m - |B_{is}|} \left( \frac{l}{L} (m - \varepsilon k) - t \right)
    \end{align*}
\end{proof}

\begin{Th}\label{sc_bound_improved}
    Пусть $\mu$ --- метод пессимистичной минимизации эмпирического риска, $S$ --- множество всех истоков графа расслоения-связности.
    Тогда верна следующая оценка вероятности переобучения:
    \begin{equation}\label{bound}
        Q_{\varepsilon}(\mu, \mathbb{X}) \leq \sum_{i = 1}^{D} \min_{s \in S} \left\{ \sum_{t = 0}^{\min(|A_{is}|, |B_{is}|)} \frac{C_{|B_{is}|}^t C_{L - u - |B_{is}|}^{l - u - t}}{C_L^l} \mathcal{H}_{L - u - |B_{is}|}^{l - u - t,\ m - |B_{is}|} \left( \frac{l}{L} (m - \varepsilon k) - t \right) \right\}
    \end{equation}
\end{Th}

\begin{proof}
    Распишем вероятность переобучения, используя формулу полной вероятности:
    \[
        Q_{\varepsilon}(\mu, \mathbb{X}) = \Prob [ \delta ( \mu X, X) \geq \varepsilon ] =
        \sum_{i = 1}^{D} \Prob [\mu X = a_i] [\delta (a_i, X) \geq \varepsilon]
    \]

    Для различных $a_s \in \mathcal{A}$ можно получить различные оценки для величины $\Prob [\mu X = a_i] [\delta (a_i, X) \geq \varepsilon]$, используя лемму 2.
    Мы вычислим такие оценки для всех $a_s$, являющихся истоками графа расслоения-связности, а затем выберем наименьшую из таких оценок:
    \begin{align*}
        Q_{\varepsilon}(\mu, \mathbb{X}) &= \sum_{i = 1}^{D} \Prob [\mu X = a_i] [\delta (a_i, X) \geq \varepsilon] \leq \\
        &\leq \sum_{i = 1}^{D} \min_{s \in S} \left\{ \sum_{t = 0}^{\min(|A_{is}|, |B_{is}|)} \frac{C_{|B_{is}|}^t C_{L - u - |B_{is}|}^{l - u - t}}{C_L^l} \mathcal{H}_{L - u - |B_{is}|}^{l - u - t,\ m - |B_{is}|} \left( \frac{l}{L} (m - \varepsilon k) - t \right) \right\}
    \end{align*}
\end{proof}

В разделе~\ref{section_comb_margins} будет произведено сравнение данной оценки с оценкой~(\ref{sc_bound_classical})
и будет показано, что новая оценка точнее на порядок или больше.

\subsection{Направления дальнейших исследований}
\begin{itemize}
    \item Обобщение оценки на настоящий граф Хассе;
    \item Получение оценок, учитывающих связь с двумя истоками или больше.
\end{itemize}

\section{Комбинаторные отступы и отбор объектов}\label{section_comb_margins}
Оценки~(\ref{sc_bound_classical}) и~(\ref{sc_bound_improved}) зависят
от всех алгоритмов семейства $\mathcal{A}$,
поэтому для вычисления этих оценок необходимо сначала найти $\mathcal{A}$.
Если выборка состоит из большого числа объектов,
то построение всего семейства алгоритмов может оказаться крайне трудоемкой задачей.
В то же время интерес представляют только алгоритмы из нижних слоев графа расслоения-связности,
так как лишь они делают существенный вклад в оценку вероятности переобучения.
В данном разделе мы покажем, что можно исключить из выборки некоторые объекты таким образом, что нижние слои графа не изменятся.

Ребро в SC-графе, идущее из вершины $a$ в вершину $a^\prime$, соответствует изменению классификации на одном объекте.
Если известно, что объект $x$ не соответствует ни одному ребру в нижних слоях графа, то его можно не рассматривать при обходе.
Выясним, что характеризует свойство объекта <<иметь ребра в нижних слоях>>.

\begin{Def}
    Комбинаторным отступом алгоритма $a_s$ на объекте $x_0$ называется следующая величина:
    \[
        d(a_s, x_0) = \min \{ d\ |\ \exists a_i :\ I(a_s, x_0) \neq I(a_i, x_0), |B_{is}| = d \}
    \]
\end{Def}

Пусть $S \subset \mathbb{A}$ --- множество всех истоков SC-графа семейства $\mathbb{A}$.
Определим отступ объекта $x$ как минимальный из отступов всех истоков на нем:
\[
    d(x) = \min_{a \in S} d(a, x)
\]

Будем говорить, что $x_0$ --- {\it порождающий} объект для алгоритма $a$, если существует такой алгоритм $a'$, что векторы ошибок $a$ и $a'$ отличаются только на $x_0$.
Очевидно, что объект $x_0$ является порождающим для $a$ тогда и только тогда, когда из $a$ выходит ребро, соответствующее объекту $x_0$.

Допустим, мы исключаем из рассмотрения объект $x_i$.
Это соответствует удалению из графа всех ребер, соответствующих $x_i$.
Пусть алгоритм $a$ порождается объектом $x_i$, тогда есть возможность, что после исключения $x_i$ этот алгоритм перестанет быть достижимым из истоков графа.
Оценим вклад $a$ в оценку (\ref{bound}).
Среди истоков графа обязательно найдется такой исток $a_s$, что $a_s \prec a$.
Это означает, что будет выполнено $|A_{is}| = 0$.
Если $I(a, x_i) \neq I(a_s, x_i)$, то $|B_{is}| \geq d(a_s, x_i) \geq d(x_i)$ в силу определения отступа.
Если же $I(a, x_i) = I(a_s, x_i)$, то, поскольку $x_i$ --- порождающий объект для $a$, найдется алгоритм $a_j$, классификация которого отличается от $a$ лишь на одном объекте, и для которого выполнено $|B_{js}| \geq d(x_i)$.
Так как векторы ошибок $a_i$ и $a_j$ отличаются лишь в одном элементе, то $|B_{is}| \geq |B_{js}| - 1 \geq d(x_i) - 1$.

Поскольку в (\ref{bound}) берется минимум по всем истокам, вклад $a$ в оценку не будет превосходить величины
\[
    \frac{C_{L - u - d(x_i) - 1}^{l - u}}{C_L^l} \mathcal{H}_{L - u - d(x_i) - 1}^{l - u,\ m - d(x_i) - 1} \left( \frac{l}{L} (m - \varepsilon k) \right)
\]
Эта величина быстро убывает при росте $d(x_i)$, поэтому, если $d(x_i)$ велико, то при исключении $x_i$ мы потеряем только алгоритмы, не вносящие большого вклада в оценку.

Итак, если известны отступы $d(x)$ всех объектов, то, исключив объекты с большими отступами, можно значительно упростить перебор.


\subsection{Вычисление отступов}

Точное вычисление отступов является задачей не менее трудной, чем полный обход SC-графа, поэтому предлагается вычислять их приближенно.

Изначально все отступы полагаются равными $n$.
Для каждого объекта $x \in \mathbb{X}$ определенное число раз случайным образом строится алгоритм, для которого $x$ является порождающим.
Пусть мы построили алгоритм $a$, тогда отступы всех объектов обновляются следующим образом:
\begin{equation}\label{marg}
    d(x_i) := \min \left( d(x_i), \min \left\{ |B_{is}| \ \big| \  a_s \in S, I(a, x_i) \neq I(a_s, x_i) \right\} \right)
\end{equation}

Опишем подробнее, как указанный метод будет работать для семейств линейных классификаторов.
Пусть мы зафиксировали объект $x$.
Для него определенное число раз повторяется следующая процедура: случайным образом выбираются $d - 1$ объектов $x_{i_1}, \dots, x_{i_{d - 1}}$, к ним добавляется $x$, и строится гиперплоскость, проходящая через эти $d$ объектов.
Эта гиперплоскость соответствует нескольким алгоритмам, так как для нее существует два способа выбрать ориентацию и $2^d$ способов выбрать классификацию $d$ порождающих объектов, причем для всех этих алгоритмов $x$ будет порождающим объектов.
Из этих алгоритмов случайным образом выбирается один (обозначим его $a$), и для всех объектов обновляются отступы по формуле (\ref{marg}).

\subsection{Эксперименты}
\subsubsection{Зависимость $t$ от зашумленности выборки}

На рис. \ref{margin_pic_first}-\ref{margin_pic_last} приведены графики зависимости оценки $Q_{\varepsilon}$ от $t$ при разных степенях зашумленности выборки, а также функции распределения отступов для этих выборок.
Зашумленность характеризуется величиной $s = \min_{a \in \mathcal{A}} m(a)$.
Все эксперименты проводились на выборках с $L = 200$, $\ell = 100$, $d = 2$, $\varepsilon = 0.1$, каждый класс генерировался из нормального распределения.
Пример выборки приведен на рис. 1.

Видно, что во всех случаях достаточно взять $t = s + 10$, чтобы либо получить хорошее приближение оценки, либо понять, что оценка слишком завышена и точное ее вычисление не имеет смысла.

Также на рис. 10 приведен аналогичный график для выборки с $L = 400$.

На рис.~\ref{margin_pic_comp} изображены оценки~(\ref{sc_bound_classical}) и~(\ref{sc_bound_improved}),
а также оценка полного скользящего контроля.
Видно, что улучшенная оценка значительно менее завышена, нежели классическая.

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/sample_err14.eps}
    \caption{Выборка с $s = 14$}
    \label{margin_pic_first}
    \hfill
    \includegraphics[width=80mm]{pics/margin/improved_bound.eps}
    \hfill
    \caption{Сравнение классической и улучшенной оценок расслоения-связности}
    \label{margin_pic_comp}
\end{multicols}
\end{figure}

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/separable.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 0$}
    \hfill
    \includegraphics[width=80mm]{pics/margin/separable_distr.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 0, CDF$}
\end{multicols}
\end{figure}

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep1.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 1$}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep1_distr.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 1, CDF$}
\end{multicols}
\end{figure}

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep8.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 8$}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep8_distr.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 8, CDF$}
\end{multicols}
\end{figure}

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep14.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 14$}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep14_distr.eps}
    \hfill
    \caption{$L = 200$, $d = 2$, $s = 14, CDF$}
\end{multicols}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{pics/margin/400_err6.eps}
    \caption{$L = 400$, $d = 2$, $s = 6$}
    \label{margin_pic_last}
\end{figure}

\subsubsection{Поведение оценки при увеличении размерности}
Были проведены эксперименты на выборках в трехмерном пространстве с $L = 200$, $\ell = 100$, $\varepsilon = 0.1$.
Результаты приведены на рис. \ref{margin_pic_first1}-\ref{margin_pic_last1}.

Вычисления стали занимать существенно больше времени, более того, оценки стали сильно завышенными даже при небольших уровнях зашумленности.

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/sep200_dim3.eps}
    \hfill
    \caption{$L = 200$, $d = 3$, $s = 0$}
    \label{margin_pic_first1}
    \hfill
    \includegraphics[width=80mm]{pics/margin/sep200_dim3_distr.eps}
    \hfill
    \caption{$L = 200$, $d = 3$, $s = 0$, CDF}
\end{multicols}
\end{figure}

\begin{figure}[h]
\begin{multicols}{2}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep200_dim3_err7.eps}
    \hfill
    \caption{$L = 200$, $d = 3$, $s = 7$}
    \hfill
    \includegraphics[width=80mm]{pics/margin/unsep200_dim3_err7_distr.eps}
    \hfill
    \caption{$L = 200$, $d = 3$, $s = 7$, CDF}
    \label{margin_pic_last1}
\end{multicols}
\end{figure}

\subsection{Направления дальнейших исследований}
\begin{itemize}
    \item Применить описанную технику для ускорения случайного блуждания по графу;
            провести эксперименты, показывающие, что удаление части объектов слабо влияет на результат.
\end{itemize}

\clearpage
\section{Приближенное вычисление оценки расслоения-связности}
Обозначим вклад алгоритма $a \in \mathcal{A}$ в оценку (\ref{bound}) через $b(a)$.

Пусть имеется набор алгоритмов $a_1, \dots, a_n$, выбранных равномерно из семейства $\mathcal{A}$.
Тогда можно получить оценку для $Q_\varepsilon$:
\[
    \hat Q_\varepsilon = \frac{1}{n} \sum_{i = 1}^{n} b(a_i)
\]

Как говорилось выше, оценка $Q_\varepsilon$ определяется в основном
вкладами алгоритмов из нижних слоев графа расслоения-связности.
Однако число алгоритмов в нижних слоях составляет лишь небольшую долю среди всех алгоритмов, поэтому вероятность того, что в выборке $a_1, \dots, a_n$ встретится алгоритм из первых $t$ слоев, крайне невысока.
Чтобы сгенерировать выборку из первых $t$ слоев, воспользуемся методом случайного блуждания по графу (см. алгоритм \ref{random_walk}),
применив его к нижним $t$ слоям.
Известно (\cite{Lovasz1993}), что если граф не является двудольным, то вероятность получить алгоритм $a$ на $i$-м шаге
стремится при росте~$i$ к величине $\pi(a) = \frac{\deg(a)}{2 |E|}$, где $|E|$ --- число ребер в графе.

\begin{algorithm}
\caption{Случайное блуждание}
\label{random_walk}
\begin{algorithmic}[1]
    \REQUIRE Граф $G = (V, E)$, стартовая вершина $v_1$, число итераций $i_{max}$
    \ENSURE Выборка $v_1, v_2, \dots, v_{i_{max}}$

    \FOR{$i = 2, \dots, i_{max}$}
        \STATE $R := \{ v \in V \ |\ (v_{i - 1}, v) \in E \}$     \COMMENT{окрестность вершины $v_{i - 1}$}
        \STATE Выбрать случайно вершину $v^\prime$ из равномерного распределения на $R$
        \STATE $v_i := v^\prime$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Ленивое случайное блуждание}
\label{lazy_random_walk}
\begin{algorithmic}[1]
    \REQUIRE Граф $G = (V, E)$, стартовая вершина $v_1$, число итераций $i_{max}$
    \ENSURE Выборка $v_1, v_2, \dots, v_{i_{max}}$

    \FOR{$i = 2, \dots, i_{max}$}
        \STATE Сгенерировать число $r$ из распределения Бернулли с $p = \frac{1}{2}$
        \IF{$r = 0$}
            \STATE $R := \{ v \in V \ |\ (v_{i - 1}, v) \in E \}$     \COMMENT{окрестность вершины $v_{i - 1}$}
            \STATE Выбрать случайно вершину $v^\prime$ из равномерного распределения на $R$
            \STATE $v_i := v^\prime$
        \ELSE
            \STATE $v_i :=  v_{i - 1}$
        \ENDIF
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Граф расслоения-связности является двудольным, поэтому для того, чтобы получить на нем то же стационарное распределение,
необходимо применять {\it ленивое случайное блуждание} (см. алгоритм \ref{lazy_random_walk}).
Оно отличается от обычного случайного блуждания тем,
что на каждом шаге с вероятностью $\frac{1}{2}$ блуждание
остается на месте, в вершине с предыдущего шага.
Это равносильно добавлению петель к каждой вершине,
в результате чего граф перестает быть двудольным,
и к нему применим указанный выше результат~(вероятность
получить вершину $a$ стремится к $\pi(a)$).

Итак, с помощью случайного блуждания мы можем получить
выборку~$a_1, \dots, a_n$ алгоритмов из первых~$t$ слоев,
где, начиная с некоторого номера,
алгоритм $a$ появляется с вероятностью~$\pi(a) = \frac{\deg(a)}{2 |E|}$.
Сделав поправку на эту вероятность, можно получить несмещенную оценку для среднего вклада по слою $m$:
\begin{equation}\label{approx_q}
    \hat Q_\varepsilon = \frac{1}{n} \sum_{i = 1}^{n} \frac{b(a_i)}{\pi(a_i)}
\end{equation}
Вычислим матожидание данной оценки~(здесь
$\hat a_1, \dots, \hat a_D$ --- все алгоритмы семейства):
\begin{align*}
    \mathbb{E} \hat Q_\varepsilon &= \frac{1}{n} \sum_{i = 1}^{n} \frac{b(a_i)}{\pi(a_i)} = \\
    &= \frac{1}{n} \sum_{i = 1}^{n} \mathbb{E} \frac{b(a_i)}{\pi(a_i)} = \\
    &= \frac{1}{n} \sum_{i = 1}^{n} \sum_{j = 1}^{D} \frac{b(\hat a_j)}{\pi(\hat a_j)} \pi(\hat a_j) = \\
    &= \frac{1}{n} \sum_{i = 1}^{n} Q_\varepsilon = \\
    &= Q_\varepsilon
\end{align*}
Таким образом, оценка~(\ref{approx_q}) действительно
является несмещенной.

Отметим, что мы осуществляем блуждание только по первым $t$ слоям графа, поэтому величина
$|E|$ --- это число ребер только в этих слоях, которое мы будем обозначать через $E_t$.
Для того, чтобы узнать точное значение $E_t$, необходимо полностью
обойти первые $t$ слоев, что крайне нежелательно.
Предлагается вместо этого предлагается преобразовать величину $\pi(a)$:
\[
    \pi(a) = \frac{\deg(a)}{2 E_t} = \frac{\deg(a)}{V_t \frac{E_t}{V_t}},
\]
где $V_t$ --- число вершин в первых $t$ слоях.
Отношение $\frac{|E|}{|V|}$ является одинаковым практически для всех
подмножеств графа расслоения-связности,
поэтому можно подставить его вместо $\frac{E_t}{V_t}$.
Величину $V_t$ можно оценить путем случайной генерации
алгоритмов из семейства~$\mathcal{A}$.

%Вместо этого предлагается подставлять в~(\ref{approx_q}) оценку $E_t$,
%основанную на гипотезе сепарабельности:
%\[
%    \hat E_t = \hat V_t \sum_{u = 0}^{L} u \lambda_u,
%\]
%где $V_t$~--- оценка числа вершин в первых $t$ слоях,
%полученная путем случайной генерации алгоритмов из семейства~$\mathcal{A}$;
%$\lambda_u$~--- оценка доли алгоритмов с~$u(a) = u$ среди всех алгоритмов семейства,
%вычисленная по $a_1, \dots, a_n$.

%в графе, состоящем только из первых $t$ слоев графа расслоения-связности.
%Вместо этого предлагается подставлять в~(\ref{approx_q}) оценку $E_t$
%по выборке $a_1, \dots, a_n$:
%\[
%    \hat E_t = \frac{1}{n} \sum_{i = 1}^{n} \frac{\deg^+(a_i)}{\pi(a_i)},
%\]
%где $\deg^+(a_i)$ --- число вершин в верхней полуокрестности вершины $a_i$
%в графе, состоящем только из первых $t$ слоев графа расслоения-связности.
%Можно показать, что данная оценка является несмещенной.

Вклады в оценку $b(a)$ в пределах одного слоя сильно варьируются,
и простое случайное блуждание может учесть эти вариации лишь при очень большом числе шагов.
Существует метод Frontier Sampling (\cite{Ribeiro2010}), позволяющий избежать таких проблем (см. алгоритм \ref{frontier_sampling}).
В данной работе в качестве стартовых вершин $P$ предлагается брать множество всех истоков графа расслоения-связности.
Показано, что если граф не является двудольным, то вероятность получить алгоритм $a$ на $i$-м шаге
стремится при росте $i$ к величине
\[
    \pi(a) = \frac{deg(a)}{2 |E|}
\]
Если произвести модификацию алгоритма, аналогичную модификации в ленивом случайном блуждании, то можно получить
такое же стационарное распределение и для двудольных графов.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{pics/rw/unsep200_err8_30000iter_mult_straighforward.eps}
    \caption{Frontier sampling. По оси $X$ --- число отброшенных первых сэмплов, по оси $Y$ --- полученная при этом оценка.}
    \label{straightforward_is_bad}
\end{figure}

Отметим, что в случайном блуждании следует отбрасывать некоторое количество первых сэмплов,
поскольку в начале блуждания распределение марковской цепи отличается от $\pi(a)$.

С помощью алгоритма~\ref{frontier_sampling} была вычислена оценка расслоения-связности
для линейно неразделимой выборки с $L = 200$.
Наилучший алгоритм на данной выборке допускал $8$ ошибок.
Оценка вычислялась по формуле~(\ref{approx_q}),
причем в качестве $E_t$ бралось истинное число ребер
в первых $t$ слоях.
Результат изображен на рис.~\ref{straightforward_is_bad}.
Видно, что методу не удается получить несмещенную оценку.
Это говорит о том, что сходимость метода крайней медленная
и не достигается даже за $30000$ тысяч итераций.

\begin{algorithm}[t]
\caption{Frontier sampling}
\label{frontier_sampling}
\begin{algorithmic}[1]
    \REQUIRE Граф $G = (V, E)$, набор стартовых вершин $P = (v^1, \dots, v^s)$, число итераций $i_{max}$
    \ENSURE Выборка $v_1, v_2, \dots, v_{i_{max}}$

    \FOR{$i = 1, \dots, i_{max}$}
        \STATE Выбрать вершину $v \in P$ с вероятностью $\frac{deg(v)}{\sum_{u \in P} deg(u)}$
        \STATE $R := \{ v^\prime \in V \ |\ (v, v^\prime) \in E \}$     \COMMENT{окрестность вершины $v$}
        \STATE Выбрать случайно вершину $v^\prime$ из равномерного распределения на $R$
        \STATE $v_i := v^\prime$
        \STATE Заменить в $P$ вершину $v$ на $v^\prime$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
\begin{multicols}{2}
    \hfill
    \includegraphics[scale=0.5]{pics/rw/unsep200_err8_50000iter_max30_oneW.eps}
    \hfill
    \caption{Ленивое случайное блуждание. По оси $X$ --- число отброшенных первых сэмплов, по оси $Y$ --- полученная при этом оценка.}
    \label{one_walker_burnin}
    \hfill
    \includegraphics[scale=0.5]{pics/rw/unsep200_err8_30000iter_mult.eps}
    \hfill
    \caption{Frontier sampling. По оси $X$ --- число отброшенных первых сэмплов, по оси $Y$ --- полученная при этом оценка.}
    \label{many_walkers_burnin}
\end{multicols}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{pics/rw/unsep200_err8_30000iter_approx_am.eps}
    \caption{Frontier sampling. По оси $X$ --- число отброшенных первых сэмплов, по оси $Y$ --- полученная при этом оценка.
                Использовалось приближение для~$|A_m|$.}
    \label{many_walkers_approx}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{pics/rw/unsep200_err8_30000iter_mult_wind1000.eps}
    \caption{Frontier sampling. По оси $X$ --- номер сэмпла, начиная с которого была взята тысяча сэмплов, по оси $Y$ --- полученная при этом оценка.}
    \label{bound_window}
\end{figure}

Увеличить сходимость позволяет следующий прием.
Преобразуем оценку вероятности переобучения:
\[
    Q_{\varepsilon}(\mu, \mathbb{X}) \leq \sum_{i = 1}^{D} b(a_i) = \sum_{m = 0}^{L} |A_m| \left( \frac{1}{|A_m|} \sum_{a \in A_m} b(a) \right),
\]
где $A_m$ --- это множество всех алгоритмов из $\mathcal{A}$, допускающих $m$ ошибок.
Тогда, если обозначить средний вклад в оценку алгоритмов из $m$-го слоя через $Q_m = \frac{1}{|A_m|} \sum_{a \in A_m} b(a)$, то получаем следующую запись для оценки:
\begin{equation}\label{bound_means}
    Q_{\varepsilon}(\mu, \mathbb{X}) \leq \sum_{m = 0}^{L} |A_m| Q_m
\end{equation}
В качестве $Q_m$ предлагается использовать следующую оценку:
\begin{equation}\label{approx_qm}
    \hat Q_m = \frac{1}{\sum_{i = 1}^{n} [m(a_i) = m]} \sum_{i = 1}^{n} [m(a_i) = m]\ \frac{1}{\pi(a_i) |V|} b(a_i)
\end{equation}
Мощность $m$-го слоя $|A_m|$ предлагается оценивать следующим образом:
\[
    |\hat A_m| = \frac{V_t}{n} \sum_{i = 1}^{n} [m(a_i) = m]
\]
А лучше так:
\[
    |\hat A_m| = \frac{V_t}{n} \sum_{i = 1}^{n} \frac{[m(a_i) = m]}{\pi(a_i)}
\]

Оценка, вычисленная данным методом для выборки, описанной выше,
изображена на рис.~\ref{many_walkers_burnin}.
В качестве $|A_m|$ использовались истинные значения.
В данном эксперименте удается получить несмещенные оценки
(среднее по всем оценкам совпадает с истинной оценкой расслоения-связности).
Также на рис.~\ref{one_walker_burnin} изображена оценка, вычисленная
с помощью обычного ленивого случайного блуждания.
Даже с использованием описанного приема она не позволяет
получить несмещенную оценку.

На рис.~\ref{many_walkers_approx} показана оценка, вычисленная с помощью
метода~\ref{frontier_sampling} с использованием приближения~$|A_m|$.
Видно, что возникает некоторое смещение оценки,
но оно является достаточно небольшим (примерно $0.05$)
и все равно позволяет судить о величине вероятности переобучения.

На рис.~\ref{bound_window} изображены оценки, вычисленные по $1000$ сэмплов,
начиная с определенного номера.
Видно, что оценки становятся несмещенными только для сэмплов с номером больше $10000$,
что говорит о небольшой скорости сходимости.
Известно~(\cite{blablabla}), что повышения сходимости можно добиться,
если на каждом шаге случайного блуждания с небольшой вероятностью $\alpha$
переходить в равномерно выбранную вершину графа.
Поскольку в нашем случае рассматриваются нижние слои графа расслоения-связности,
то равномерной генерации можно добиться путем
обучения по случайной подвыборке.

\subsection{Отбор признаков}
Эксперименты проводились на наборе данных Ecoli из репозитория UCI.
В данных имелось два категориальных признака, которые были исключены.
Проекции выборки на двухэлементные подмножества признаков изображены
на рис.~\ref{sample}.

Для всех двухэлементных подмножеств признаков были вычислены два критерия качества:
основанный на комбинаторных оценках и основанный на регуляризации.

Критерий, основанный на регуляризации, вычислялся по формуле
\[
    Q_r = \nu(a, X) + \|w\|^2,
\]
где $a$ --- линейный алгоритм, построенный методом SVM, $w$ --- соответствующий ему вектор весов.

Вычисление комбинаторного критерия состояло из следующих шагов:
\begin{enumerate}
    \item Строилась линейная разделяющая поверхность методом SVM.
    \item Из нее осуществлялся спуск вниз по SC-графу до истока.
    \item Из этого истока запускался обход всех слоев SC-графа вплоть до $(m_0 + 3)$-го,
            где $m_0$ --- число ошибок найденного истока; затем фиксировались все истоки, найденные
            во время этого обхода.
    \item Оценивался профиль расслоения путем случайной генерации $20000$ объектов.
    \item Генерировалась $1000$ объектов из $20$ нижних слоев графа с помощью случайного блуждания.
    \item Производилось обращение оценки с $\eta = \frac{1}{2}$.
    \item вычислялся комбинаторный критерий по формуле
            \[
                Q_c = \nu(a_0, X) + \varepsilon\left(\frac{1}{2}\right),
            \]
            где $a_0$ --- лучший алгоритм в семействе.
\end{enumerate}

Величины, полученные с помощью описанных критериев, приведены в таблицах~\ref{table_comb} и~\ref{table_reg}.
Подмножества, отсортированные по качеству с точки зрения этих критериев, записаны в таблице~\ref{table_sort}.

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | c | c | c | c | c | c |}
        \hline
            &   $1$ &   $2$         &   $3$     &   $4$     &   $5$     \\ \hline
        $1$ &       &   $0.3214$    &   $0.306$ &   $0.142$ &   $0.217$ \\ \hline
        $2$ &       &               &   $0.327$ &   $0.128$ &   $0.235$ \\ \hline
        $3$ &       &               &           &   $0.229$ &   $0.339$ \\ \hline
        $4$ &       &               &           &           &   $0.119$ \\ \hline
        $4$ &       &               &           &           &           \\ \hline
    \end{tabular}
    \caption{Качество двухэлементных подмножеств с точки зрения комбинаторного критерия}
    \label{table_comb}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | c | c | c | c | c | c |}
        \hline
            &   $1$ &   $2$         &   $3$     &   $4$     &   $5$     \\ \hline
        $1$ &       &   $112.5$     &   $113$   &   $88.9$  &   $107.9$ \\ \hline
        $2$ &       &               &   $155.7$ &   $93$    &   $131.6$ \\ \hline
        $3$ &       &               &           &   $107.5$ &   $129.8$ \\ \hline
        $4$ &       &               &           &           &   $109.1$ \\ \hline
        $4$ &       &               &           &           &           \\ \hline
    \end{tabular}
    \caption{Качество двухэлементных подмножеств с точки зрения регуляризационного критерия}
    \label{table_reg}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
    \begin{tabular}{ | c | c | c |}
        \hline
            &   Комбинаторный критерий  &   Регуляризационный критерий \\ \hline
        $1$ &   $\{ 4, 5 \}$            &   $\{ 1, 4 \}$               \\ \hline
        $2$ &   $\{ 2, 4 \}$            &   $\{ 2, 4 \}$               \\ \hline
        $3$ &   $\{ 1, 4 \}$            &   $\{ 3, 4 \}$               \\ \hline
        $4$ &   $\{ 1, 5 \}$            &   $\{ 1, 5 \}$               \\ \hline
        $5$ &   $\{ 3, 4 \}$            &   $\{ 4, 5 \}$               \\ \hline
        $6$ &   $\{ 2, 5 \}$            &   $\{ 1, 2 \}$               \\ \hline
        $7$ &   $\{ 1, 3 \}$            &   $\{ 1, 3 \}$               \\ \hline
        $8$ &   $\{ 1, 2 \}$            &   $\{ 3, 5 \}$               \\ \hline
        $9$ &   $\{ 2, 3 \}$            &   $\{ 2, 5 \}$               \\ \hline
        $10$&   $\{ 3, 5 \}$            &   $\{ 2, 3 \}$               \\ \hline
    \end{tabular}
    \caption{Подмножества, отсортированные по убыванию качества}
    \label{table_sort}
\end{center}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.1\textwidth]{pics/feature_selection/sample.eps}
    \caption{Двухэлементные подмножества признаков}
    \label{sample}
\end{figure}

\subsection{Направления дальнейших исследований}
\begin{itemize}
    \item Исключить из описанного подхода вычисление средних оценок по слоям,
            перейти к непосредственной оценке вероятности переобучения путем случайного блуждания;
    \item Ускорить сходимость, добавив небольшое количество обучений по случайным подвыборкам;
    \item Продолжить эксперименты с отбором признаков;
    \item Встроить в эксперименты построений композиций.
\end{itemize}

\clearpage
\section{Общий метод обхода графа расслоения-связности}
Пусть заданы выборка $\mathbb{X} = \{ x_1, \dots, x_n \} \subset \mathbb{R}^d$ и целевая функция $y : \mathbb{X} \rightarrow \mathbb{Y}$.
Будем считать, что семейство алгоритмов задается следующим образом:
\[
    \mathcal{A} = \{
        a_\theta(x) \cond a_\theta(x) = \left[  K(x, \theta) < 0  \right],  \theta \in \Theta
    \},
\]
где $\Theta = \mathbb{R}^p$ --- множество параметров, $K(x, \theta)$ --- некоторая фиксированная функция.

Для каждого объекта $x_i \in \mathbb{X}$ определим гиперповерхность $S_i$ в  $\Theta$:
\[
    S_i = \{
        \theta \in \Theta  \cond  K(x_i, w) = 0
    \}
\]

Набор гиперповерхностей $\{  S_1, \dots, S_n  \}$ задает разбиение $\Theta$ на максимальные связные области, не пересекающиеся ни с одной
из этих гиперповерхностей.
Такие области называются {\it ячейками}, а набор ячеек --- {\it конфигурацией} гиперповерхностей.
Основное свойство такого разбиения состоит в том, что все алгоритмы, лежащие в одной ячейке, одинаково классифицируют всю выборку.
Значит, найдя все ячейки конфигурации, мы получим описание всего семейства $\mathcal{A}$.

\subsection{Известные результаты}
Пусть любая гиперповерхность $S_i$ представима в виде
\[
    S_i = (Q_i = 0) \wedge F_i \left(
        P_{i_1} \sigma_{i_1} 0, \dots, P_{i_u} \sigma_{i_u} 0
    \right),
\]
где $F_i$ --- булева формула, $Q_i, P_{i_1}, \dots, P_{i_u}$ --- полиномы над $\Theta$, $\sigma_{i_j} \in \{ \leq, \geq \}$.

Известно, что в этом случае можно найти {\it разбиение на цилиндрические ячейки}, которое является подразбиением ячеек конфигурации \cite{chazelle91decomposition}.

Также существует алгоритм поиска ячеек для более общего случая, когда все $S_i$ являются полупфаффовыми множествами \cite{gabrielov95stratification}.
Примерами полупфаффовых множеств являются графики тригонометрических, экспоненциальных и логарифмических функций \cite{agarwal98arrangements}.

\subsection{Обход нижних слоев графа расслоения-связности}
Мы сосредоточимся на простых семействах алгоритмов и получим для них алгоритм обхода нижних слоев SC-графа.

Предъявим следующие {\it требования регулярности} к $\mathcal{A}$:
\begin{enumerate}
    \item Одному классу эквивалентности алгоритмов соответствует ровно одна ячейка.
    \item Для любой размерности $d$ существует такое число $m = m(d)$, что по любым $m$ объектам выборки можно
            построить единственный алгоритм $a_\theta(x)$, из которого малыми изменениями $\theta$ можно получить
            любую классификацию этих $m$ точек, не изменив классификацию остальных точек.
    \item Для любого класса эквивалентности $a$ найдется $m$ объектов, по которым можно построить алгоритм, лежащий в этом классе.
            Множество таких наборов будем обозначать через $T_a$.
    %\item Существует эффективный способ для любого алгоритма $a$ найти набор из $m$ объектов, по которым можно построить алгоритм, эквивалентный $a$
    \item $T_a$ является $1$-связным, то есть из любого его набора можно получить любой другой, меняя только
            по одному элементу на каждом шаге, и не выходя при этом за пределы $T_a$.
    \item Если $a$ и $a^\prime$ --- соседние алгоритмы, то $T_{a} \cap T_{a^\prime} \neq \emptyset$.
\end{enumerate}

Примерами семейств алгоритмов, удовлетворяющих этим требованиям, являются:
\begin{itemize}
    \item конъюнкции: $K(x, \theta) = \prod_{i = 1}^d \left[  x_i < \theta_i  \right]$
    \item линейные классификаторы: $K(x, \theta) = \langle x, \theta \rangle$
    \item шары: $K(x, \theta) = K(x, \theta_1, \theta_2) = \rho (x, \theta_1) - \theta_2$
    \item SVM: $K(x, \theta) = \sum_{i = 1}^{h} \theta_i y_i M(x, x_i) - \theta_0$
\end{itemize}

Итак, пусть $\mathcal{A}$ --- семейство алгоритмов, удовлетворяющее указанным выше требованиям, и пусть известен лучший или почти лучший алгоритм $a_0$.
Тогда, используя алгоритм \ref{traversal_alg} для поиска соседних вершин, можно осуществить обход графа расслоения-связности, начиная с $a_0$.

\begin{algorithm}[t]
\caption{Построение окрестности алгоритма $a$}
\label{traversal_alg}
\begin{algorithmic}[1]
    \REQUIRE $\mathbb{X}$, $a$;
    \ENSURE $V_{a}$ --- окрестность алгоритма $a$;
    \STATE Найти набор $D_0 = (x_{i_1}, \dots, x_{i_m})$, по которому можно построить алгоритм, эквивалентный $a$
    \STATE $ T_{a} := \{ D_0 \} $
    \STATE $D_0.\text{проверен} := \text{нет}$
    \STATE $ V_{a} := \emptyset $
    \WHILE{ $ \exists D \in T_{a} : \ D.\text{проверен} = \text{нет} $ }
        \FORALL{ $x_j \in D$ }
            \FORALL{ $t \in \{1, \dots n \} \setminus D $ }
                %\STATE $ D^\prime := (x_{i_1}, \dots, x_{i_{j - 1}}, x_{j}, x_{i_{j + 1}}, \dots, x_{i_m})$
                \STATE $ D^\prime := \left( D \setminus \{x_j\} \right) \cup x_t $
                \STATE $\hat \theta := НайтиАлгоритм(D^\prime)$
                \IF{ из $\hat \theta$ можно получить алгоритм с таким же вектором ошибок, как у $a$ }
                    \STATE $T_{a_0} := T_{a_0} \cup \{ D^\prime \}$
                    \STATE $D^\prime .\text{проверен} := \text{нет}$
                \ENDIF
                \IF{ из $\hat \theta$ можно получить алгоритм, вектор ошибок которого отличается от $a$ ровно на одном объекте }
                    \STATE $ V_{a} := V_{a} \cup \{ D^\prime \} $
                \ENDIF
                \STATE $D.\text{проверен} := \text{да}$
            \ENDFOR
        \ENDFOR
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Направления дальнейших исследований}
\begin{itemize}
    \item Показать для указанных семейств, что они действительно удовлетворяют условиям регулярности;
    \item Попытаться увеличить эффективность алгоритма.
\end{itemize}

\end{document}
